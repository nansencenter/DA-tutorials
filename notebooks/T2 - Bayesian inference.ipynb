{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import resources.workspace as ws\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "% START OF MACRO DEF\n",
    "% DO NOT EDIT IN INDIVIDUAL NOTEBOOKS, BUT IN macros.py\n",
    "%\n",
    "\\newcommand{\\Reals}{\\mathbb{R}}\n",
    "\\newcommand{\\Expect}[0]{\\mathbb{E}}\n",
    "\\newcommand{\\NormDist}{\\mathcal{N}}\n",
    "%\n",
    "\\newcommand{\\DynMod}[0]{\\mathscr{M}}\n",
    "\\newcommand{\\ObsMod}[0]{\\mathscr{H}}\n",
    "%\n",
    "\\newcommand{\\mat}[1]{{\\mathbf{{#1}}}} \n",
    "%\\newcommand{\\mat}[1]{{\\pmb{\\mathsf{#1}}}}\n",
    "\\newcommand{\\bvec}[1]{{\\mathbf{#1}}} \n",
    "%\n",
    "\\newcommand{\\trsign}{{\\mathsf{T}}} \n",
    "\\newcommand{\\tr}{^{\\trsign}} \n",
    "\\newcommand{\\tn}[1]{#1} \n",
    "\\newcommand{\\ceq}[0]{\\mathrel{≔}}\n",
    "%\n",
    "\\newcommand{\\I}[0]{\\mat{I}} \n",
    "\\newcommand{\\K}[0]{\\mat{K}}\n",
    "\\newcommand{\\bP}[0]{\\mat{P}}\n",
    "\\newcommand{\\bH}[0]{\\mat{H}}\n",
    "\\newcommand{\\bF}[0]{\\mat{F}}\n",
    "\\newcommand{\\R}[0]{\\mat{R}}\n",
    "\\newcommand{\\Q}[0]{\\mat{Q}}\n",
    "\\newcommand{\\B}[0]{\\mat{B}}\n",
    "\\newcommand{\\C}[0]{\\mat{C}}\n",
    "\\newcommand{\\Ri}[0]{\\R^{-1}}\n",
    "\\newcommand{\\Bi}[0]{\\B^{-1}}\n",
    "\\newcommand{\\X}[0]{\\mat{X}}\n",
    "\\newcommand{\\A}[0]{\\mat{A}}\n",
    "\\newcommand{\\Y}[0]{\\mat{Y}}\n",
    "\\newcommand{\\E}[0]{\\mat{E}}\n",
    "\\newcommand{\\U}[0]{\\mat{U}}\n",
    "\\newcommand{\\V}[0]{\\mat{V}}\n",
    "%\n",
    "\\newcommand{\\x}[0]{\\bvec{x}}\n",
    "\\newcommand{\\y}[0]{\\bvec{y}}\n",
    "\\newcommand{\\z}[0]{\\bvec{z}}\n",
    "\\newcommand{\\q}[0]{\\bvec{q}}\n",
    "\\newcommand{\\br}[0]{\\bvec{r}}\n",
    "\\newcommand{\\bb}[0]{\\bvec{b}}\n",
    "%\n",
    "\\newcommand{\\bx}[0]{\\bvec{\\bar{x}}}\n",
    "\\newcommand{\\by}[0]{\\bvec{\\bar{y}}}\n",
    "\\newcommand{\\barB}[0]{\\mat{\\bar{B}}}\n",
    "\\newcommand{\\barP}[0]{\\mat{\\bar{P}}}\n",
    "\\newcommand{\\barC}[0]{\\mat{\\bar{C}}}\n",
    "\\newcommand{\\barK}[0]{\\mat{\\bar{K}}}\n",
    "%\n",
    "\\newcommand{\\D}[0]{\\mat{D}}\n",
    "\\newcommand{\\Dobs}[0]{\\mat{D}_{\\text{obs}}}\n",
    "\\newcommand{\\Dmod}[0]{\\mat{D}_{\\text{obs}}}\n",
    "%\n",
    "\\newcommand{\\ones}[0]{\\bvec{1}} \n",
    "\\newcommand{\\AN}[0]{\\big( \\I_N - \\ones \\ones\\tr / N \\big)}\n",
    "%\n",
    "% END OF MACRO DEF\n",
    "$\n",
    "# The Gaussian (Normal) distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the Gaussian random variable $x \\sim \\mathcal{N}(b, B)$.\n",
    "\n",
    "Equivalently, we may write\n",
    "$\\begin{align}\n",
    "p(x) = \\mathcal{N}(x \\mid b, B)\n",
    "\\end{align}$\n",
    "for its probability density function (**pdf**), which is given by\n",
    "$$\\begin{align}\n",
    "\\mathcal{N}(x \\mid b, B) = (2 \\pi B)^{-1/2} e^{-(x-b)^2/2 B} \\, , \\tag{G1}\n",
    "\\end{align}$$\n",
    "for $x \\in (-\\infty, +\\infty)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.2:** Code it up (complete the code below)! Hints:\n",
    "* Note that `**` is the power operator in Python.\n",
    "* As in Matlab, $e^x$ is available as `np.exp(x)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_G1(x, b, B):\n",
    "    \"Univariate (scalar), Gaussian pdf\"\n",
    "    ### INSERT ANSWER HERE ###\n",
    "    return pdf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ws.show_answer('pdf_G1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's plot the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density parameters\n",
    "b = 0  # mean     of distribution\n",
    "B = 25 # variance of distribution\n",
    "\n",
    "# Grid computations\n",
    "N  = 201                    # num of grid points\n",
    "xx = np.linspace(-20, 20,N) # grid\n",
    "dx = xx[1]-xx[0]            # grid spacing\n",
    "pp = pdf_G1(xx, b, B)       # pdf values\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 2))\n",
    "plt.plot(xx, pp);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could for example be the pdf of a stochastic noise variable. It could also describe our *quantitative belief* and uncertainty about a parameter (or state), which we model as randomness in Bayesian statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.3:** Play around with `b` and `B` (and re-run the above code) and answer these questions by looking at the resulting figure:\n",
    " * How does the pdf curve change when `b` changes?\n",
    " * How does the pdf curve change when you increase `B`?\n",
    " * In a few words, describe the shape of the Gaussian pdf curve. Does this ring a bell for you? Hint: it should be clear as a bell!\n",
    " \n",
    "<mark><font size=\"-1\">\n",
    "<b>NB:</b> Restore `B=25` and re-run the above cell (this is a convenient value for the below examples)\n",
    "</font></mark>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.4*:** Recall the definition of the expectation (in $x$), namely\n",
    "$$\\Expect [f(x)] \\mathrel{≔} \\int p(x) f(x) \\, d x \\,,$$\n",
    "where the integral is over the domain of $x$.\n",
    "\n",
    "Recall $p(x) = \\mathcal{N}(x \\mid b, B)$ from eqn (G1).  \n",
    "Use pen, paper, and calculus to show that\n",
    " - (i) $E[1] = 1$.  \n",
    "   *Hint: This is actually quite difficult.  \n",
    "   Both Bernouilli and Laplace both did not figure it out\n",
    "   until C. F. Gauss managed it by computing $(E[1])^2$ instead.  \n",
    "   Try doing the same!* \n",
    " - (ii) the first parameter, $b$, indicates its mean, i.e. that $b = \\Expect[x]$.\n",
    " - (iii) the second parameter, $B>0$, indicates its variance, i.e. that $B = \\Expect[(x-b)^2]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.5:** Recall $p(x) = \\mathcal{N}(x \\mid b, B)$ from eqn (G1).  \n",
    "Use pen, paper, and calculus to answer the following questions,  \n",
    "which derive some helpful mnemonics about the distribution.\n",
    "\n",
    " * (i) Find $x$ such that $p(x) = 0$.\n",
    " * (ii) Where is the location of the mode (maximum) of the distribution?\n",
    "I.e. find $x$ such that $\\frac{d p}{d x}(x) = 0$.  \n",
    "Hint: it's easier to analyse $\\log p(x)$ rather than $p(x)$ itself.\n",
    " * (iii) Where is the inflection point? I.e. where $\\frac{d^2 p}{d x^2}(x) = 0$.\n",
    " * (iv) Some forms of \"sensitivity analysis\" (a basic form of uncertainty quantification) consist in evaluating $\\frac{d^2 p}{d x^2}(x)$ at the mode.  \n",
    "Explain this by reference to the Gaussian shape.\n",
    "Hint: calculate and interpret $\\frac{d^2 p}{d x^2}(b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The multivariate (i.e. vector) case\n",
    "Here's the pdf of the *multivariate* Gaussian:\n",
    "$$\\begin{align}\n",
    "\\NormDist(x \\mid  b, B)\n",
    "&=\n",
    "|2 \\pi B|^{-1/2} \\, \\exp\\Big(-\\frac{1}{2}\\|x-b\\|^2_B\\Big) \\, , \\tag{GM}\n",
    "\\end{align}$$\n",
    "where $|.|$ represents the determinant, and $\\|.\\|_W$ represents the norm with weighting: $\\|x\\|^2_W = x^T W^{-1} x$.  \n",
    "In this multivariate case, $B$ is called the *covariance* (matrix).\n",
    "\n",
    "The following implements this pdf. Take a moment to digest the code. Don't worry if you don't understand all of the details. Hints:\n",
    " * `@` produces matrix multiplication (`*` in `Matlab`);\n",
    " * `*` produces array multiplication (`.*` in `Matlab`);\n",
    " * `axis=-1` makes `np.sum()` work along the last dimension of an ND-array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import det, inv\n",
    "\n",
    "def weighted_norm22(xx, W):\n",
    "    \"Computes the norm of each vector (on the last axis) of xx, weighted by W.\"\n",
    "    ww = np.sum( (xx @ inv(W)) * xx, axis=-1)\n",
    "    return ww\n",
    "\n",
    "def pdf_GM(xx, b, B):\n",
    "    \"pdf -- Gaussian, Multivariate: N(x | b, B)\"\n",
    "    c = np.sqrt(det(2*np.pi*B))\n",
    "    return 1/c * np.exp(-0.5*weighted_norm22(xx - b, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code plots the pdf as contour (iso-density) curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX, YY = np.meshgrid(xx, xx)\n",
    "grid = np.dstack((XX, YY))\n",
    "\n",
    "@ws.interact(corr=(0, 1, .05), var1=(0.1**2, 10**2))\n",
    "def plot_Gaussian_contours(corr=0.7, var1=1):\n",
    "    var2 = 1\n",
    "    cov12 = np.sqrt(var1 * var2) * corr\n",
    "    Cov = B * np.array([[var1  , cov12],\n",
    "                        [cov12 , var2]])\n",
    "    # Eval\n",
    "    pp = pdf_GM(grid, b=0, B=Cov)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.contour(XX, YY, pp)\n",
    "    plt.axis('equal');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.7:** How do the contours look? Try to understand why. Cases:\n",
    " * (a) correlation=0.    \n",
    " * (b) correlation=0.99.\n",
    " * (c) correlation=0.5. (Note that we've used `plt.axis('equal')`).\n",
    " * (d) correlation=0.5, but with non-equal variances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.8:** Play the [correlation game](http://guessthecorrelation.com/) (doesn't work right in Chrome) until you get a score (shown as gold coins) of 5 or more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.9:**\n",
    "* What's the difference between correlation and covariance?\n",
    "* What's the difference between correlation (or covariance) and dependence?\n",
    "* Does correlation imply causation?\n",
    "* Can you use correlation to in making predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes' rule\n",
    "Bayes' rule is how we do inference.  \n",
    "It defines how we should merge our prior (quantitative belief) about $x$,  \n",
    "when given an observation $y$ somehow related to $x$.\n",
    "\n",
    "\n",
    "For continuous random variables, $x$ and $y$, it reads:\n",
    "\n",
    "$$\\begin{align}\n",
    "p(x|y) = \\frac{p(x) \\, p(y|x)}{p(y)} \\, , \\tag{2}\n",
    "\\end{align}$$\n",
    "\n",
    "or, in words:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{\"posterior\" (pdf of $x$ given $y$)}\n",
    "\\; = \\;\n",
    "\\frac{\\text{\"prior\" (pdf of $x$)}\n",
    "\\; \\times \\;\n",
    "\\text{\"likelihood\" (pdf of $y$ given $x$)}}\n",
    "{\\text{\"normalization\" (pdf of $y$)}} \\, .\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ws.show_example('BR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.10:** Derive Bayes' rule from the definition of [conditional pdf's](https://en.wikipedia.org/wiki/Conditional_probability_distribution#Conditional_continuous_distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ws.show_answer('BR derivation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>Exercises marked with an asterisk (*) are optional.</em>\n",
    "\n",
    "**Exc 2.11*:** Slightly after reverend T. Bayes, P. S. Laplace also independently (and more clearly) developed Bayes' rule, published in 1774. Some time thereafter, what we now call \"statistical inference\" came to be known as the reasoning of \"inverse probability\". Nowadays, \"inverse problems\" are often given a statistical interpretation. Considering this context, why do you think we use $x$ for the \"unknown\", and $y$ for the known/given/fixed data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ws.show_answer('inverse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computers generally work with discrete, numerical representations of mathematical entities.\n",
    "Numerically, pdfs may be represented by their `values` on a grid, such as `xx` from above. Bayes' rule (2) then consists of *grid-point-wise* multiplication, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bayes_rule(prior_values, lklhd_values, dx):\n",
    "    \"Numerical (pointwise) implementation of Bayes' rule.\"\n",
    "    pp = prior_values * lklhd_values   # pointwise multiplication\n",
    "    posterior_values = pp/(sum(pp)*dx) # normalization\n",
    "    return posterior_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows Bayes' rule in action.  \n",
    "Again, remember that the only thing it's doing is multiplying the `prior value` and `likelihood value` at each gridpoint.  \n",
    "Move the sliders with the arrow keys to animate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the prior's parameters\n",
    "b = 0 # mean\n",
    "B = 1 # variance\n",
    "\n",
    "@ws.interact(y=(-10, 10, 1), R=(0.01, 20, 0.2))\n",
    "def animate_Bayes(y=4.0, R=1):\n",
    "    prior_vals = pdf_G1(xx, b, B)\n",
    "    lklhd_vals = pdf_G1(y, xx, R)\n",
    "    \n",
    "    postr_vals = Bayes_rule(prior_vals, lklhd_vals, xx[1]-xx[0])\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(xx, prior_vals, label='prior $\\mathcal{N}(x | b, B)$')\n",
    "    plt.plot(xx, lklhd_vals, label='likelihood $\\mathcal{N}(y | x, R)$')\n",
    "    plt.plot(xx, postr_vals, label='posterior - pointwise')\n",
    "    \n",
    "    ### Uncomment this block AFTER doing the Exc 2.24 ###\n",
    "    # xhat, P = Bayes_rule_G1(b, B, y, R)\n",
    "    # postr_vals2 = pdf_G1(xx, xhat, P)\n",
    "    # plt.plot(xx, postr_vals2, '--', label='posterior - parametric\\n $\\mathcal{N}(x|\\hat{x}, P)$')\n",
    "    \n",
    "    plt.ylim(ymax=0.6)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.12:** This exercise serves to make you acquainted with how Bayes' rule blends information.  \n",
    "Move the sliders to see what happens, and answer the following:\n",
    " * What happens to the posterior when $R \\rightarrow \\infty$ ?\n",
    " * What happens to the posterior when $R \\rightarrow 0$ ?\n",
    " * Move around $y$. What is the posterior's location (mean/mode) when $R = B$ ?\n",
    " * Does the posterior scale (width) depend on $y$?  \n",
    "   What does this mean [information-wise](https://en.wikipedia.org/wiki/Differential_entropy#Differential_entropies_for_various_distributions)?\n",
    " * Consider the shape (ignoring location & scale) of the posterior. Does it depend on $R$ or $y$?\n",
    " * Can you see a shortcut to computing this posterior rather than having to do the pointwise multiplication?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ws.show_answer('Posterior behaviour')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.14:** Show that the normalization in `Bayes_rule()` amounts to the same as dividing by $p(y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ws.show_answer('BR normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, since $p(y)$ is implicitly known,\n",
    "we often don't bother to write it down, simplifying Bayes' rule (2) to\n",
    "$$\\begin{align}\n",
    "p(x|y) \\propto p(x) \\, p(y|x) \\, .  \\tag{3}\n",
    "\\end{align}$$\n",
    "In fact, do we even need to care about $p(y)$ at all? All we really need to know is how much more likely some possible $x = a$ (or an interval around $a$) is compared to any other $x=b$. There is no additional information in $p(y)$, as reflected in the fact that it is implicitly known by the integral $\\int p(x) \\, p(y|x) \\, d x = p(y)$.\n",
    "\n",
    "And if we want to be really philosophical, we can note that this last equality is true only because of the convention that all densities integrate to $1$. Otherwise it would be a proportionality. And something that holds only by convention cannot contain any additional information.\n",
    "\n",
    "PS1: In some cases (not of our concern here) Bayes' rule is applied to random variables where $y$ is not a given constant. In this case one must of course also keep track of $p(y)$.\n",
    "\n",
    "PS2: There are methods where $ p(x|y)$ is not known (has not been evaluated) for all $x$, but only at a few points.\n",
    "In these methods, estimation of the normalisation factor becomes an important question too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.15*:** \n",
    "* (a) Implement a \"uniform\" (or \"flat\" or \"box\") distribution pdf and call it `pdf_U1(x, b, B)`. These <a href=\"https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)#Moments\">formulae</a> for its mean/variance will be useful. In the above animations, replace `pdf_G1` with your new `pdf_U1` (both for the prior and likelihood). Ensure that everything is working correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ws.show_answer('pdf_U1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (b) \n",
    " - Why (in the figure) are the walls of the pdf (ever so slightly) inclined?\n",
    " - What happens when you move the prior and likelihood too far apart? Is the fault of the implementation, the math, or the problem statement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ws.show_answer('BR U1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (c)*:\n",
    " - Re-do Exc 2.12, now with `pdf_U1`.\n",
    "* (d)*:\n",
    " - Now test a Gaussian prior with a uniform likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark><font size=\"-1\">\n",
    "<b>NB:</b> At the end of this exercise, restore `pdf_G1` (both the prior and likelihood) in the above animation (for later use). \n",
    "</font></mark>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gaussian-Gaussian Bayes\n",
    "\n",
    "The above animation shows Bayes' rule in 1 dimension. Previously, we saw how a Gaussian looks in 2 dimensions. Can you imagine how Bayes' rule looks in 2 dimensions? In higher dimensions, these things get difficult to imagine, let alone visualize.\n",
    "\n",
    "Similarly, the size of the calculations required for Bayes' rule poses a difficulty. Indeed, the following exercise shows that (pointwise) multiplication for all grid points becomes preposterous in high dimensions.\n",
    "\n",
    "**Exc 2.16:**\n",
    " * (a) How many point-multiplications are needed on a grid with $N$ points in $M$ dimensions? (Imagine an $M$-dimensional cube where each side has a grid with $N$ points on it)\n",
    " * (b) Suppose we model 15 physical quantities, on each grid point, on a discretized surface model of Earth. Assume the resolution is $1^\\circ$ for latitude (110km), $1^\\circ$ for longitude. How many variables are there in total? This is the dimensionality ($M$) of the problem.\n",
    " * (c) Suppose each variable is has a pdf represented with a grid using only $N=10$ points. How many multiplications are necessary to calculate Bayes rule (jointly) for all variables on our Earth model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ws.show_answer('Dimensionality a')\n",
    "#ws.show_answer('Dimensionality b')\n",
    "#ws.show_answer('Dimensionality c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In response to this computational difficulty, we try to be smart and do something more analytical (\"pen-and-paper\"): we only compute the parameters (mean and (co)variance) of the posterior pdf.\n",
    "\n",
    "This is doable and quite simple in the Gaussian-Gaussian case:  \n",
    "With a prior $p(x) = \\mathcal{N}(x \\mid b,B)$ and a likelihood $p(y|x) = \\mathcal{N}(y \\mid x,R)$,\n",
    "the posterior is\n",
    "$$\\begin{align}\n",
    "p(x|y)\n",
    "&= \\mathcal{N}(x \\mid \\hat{x},P) \\tag{4} \\, ,\n",
    "\\end{align}$$\n",
    "where, in the univariate (1-dimensional) case:\n",
    "$$\\begin{align}\n",
    "    P &= 1/(1/B + 1/R) \\, , \\tag{5} \\\\\\\n",
    "  \\hat{x} &= P(b/B + y/R) \\, .  \\tag{6} \n",
    "\\end{align}$$\n",
    "\n",
    "The multivariate case is discussed in a later tutorial; for now, try to tackle exc 2.18."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exc  2.18 'Gaussian Bayes':\n",
    "Derive the above expressions for $P$ and $\\hat{x}$\n",
    "from Bayes' rule (3) and the expression for a Gaussian pdf (G1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ws.show_answer('BR Gauss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.20:** Algebra exercise: Show that $P = K R$, where\n",
    "$$K = B/(B+R) \\,,    \\tag{7}$$\n",
    "is called the \"Kalman gain\".\n",
    "Then shown that eqns (5) and (6) can be rewritten as\n",
    "$$\\begin{align}\n",
    "    P &= (1-K)B \\, ,  \\tag{8} \\\\\\\n",
    "  \\hat{x} &= b + K (y-b) \\tag{9} \\, ,\n",
    "\\end{align}$$\n",
    "*Hint: For eqn (8), begin from the right-hand side.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.22*:** Consider the formula for $K$ and its role in the previous couple of equations. Why do you think $K$ is called a \"gain\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ws.show_answer('KG intuition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.24:** Implement a Gaussian-Gaussian Bayes' rule (eqns 5 and 6, or eqns 8 and 9) by completing the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bayes_rule_G1(b, B, y, R):\n",
    "    ### INSERT ANSWER HERE ###\n",
    "    return xhat, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ws.show_answer('BR Gauss code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.26:** Go back to the above animation code, and uncomment the block that uses `Bayes_rule_G1()`. Re-run.  \n",
    "Make sure its curve coincides with that which uses pointwise multiplication (i.e. `Bayes_rule()`).\n",
    "This is the main secret of the \"Kalman filter\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.30*:** Why are we so fond of the Gaussian assumption?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ws.show_answer('Why Gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: [Univariate (scalar) Kalman filtering](T3%20-%20Univariate%20Kalman%20filtering.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
