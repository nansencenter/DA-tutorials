{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote = \"https://raw.githubusercontent.com/nansencenter/DA-tutorials\"\n",
    "!wget -qO- {remote}/master/notebooks/resources/colab_bootstrap.sh | bash -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08888704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources import show_answer, interact,  import_from_nb, nonchalance\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as rnd\n",
    "import scipy.linalg as sla\n",
    "from mpl_tools.place import freshfig\n",
    "plt.ion();\n",
    "plt.style.use(\"default\") # need tick markers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42a0085",
   "metadata": {},
   "source": [
    "# T7 - Spatial statistics (\"geostatistics\") & kriging\n",
    "\n",
    "In T4 and T5, we performed state estimation for scalar and vector (length-4) noisy, linear AR(1) processes,\n",
    "while T6 illustrated several nonlinear and chaotic dynamics, including the 1D (length-40) Lorenz-96 system.\n",
    "We now turn to estimating static (non-dynamic) 1D, 2D, and 3D spatial fields\n",
    "using the method known (in geostatistics) as **kriging**,\n",
    "**radial basis function (RBF)** interpolation (in mathematics),\n",
    "and **Gaussian process (GP)** regression (in machine learning).\n",
    "$\n",
    "\\newcommand{\\mat}[1]{{\\mathbf{{#1}}}}\n",
    "\\newcommand{\\vect}[1]{{\\mathbf{#1}}}\n",
    "\\newcommand{\\Expect}[0]{\\mathbb{E}}\n",
    "\\newcommand{\\NormDist}{\\mathscr{N}}\n",
    "\\newcommand{\\trsign}{{\\mathsf{T}}}\n",
    "\\newcommand{\\tr}{^{\\trsign}}\n",
    "$\n",
    "\n",
    "Once again, randomness will serve as a stand-in for uncertainty and will generate our synthetic experiments.\n",
    "Conceptually, little is new ‚Äì once discretized and flattened, spatial fields are simply long vectors of unknowns.\n",
    "The challenge lies in their scale: such long vectors lead to large and unwieldy covariance matrices.\n",
    "Moreover, rather than treating the covariance matrix as merely a symmetric table of numbers,\n",
    "we will explicitly model it through spatial relationships and distances\n",
    "between field elements. This type of covariance modeling may be used to\n",
    "*generate* priors, and the experience gained will help in understanding\n",
    "covariance localization.\n",
    "\n",
    "As always, we discretize the domains.\n",
    "The following sets up a lattice for fields on a 1D domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5292eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid1D = np.linspace(0, 1, 21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f6d4a",
   "metadata": {},
   "source": [
    "The following sets up a 2D grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf5431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid2x, grid2y = np.meshgrid(grid1D, grid1D)\n",
    "grid2x.shape  # `grid2y` has same shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93497302",
   "metadata": {},
   "source": [
    "Now we \"flatten\" (a.k.a. \"(un)ravel\", \"vectorize\", or \"string out\") this explicitly 2D grid of nodes into a list of 2D points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89116e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid2D = np.dstack([grid2x, grid2y]).reshape((-1, 2))\n",
    "grid2D.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438460b0",
   "metadata": {},
   "source": [
    "## Random fields and variograms\n",
    "\n",
    "Denote a *physical* location (i.e., a 1D/2D/3D coordinate) by $s$.\n",
    "A ***random field***, $X(s)$, is a function that takes random values at each point $s$.\n",
    "The near-synonymous term *regionalized variable* emphasizes the continuity of $s$.\n",
    "\n",
    "Random fields are commonly assumed to be ***stationary***,\n",
    "meaning that the dependence between $X(s_{1})$ and $X(s_{2})$\n",
    "for any two locations $s_1, s_2$ can be described solely in terms of their separation or,\n",
    "assuming isotropy (rotational symmetry), their distance $d = \\| s_{1} - s_{2} \\|$.\n",
    "\n",
    "For now, we assume that the mean $\\Expect X(s)$ is known and, by stationarity, constant,\n",
    "leaving the covariance as the most important descriptor.\n",
    "Again, by stationarity, this depends only on distance,\n",
    "so we can describe the full covariance of the field solely in terms of the\n",
    "\"(auto-)***covariance function***\": $$C(d) = \\mathbb{Cov}[X(s_{1}), X(s_{2})] \\,.$$\n",
    "<details style=\"border: 1px solid #aaaaaa; border-radius: 4px; padding: 0.5em 0.5em 0;\">\n",
    "  <summary style=\"font-weight: normal; font-style: italic; margin: -0.5em -0.5em 0; padding: 0.5em;\">\n",
    "    Geostatistics usually works with a reformulation of $C(d)$ called \"(semi-)<strong>variogram\"</strong>,\n",
    "    which flips the graph of $C(d)$ on its head ... (optional reading üîç):\n",
    "    $$\\gamma(d) = C(0) - C(d)$$\n",
    "  </summary>\n",
    "\n",
    "  Not all functions are valid variograms; they must evidently be non-negative,\n",
    "  have the value $0$ at $d=0$ (why?),\n",
    "  and result in positive-definite covariance matrices.\n",
    "  A sufficient condition is that the corresponding covariance is convex and decays\n",
    "  (but does not reach) toward zero at infinity.\n",
    "  However, covariance functions that oscillate around zero (Mat√©rn)\n",
    "  or have finite support (triangular) also exist.\n",
    "\n",
    "  A wider definition of the variogram also exists:\n",
    "  half (whence the optional \"semi\" prefix) the variance of the increment, i.e. of $X(s_{1}) - X(s_{2})$.\n",
    "  This definition only requires second-order stationarity of the *increments*,\n",
    "  which is a weaker requirement called \"intrinsic stationarity\",\n",
    "  and does not require or imply the existence of a covariance function $C(d)$.\n",
    "  Thus, it can be used to estimate infinite-variance (heavy-tail or Brownian-motion-like) processes,\n",
    "  but only if the method constrains the sum of its weights to one,\n",
    "  making the linear combination of the estimate \"authorized/allowable.\"\n",
    "  This condition coincides with that imposed by an unknown mean\n",
    "  and does not represent a severe restriction.\n",
    "  - - -\n",
    "</details>\n",
    "\n",
    "Turning things around, we can start by prescribing a *model* variogram (i.e., covariance)\n",
    "that we believe the random field has.\n",
    "Some of the most commonly used examples are implemented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccf721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vg_models = {\n",
    "    \"expo\":      lambda d: 1 - np.exp(-d),\n",
    "    \"Gauss\":     lambda d: 1 - np.exp(-(d) ** 2),\n",
    "    \"Cauchy\":    lambda d: 1 - 1 / (1 + d ** 2),\n",
    "    \"triangular\":lambda d: d.clip(max=1),\n",
    "    \"linear\":    lambda d: d,      # NB: intrinsically stationary, but\n",
    "    # does not produce valid covariances ‚áí requires ordinary kriging.\n",
    "    \"quadratic\": lambda d: d**2,   # NB: valid variogram, but degenerate.\n",
    "    \"cubic\":     lambda d: d ** 3, # NB: Not a valid variogram, but\n",
    "    # a \"generalized covariance func.\" ‚áí requires order-1 intrinsic/universal kriging.\n",
    "    # NB: Not to be confused with the \"cubic\" covariance function.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82064f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variograms(model=\"Gauss\", Range=0.3, nugget=0, sill=1):\n",
    "    \"\"\"Create variogram (function) for the given parameters.\"\"\"\n",
    "    def vg(dists):\n",
    "        dists = np.asarray(dists)\n",
    "        dists = dists / Range\n",
    "        gamma = vg_models[model](dists)\n",
    "        gamma *= sill\n",
    "        gamma = np.where(dists != 0, nugget + (1 - nugget) * gamma, gamma)\n",
    "        return gamma\n",
    "    return vg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c53d9c1",
   "metadata": {},
   "source": [
    "Beware that \"Gaussian\" and the other names used here\n",
    "refer to the consequent shape of the covariance function,\n",
    "which is **not** to be confused with the probability density itself (at any given location).\n",
    "Below is a visual illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b49c807",
   "metadata": {},
   "outputs": [],
   "source": [
    "vg_params = dict(Range=(.01, 4), nugget=(0, 1, .01), sill=(0.1, 5))\n",
    "@interact(**vg_params)\n",
    "def plot_variograms(Range=0.3, nugget=0, sill=1):\n",
    "    fig, ax = plt.subplots(figsize=(6, 3))\n",
    "    ax.set_ylim(0, sill)\n",
    "    ax.set_xlim(left=0)\n",
    "    for i, model in enumerate(vg_models):\n",
    "        vg = variograms(model, Range, nugget, sill)\n",
    "        ax.plot(grid1D, vg(grid1D), lw=2, color=f\"C{i}\", label=model)\n",
    "    plt.xlabel(\"distance/lag\")\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022629eb",
   "metadata": {},
   "source": [
    "Now that we're in possession of a valid variogram,\n",
    "we can compute a covariance matrix between any sets of points as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17026cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def covar_theoretical(coords, variogram):\n",
    "    C0 = variogram(np.inf)\n",
    "    assert C0 < np.inf, \"Not a valid covariance\"\n",
    "    dists = dist_euclid(coords, coords)\n",
    "    return C0 - variogram(dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ad003b",
   "metadata": {},
   "source": [
    "We also need to be able to compute pair-wise distances for each (discretized) location of our field.\n",
    "The following computes distances between two sets (not necessarily of same length) of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbfe6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_euclid(A, B):\n",
    "    \"\"\"l2-norm between each point (row) of A and B\"\"\"\n",
    "    # Like scipy.spatial.distance.pdist(A) + `squareform` if A==B\n",
    "    if A.ndim == 1:\n",
    "        return abs(A[:, None] - B)\n",
    "    d = A[:, None, :] - B\n",
    "    d2 = np.sum(d**2, axis=-1)\n",
    "    return np.sqrt(d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3397d6d",
   "metadata": {},
   "source": [
    "## Gaussian fields\n",
    "\n",
    "<details style=\"border: 1px solid #aaaaaa; border-radius: 4px; padding: 0.5em 0.5em 0;\">\n",
    "  <summary style=\"font-weight: normal; font-style: italic; margin: -0.5em -0.5em 0; padding: 0.5em;\">\n",
    "    Gaussianity becomes even more useful when working with spatial fields ... (optional reading üîç)\n",
    "  </summary>\n",
    "  Non-Gaussian random fields, such as those with Student-t or Laplace marginals, exist,\n",
    "  but Gaussianity makes sampling and conditioning uniquely\n",
    "  tractable, even for large vectors, since everything reduces to linear algebra.\n",
    "\n",
    "  Besides, although **Markov** random fields of any dimension exist for many\n",
    "  distributions (all in the Gibbs family), the familiar **locality** of the\n",
    "  first-order autoregressive (AR(1)) process‚Äîthat the conditionals are Markov,\n",
    "  have linear expectation (w.r.t. the neighboring value), and constant variance (idem)‚Äî\n",
    "  requires Gaussianity in higher dimensions.\n",
    "  To be clear, the 1D field defined by a simple (and physically interpretable) AR(1) process\n",
    "  $x_{k+1} = a x_k + \\epsilon_k$ exhibits these properties for *any* distribution by construction.\n",
    "  But in 2D or higher-dimensional lattices, such \"directed autoregressive\" constructions become\n",
    "  anisotropic and dependent on the chosen ordering, losing stationarity.\n",
    "  Instead, the AR(1) properties can be salvaged by imposing them directly on the conditionals but,\n",
    "  as shown by Besag (1974), Gaussianity is required to consistently define a global (joint) distribution.\n",
    "  The Markov (sparse) structure also makes the sampling and conditioning even more efficient.\n",
    "  - - -\n",
    "</details>\n",
    "and so will be assumed in what follows.\n",
    "Recall that Gaussian random variables (including vectors and fields)\n",
    "are fully specified by their mean and covariance.\n",
    "Thus, once we are in possession of a covariance matrix,\n",
    "we can sample discrete fields as random vectors, just as we did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49171bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_GM, = import_from_nb(\"T2\", [\"sample_GM\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2650758c",
   "metadata": {},
   "source": [
    "Let's try it out.\n",
    "\n",
    "### 1D example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b8bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(**vg_params, model=list(vg_models), N=(1, 30), top=True)\n",
    "def sample_1D(Range=0.3, nugget=1e-2, sill=1, model=\"expo\", N=10):\n",
    "    variogram = variograms(model, Range, nugget, sill)\n",
    "    C = covar_theoretical(grid1D[:, None], variogram)\n",
    "    fields = sample_GM(C=C, N=N, rng=3000)\n",
    "\n",
    "    fig, axs = plt.subplots(figsize=(10, 4), ncols=2)\n",
    "    fig.colorbar(axs[0].matshow(C, cmap=\"inferno\", vmin=0, vmax=sill), ax=axs[0], shrink=0.8)\n",
    "    axs[1].plot(grid1D, fields);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40184795",
   "metadata": {},
   "source": [
    "#### Exc ‚Äì impact of variogram parameters\n",
    "\n",
    "Refer to both of the above interactive widgets to answer the following.\n",
    "\n",
    "- (a) What happens to the covariance matrix and the fields when `Range` $\\rightarrow \\infty$ ?\n",
    "- (b) What about `Range` $\\rightarrow 0$ ?\n",
    "- (c) Can you reproduce this using `nugget` somehow ?\n",
    "- (d) Try `nugget = 0` and `0.01`. Explain why the impact is much more noticeable\n",
    "  in the Gauss and Cauchy cases, sometimes causing errors. *Hint: These are due to numerical imprecision and can be avoided with a small `Range`.*\n",
    "- (e) How does changing `sill` affect the fields?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1db7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('variogram params')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbc0c69",
   "metadata": {},
   "source": [
    "### 2D example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e15218",
   "metadata": {},
   "outputs": [],
   "source": [
    "variogram = variograms(\"Gauss\", 1, nugget=1e-4, sill=1)\n",
    "C = covar_theoretical(grid2D, variogram)\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def62f64",
   "metadata": {},
   "source": [
    "Note that the size of the covariance matrix is the square of a single 2D field's size.\n",
    "It's interesting to inspect the covariance matrix in this 2D case,\n",
    "which has a nested structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6457e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = freshfig(\"2D covar\")\n",
    "C0 = variogram(np.inf)\n",
    "ax.matshow(C, cmap=\"inferno\", vmin=0, vmax=C0);\n",
    "ax.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7aafea",
   "metadata": {},
   "source": [
    "Now let's generate and plot some realizations of the corresponding Gaussian\n",
    "random field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444c6305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot2d(ax, field, contour=True, show_obs=True, cmap=\"PiYG\"):\n",
    "    vmin = -3.5*np.sqrt(C0)\n",
    "    vmax = +3.5*np.sqrt(C0)\n",
    "\n",
    "    ax.set(aspect=\"equal\", xticks=[0, 1], yticks=[0, 1])\n",
    "\n",
    "    if show_obs:\n",
    "        ax.plot(*obs_loc.T, \"ko\", ms=4)\n",
    "        ax.plot(*obs_loc.T, \"yo\", ms=1)\n",
    "\n",
    "    field = field.reshape(grid2x.shape)\n",
    "    if contour:\n",
    "        Œµ = 1e-12 # fudge to center colors right below 0\n",
    "        levels = np.arange(vmin - Œµ, vmax + .5 + Œµ, .5) \n",
    "        return ax.contourf(field, levels=levels, extent=(0, 1, 0, 1), extend=\"both\", cmap=cmap)\n",
    "    else:\n",
    "        return ax.pcolormesh(grid2x, grid2y, field, shading=\"auto\", vmin=vmin, vmax=vmax, cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128e0d7e",
   "metadata": {},
   "source": [
    "Beware that `contourf` (which works with regularly spaced grids) uses simple bilinear interpolation\n",
    "between nodes, making the fields appear higher resolution than they actually are.\n",
    "Therefore, you may want to use `contour=False` instead to view the actual grid \"pixels\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df29848",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(**vg_params, model=list(vg_models))\n",
    "def sample_2D(Range=0.3, nugget=1e-2, model=\"Gauss\"):\n",
    "    fig, axs = plt.subplots(figsize=(8, 4), nrows=2, ncols=4, sharex=True, sharey=True)\n",
    "    C = covar_theoretical(grid2D, variograms(model, Range, nugget, sill=1))\n",
    "    fields = sample_GM(C=C, N=len(axs.ravel()), rng=3000)\n",
    "\n",
    "    for ax, field in zip(axs.ravel(), fields.T):\n",
    "        cb = plot2d(ax, field, show_obs=False)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.colorbar(cb, ax=axs.ravel().tolist(), shrink=0.8);\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a35b65b",
   "metadata": {},
   "source": [
    "## Estimation\n",
    "\n",
    "### Problem\n",
    "\n",
    "For our estimation target ($\\vect{x}$), we use the default covariance defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c7da6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = sample_GM(C=C, N=1).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ae9551",
   "metadata": {},
   "source": [
    "For our observations, we randomly pick some grid locations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15a0ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nObs = 10\n",
    "rnd.seed(3000)\n",
    "obs_idx = rnd.choice(len(grid2D), nObs, replace=False)\n",
    "obs_loc = grid2D[obs_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82e76e9",
   "metadata": {},
   "source": [
    "...such that the observation values ($\\vect{y}$) are simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714bf03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = truth[obs_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08e120b",
   "metadata": {},
   "source": [
    "However (unlike classical bilinear/bicubic interpolation),\n",
    "the following methods work perfectly well with irregular observations,\n",
    "irregular grids, or different orderings of the flattened grid nodes.\n",
    "\n",
    "### Methods\n",
    "\n",
    "The following is a register of fields to include in plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddc6a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "estims = dict(Truth=truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0891244",
   "metadata": {},
   "source": [
    "Each time you (have tried to) implement one of the following methods,\n",
    "come back up here and re-run this next cell\n",
    "to visually compare and assess your method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deca3b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = freshfig(figsize=(8, 4), squeeze=False, nrows=2, ncols=len(estims), sharex=True, sharey=True)\n",
    "axs[0, 0].set_ylabel(\"Estimate\");\n",
    "axs[1, 0].set_ylabel(\"Errors\");\n",
    "\n",
    "for name, ax1, ax2 in zip(estims, *axs):\n",
    "    x = estims[name]\n",
    "    ax1.set(title=name)\n",
    "    cb1 = plot2d(ax1, x)\n",
    "    cb2 = plot2d(ax2, x - truth, cmap=\"seismic\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.colorbar(cb1, ax=axs[0].tolist(), shrink=0.8);\n",
    "fig.colorbar(cb2, ax=axs[1].tolist(), shrink=0.8);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f673b5",
   "metadata": {},
   "source": [
    "Pre-compute some objects that see repeated use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5370062",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists_yy = dist_euclid(obs_loc, obs_loc)\n",
    "dists_xy = dist_euclid(grid2D, obs_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c00359",
   "metadata": {},
   "source": [
    "The cells below contain snippets of different spatial interpolation methods\n",
    "followed by a cell that plots the interpolants.\n",
    "Complete the code snippets.\n",
    "\n",
    "#### Exc: Nearest neighbour interpolation\n",
    "\n",
    "Implement the method [(Wikipedia)](https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation)\n",
    "by adjusting the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e8cf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_obs = np.zeros(len(grid2D), dtype=int)  ### FIX THIS ###\n",
    "estims[\"Nearest-n.\"] = observations[nearest_obs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9c012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('nearest neighbour interp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f2d7df",
   "metadata": {},
   "source": [
    "#### Exc: Inverse distance weighting\n",
    "\n",
    "Implement the method [(Wikipedia)](https://en.wikipedia.org/wiki/Inverse_distance_weighting).\\\n",
    "*Hint: The `errstate` context silences warnings due to division by 0 (whose special case is treated further down).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21fc64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exponent = 3\n",
    "with np.errstate(invalid='ignore', divide='ignore'):\n",
    "    weights = np.zeros_like(dists_xy)  ### FIX THIS ###    \n",
    "estims[\"Inv-dist.\"] = weights @ observations # Apply weights\n",
    "estims[\"Inv-dist.\"][obs_idx] = observations # Fix singularities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d22ec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('inv-dist weight interp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e46ea4",
   "metadata": {},
   "source": [
    "<a name='Exc:-\"simple\"-kriging'></a>\n",
    "\n",
    "#### Exc: \"simple\" kriging\n",
    "\n",
    "Consider the random value $x(s)$ of the field at a single location, and drop the $s$.\n",
    "Kriging minimizes the mean square error\n",
    "$\\text{MSE} = \\Expect (\\hat{x} - x)^2$ among all linear estimators\n",
    "of the form $\\hat{x} = \\vect{w}\\tr \\vect{y}$ that are unbiased.\n",
    "<details style=\"border: 1px solid #aaaaaa; border-radius: 4px; padding: 0.5em 0.5em 0;\">\n",
    "  <summary style=\"font-weight: normal; font-style: italic; margin: -0.5em -0.5em 0; padding: 0.5em;\">\n",
    "    Thus, kriging seeks the best linear unbiased *predictor* (BLUP),\n",
    "    which is similar but different from the BLUE (optional reading...)\n",
    "  </summary>\n",
    "\n",
    "  Note that unbiasedness means the MSE can also be termed the error \"variance\".\n",
    "  Thus, the metric may easily be confused with the variance of the estimator alone,\n",
    "  which is what the BLUE (Best Linear Unbiased Estimator) of classical Gauss-Markov theory minimizes,\n",
    "  but which is not suitable for the current setting.\n",
    "  To wit, the BLUE for linear regression is derived to estimate a fixed (but unknown) parameter,\n",
    "  while kriging tries to *predict* a *random* value $X$ of mean $\\mu$\n",
    "  (one conventionally \"estimates\" fixed effects and \"predicts\" random effects).\n",
    "\n",
    "  This additional randomness (i.e., in the estimand as well as in the observations)\n",
    "  makes the criterion of unbiasedness much more bland and less influential than in the case of the BLUE.\n",
    "  For instance, if the mean is known (effectively zero), then the BLUE is simply zero\n",
    "  (it is unbiased and has zero variance)!\n",
    "  Or if the mean is unknown, unbiasedness merely imposes that the weights sum to one,\n",
    "  which is entirely independent of the observation configuration.\n",
    "\n",
    "  In an effort to make the unbiasedness criterion more informative,\n",
    "  as for the BLUE, one could condition on the random (but realized) $x(s)$.\n",
    "  However, the resulting expectation at the observation points all simply become $x(s)$,\n",
    "  and the same would happen for any other location $s'$, meaning that unbiasedness\n",
    "  remains highly uninformative.\n",
    "  Indeed, the resulting weights are $\\frac{\\vect{1}\\tr C^{-1}}{\\vect{1}\\tr C^{-1} \\vect{1}}$,\n",
    "  which does not depend in any way on the location of $x$.\n",
    "\n",
    "  Alternatively, one could try to remove the other source of randomness,\n",
    "  namely that of $Y$, by conditioning on it to estimate the *posterior* of $X$.\n",
    "  This works well enough but requires Gaussianity assumptions for tractability\n",
    "  (otherwise, the posterior mean, e.g., is not a definite quantity).\n",
    "  Further details are provided below.\n",
    "  - - -\n",
    "</details>\n",
    "\n",
    "For now, assume the mean is constant in space and known.\n",
    "Since it is easy to subtract (and later re-include) the mean from both $x$ and the data $\\vect{y}$,\n",
    "we simply assume the mean is zero.\n",
    "Thus, $\\Expect \\vect{y} = \\vect{0}$ and $\\Expect \\hat{x} = 0$ for any weights $\\vect{w}$,\n",
    "and so $\\hat{x}$ is inherently unbiased.\n",
    "Meanwhile,\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\text{MSE}\n",
    "  % \\Expect \\big( \\hat{x} - x \\big)^2\n",
    "  &= \\Expect \\big( \\vect{w}\\tr \\vect{y} - x \\big)^2 \\\\\n",
    "  % &= \\Expect \\big( \\vect{w}\\tr \\vect{y} \\vect{y}\\tr \\vect{w} - 2 x \\vect{y}\\tr \\vect{w} + x^2 \\big) \\\\\n",
    "  &= \\vect{w}\\tr \\Expect \\big( \\vect{y} \\vect{y}\\tr \\big) \\vect{w}\n",
    "     - 2 \\Expect \\big( x \\vect{y}\\tr \\big) \\vect{w}\n",
    "     + \\Expect \\big( x^2 \\big) \\\\\n",
    "  &= \\vect{w}\\tr \\mat{C}_{yy} \\vect{w}\n",
    "     - 2 \\vect{c}_{xy}\\tr \\vect{w}\n",
    "     + C(0) \\,,\n",
    "\\end{align}\n",
    "$$\n",
    "whose minimum can be found by setting the derivative with respect to $\\vect{w}$ to zero,\n",
    "yielding\n",
    "$\\vect{w}_{\\text{SK}} = \\mat{C}_{yy}^{-1} \\, \\vect{c}_{xy}$\n",
    "and the corresponding simple kriging (SK) estimate\n",
    "$\\hat{x}_{\\text{SK}}$.\n",
    "\n",
    "These weights can be identified with those from the\n",
    "[regression perspective on the Kalman filter (T5)](T5%20-%20Multivariate%20Kalman%20filter.ipynb#Regression-perspective),\n",
    "which we derived as the posterior/conditional mean of a joint Gaussian distribution\n",
    "and simplified the use of Bayes' rule.\n",
    "This is how it's done for GP regression, and [Krige (1951)](#References) was also aware of this.\n",
    "We also [recall (T3)](T3%20-%20Bayesian%20inference.ipynb#Exc-(optional)-‚Äì-optimalities) that linear regression is the BLUE.\n",
    "Thus, we are effectively performing linear regression at any/all locations $s$,\n",
    "of which there are infinitely (uncountably) many.\n",
    "This reflects the fact that kriging is a so-called non-parametric method\n",
    "and that the variogram (seen as a kernel) has infinite rank.\n",
    "\n",
    "The fact that kriging can be derived as a posterior Gaussian distribution\n",
    "makes it evident that kriging also provides an uncertainty estimate,\n",
    "namely the posterior covariance matrix. This can also be derived from the linear, unbiased MSE perspective\n",
    "and is a major benefit compared to the other interpolation methods we tested above.\n",
    "\n",
    "Yet another perspective on kriging is that of **radial basis function (RBF) interpolation**,\n",
    "where it is derived by fitting $N$ radial functions to interpolate $N$ data points (observations).\n",
    "Ultimately, the various kriging methods can all be written as\n",
    "$\\vect{y}\\tr \\mat{C}^{-1} \\vect{d}$, where $\\vect{y}$ is the observations\n",
    "and the product of the last two factors is seen as the \"weights\" initially solved for.\n",
    "With RBFs, the parentheses (ordering of computations) shift, and the \"weights\" solved for are given\n",
    "by the first two factors. This perspective is termed \"dual\" in kriging.\n",
    "\n",
    "Implement the method [(Wikipedia)](https://en.wikipedia.org/wiki/Kriging#Simple_kriging).  \n",
    "*Hint: You may use `sla.inv`, but `sla.solve` is better, and `sla.lstsq` is even better.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab70a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "C0 = variogram(np.inf)\n",
    "### FIX THIS ###\n",
    "covar_yy = ...\n",
    "cross_xy = ...\n",
    "weights = np.zeros_like(dists_xy) # cross_xy \"/\" covar_yy\n",
    "estims[\"S.Kriging\"] = weights @ observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5648560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('Simple kriging')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e19eb6",
   "metadata": {},
   "source": [
    "### More kriging\n",
    "\n",
    "The above 2D case used the same variogram that generated the truth.\n",
    "But in practice, we do not know this variogram (whose very existence is a theoretical assumption),\n",
    "so it must also be estimated.\n",
    "To that end, let's gain some understanding of the variogram's impact\n",
    "on the resulting *estimate*. We focus on the 1D problem because its illustrations are clearer.\n",
    "\n",
    "For our estimation target, we use $x(s) = \\sin (s^2)$ for $s \\in [0, L]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b838f4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def true(s):\n",
    "    return np.sin(s**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d2013f",
   "metadata": {},
   "source": [
    "The observations ($\\vect{y}$) are taken at the following (configurable) locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c07f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_obs_loc(nObs, spacing, L):\n",
    "    return L/2 + spacing * np.linspace(-L/2, L/2, nObs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c221d",
   "metadata": {},
   "source": [
    "Visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea61cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(**vg_params, vg_model=list(vg_models), L=(1, 10, 0.1), nObs=(1, 100, 1), spacing=(0.01, 1))\n",
    "def kriging_1d(L=4, nObs=6, spacing=0.5, vg_model=\"expo\", Range=0.3, nugget=0):\n",
    "    # Experiment setup\n",
    "    grid = np.linspace(0, L, 1001)\n",
    "    truth = true(grid)\n",
    "    obs_loc = gen_obs_loc(nObs, spacing, L)\n",
    "    observs = true(obs_loc)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.plot(grid, truth, \"-k\", label=\"truth\")\n",
    "    ax.plot(obs_loc, observs, 'ok', label='data', ms=12, zorder=9)\n",
    "\n",
    "    # Kriging setup\n",
    "    vg = variograms(vg_model, Range=Range, nugget=nugget)\n",
    "    dists_yy = dist_euclid(obs_loc, obs_loc)\n",
    "    dists_xy = dist_euclid(grid, obs_loc)\n",
    "\n",
    "    with nonchalance():\n",
    "        mu = 0\n",
    "        interp = mu + simple_kriging(vg, dists_xy, dists_yy, observs - mu)\n",
    "        ax.plot(grid, interp, 'C1', label=\"S-krig\", lw=5)\n",
    "        ax.axhline(mu, c=\"k\", lw=0.5)\n",
    "\n",
    "    with nonchalance():\n",
    "        interp = ordinary_kriging(vg, dists_xy, dists_yy, observs) \n",
    "        ax.plot(grid, interp, 'C2', label=\"O-krig\", lw=4)\n",
    "\n",
    "    with nonchalance():\n",
    "        regressors = [np.ones(nObs), obs_loc]\n",
    "        regressands = [np.ones(len(grid)), grid]\n",
    "        interp = universal_kriging(vg, dists_xy, dists_yy, observs, regressors, regressands)\n",
    "        ax.plot(grid, interp, 'C3', label=\"U-krig\", lw=3)\n",
    "\n",
    "        from scipy.interpolate import CubicSpline, PchipInterpolator, Akima1DInterpolator\n",
    "        ax.plot(grid, CubicSpline(obs_loc, observs, bc_type=\"natural\")(grid), 'C4', label=\"N-spline\")\n",
    "        # ax.plot(grid, Akima1DInterpolator(obs_loc, observs)(grid), 'C6', label=\"Akima spline\")\n",
    "        # ax.plot(grid, PchipInterpolator(obs_loc, observs)(grid), 'C5', label=\"PCHIP spline\")\n",
    "\n",
    "    ax.legend(loc='lower left', ncols=2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be161864",
   "metadata": {},
   "source": [
    "#### Exc: Impact of variogram on kriged fields\n",
    "\n",
    "Add the simple kriging interpolant by copy-pasting your solution from above into the function below.\n",
    "*Hint: Move (and de-indent) code out of the `nonchalance()` context if you want to be\n",
    "able to view error messages (to debug errors).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ce9578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_kriging(vg, dists_xy, dists_yy, observations):\n",
    "    field_estimate = ...\n",
    "    return field_estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ab959b",
   "metadata": {},
   "source": [
    "- (a) What happens when `Range` $\\to 0$ ? What about $\\to \\infty$? What about intermediate values?\n",
    "- (b) Try the same with Gauss/Cauchy variograms. What is the main difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d944b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('Interpolant = f(Variogram)', 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd6ccdd",
   "metadata": {},
   "source": [
    "#### Exc (optional) ‚Äì Ordinary kriging\n",
    "\n",
    "Let us do away with the assumption that the mean of the field is known,\n",
    "all the while retaining the assumption that it is constant in space.\n",
    "The resulting method is called ordinary kriging.\n",
    "In this case, unbiasedness of $\\hat{x} = \\vect{w}\\tr \\vect{y}$ requires that the weights sum to one.\n",
    "This can be imposed on the MSE minimization using a Lagrange multiplier $\\lambda$,\n",
    "yielding the augmented system to solve:\n",
    "$$ \\begin{pmatrix} \\mat{C}_{yy} & \\vect{1} \\\\ \\vect{1}\\tr & 0 \\end{pmatrix} \\begin{pmatrix} \\vect{w} \\\\ \\lambda \\end{pmatrix}\n",
    "= \\begin{pmatrix} \\vect{c}_{xy} \\\\ 1 \\end{pmatrix} \\,.$$\n",
    "\n",
    "Ordinary kriging can be reproduced by separately kriging the mean and the resulting residual field [(Wackernagel, 2013)](#References).\n",
    "\n",
    "It is also important to note that the MSE ‚Äì by its resemblance to intrinsic increments ‚Äì\n",
    "can be expressed in terms of variograms even when the covariance function does not exist,\n",
    "and the same goes for the method of OK. Somewhat surprisingly, the linear system to be solved looks exactly the same, except with covariances replaced by variograms.\n",
    "In this case, the unbiasedness requirement also manifests as a requirement\n",
    "that any variance expressed as a linear combination of variograms be positive.\n",
    "In the RBF perspective, this requirement guarantees the existence and uniqueness of the solution to the OK equations.\n",
    "\n",
    "- (a) Implement OK below *using variograms*.\\\n",
    "  Before each of the following questions, re-run the above plotting cell to reset the parameters to their written defaults.\n",
    "- (b) Which variogram model(s) does OK \"unlock\"?\n",
    "- (c) What is peculiar about the interpolant for each of the new variograms?\n",
    "- (d) Set `nObs = 1`. What is the difference between the SK and OK interpolants?\n",
    "- (e) Set `nObs = 2`. What is the difference between the SK and OK interpolants? Explore with a few different `spacing` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b975a812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('Ordinary kriging', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247dee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordinary_kriging(vg, dists_xy, dists_yy, observations):\n",
    "    field_estimate = ...\n",
    "    return field_estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bd550e",
   "metadata": {},
   "source": [
    "#### Exc (optional) ‚Äì Universal kriging\n",
    "\n",
    "In addition to the flat/constant *feature* $\\vect{1}$,\n",
    "one can add additional regressors in a similar fashion to how the OK system augments the SK system of equations,\n",
    "producing universal kriging (UK).\n",
    "If the features are monomials (evaluated at the locations of $x$ and $\\vect{y}$),\n",
    "then they may well be called *trends*,\n",
    "and the UK method is called intrinsic kriging (IK).\n",
    "Their order $+1$ defines the maximum allowable order of the *generalized* covariance function\n",
    "(for the `cubic` \"variogram\" above, we need to include linear trends),\n",
    "again rendering the augmented linear system well-posed.\n",
    "\n",
    "- (a) Implement UK below and answer the following.\n",
    "- (b) Can you make the UK interpolant reproduce `CubicSpline`?\n",
    "  What happens when you vary `Range`?\n",
    "- (c) Although theoretically suboptimal\n",
    "  (since it assumes a constant/flat mean, and does not support generalized covariance functions),\n",
    "  the OK system is still solvable with a cubic variogram.\n",
    "  Can you find experimental control settings that show the OK estimate does not\n",
    "  exactly reproduce the spline solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e052bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('Universal kriging', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8c8fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def universal_kriging(vg, dists_xy, dists_yy, observations, regressors, regressands):\n",
    "    field_estimate = ...\n",
    "    return field_estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48111d1",
   "metadata": {},
   "source": [
    "Further generalizations include co-kriging (vector-valued fields).\n",
    "Also, since the kriging mean is smoother than any realization (has lower variance,\n",
    "which favors but does not guarantee spatial/spectral smoothness),\n",
    "conditional generation (simulation) of fields is frequently used.\n",
    "\n",
    "## Summary\n",
    "\n",
    "Random, (geo)spatial fields are often modeled by their (auto-)correlation function or\n",
    "‚Äì more generally ‚Äì the (semi-)variogram.\n",
    "Covariances also form the basis of a family of spatial interpolation and approximation\n",
    "methods known as kriging.\n",
    "\n",
    "### Next: [T8 - Monte-Carlo & ensembles](T8%20-%20Monte-Carlo%20%26%20ensembles.ipynb)\n",
    "\n",
    "<a name=\"References\"></a>\n",
    "\n",
    "### References\n",
    "\n",
    "<!--\n",
    "\n",
    "@book{chiles2009geostatistics,\n",
    "  title={Geostatistics: Modeling Spatial Uncertainty},\n",
    "  author={Chil{\\`e}s, J.P. and Delfiner, P.},\n",
    "  isbn={9780470317839},\n",
    "  series={Wiley Series in Probability and Statistics},\n",
    "  url={https://books.google.no/books?id=tZl07WdjYHgC},\n",
    "  year={2009},\n",
    "  publisher={Wiley}\n",
    "}\n",
    "\n",
    "@book{wackernagel2013multivariate,\n",
    "  title={Multivariate Geostatistics: An Introduction with Applications},\n",
    "  author={Wackernagel, H.},\n",
    "  isbn={9783662052945},\n",
    "  year={2013},\n",
    "  publisher={Springer Berlin Heidelberg}\n",
    "}\n",
    "\n",
    "@article{krige1951statistical,\n",
    "  title={A statistical approach to some basic mine valuation problems on the Witwatersrand},\n",
    "  author={Krige, Daniel G},\n",
    "  journal={Journal of the Southern African Institute of Mining and Metallurgy},\n",
    "  volume={52},\n",
    "  number={6},\n",
    "  pages={119--139},\n",
    "  year={1951},\n",
    "  publisher={Southern African Institute of Mining and Metallurgy}\n",
    "}\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,nb_mirrors//py:light,nb_mirrors//md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
