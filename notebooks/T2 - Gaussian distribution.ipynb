{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f94593",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote = \"https://raw.githubusercontent.com/nansencenter/DA-tutorials\"\n",
    "!wget -qO- {remote}/master/notebooks/resources/colab_bootstrap.sh | bash -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb7f04e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from resources import show_answer, interact\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff70496",
   "metadata": {},
   "source": [
    "We start by reviewing the most useful of probability distributions.\n",
    "# T2 - The Gaussian (Normal) distribution\n",
    "$\n",
    "% ######################################## Loading TeX (MathJax)... Please wait ########################################\n",
    "\\newcommand{\\Reals}{\\mathbb{R}} \\newcommand{\\Expect}[0]{\\mathbb{E}} \\newcommand{\\NormDist}{\\mathscr{N}} \\newcommand{\\DynMod}[0]{\\mathscr{M}} \\newcommand{\\ObsMod}[0]{\\mathscr{H}} \\newcommand{\\mat}[1]{{\\mathbf{{#1}}}} \\newcommand{\\bvec}[1]{{\\mathbf{#1}}} \\newcommand{\\trsign}{{\\mathsf{T}}} \\newcommand{\\tr}{^{\\trsign}} \\newcommand{\\ceq}[0]{\\mathrel{≔}} \\newcommand{\\xDim}[0]{D} \\newcommand{\\supa}[0]{^\\text{a}} \\newcommand{\\supf}[0]{^\\text{f}} \\newcommand{\\I}[0]{\\mat{I}} \\newcommand{\\K}[0]{\\mat{K}} \\newcommand{\\bP}[0]{\\mat{P}} \\newcommand{\\bH}[0]{\\mat{H}} \\newcommand{\\bF}[0]{\\mat{F}} \\newcommand{\\R}[0]{\\mat{R}} \\newcommand{\\Q}[0]{\\mat{Q}} \\newcommand{\\B}[0]{\\mat{B}} \\newcommand{\\C}[0]{\\mat{C}} \\newcommand{\\Ri}[0]{\\R^{-1}} \\newcommand{\\Bi}[0]{\\B^{-1}} \\newcommand{\\X}[0]{\\mat{X}} \\newcommand{\\A}[0]{\\mat{A}} \\newcommand{\\Y}[0]{\\mat{Y}} \\newcommand{\\E}[0]{\\mat{E}} \\newcommand{\\U}[0]{\\mat{U}} \\newcommand{\\V}[0]{\\mat{V}} \\newcommand{\\x}[0]{\\bvec{x}} \\newcommand{\\y}[0]{\\bvec{y}} \\newcommand{\\z}[0]{\\bvec{z}} \\newcommand{\\q}[0]{\\bvec{q}} \\newcommand{\\br}[0]{\\bvec{r}} \\newcommand{\\bb}[0]{\\bvec{b}} \\newcommand{\\bx}[0]{\\bvec{\\bar{x}}} \\newcommand{\\by}[0]{\\bvec{\\bar{y}}} \\newcommand{\\barB}[0]{\\mat{\\bar{B}}} \\newcommand{\\barP}[0]{\\mat{\\bar{P}}} \\newcommand{\\barC}[0]{\\mat{\\bar{C}}} \\newcommand{\\barK}[0]{\\mat{\\bar{K}}} \\newcommand{\\D}[0]{\\mat{D}} \\newcommand{\\Dobs}[0]{\\mat{D}_{\\text{obs}}} \\newcommand{\\Dmod}[0]{\\mat{D}_{\\text{obs}}} \\newcommand{\\ones}[0]{\\bvec{1}} \\newcommand{\\AN}[0]{\\big( \\I_N - \\ones \\ones\\tr / N \\big)}\n",
    "$\n",
    "## Probability \n",
    "Probability of an event is defined as\n",
    "\\begin{equation}\n",
    "  \\label{eq:probability_as_rel_freq}\n",
    "  P(\\text{event}) = \\frac{\\text{\\# favorable outcomes}}{\\text{\\# possible outcomes}}\n",
    "\\end{equation}\n",
    "although the formal definition goes back to James Bernoulli (1713)\n",
    "As stated by Laplace (Théorie Analytique des Probabilités (1812):\n",
    "\n",
    "      The Probability for an event is the ratio of the number of cases favorable to it, to the number of all\n",
    "      cases possible when nothing leads us to expect that any one of these cases should occur more than\n",
    "      any other, which renders them, for us, equally possible.\n",
    "\n",
    "## The univariate (a.k.a. 1-dimensional, scalar) case\n",
    "Consider the Gaussian random variable $x \\sim \\NormDist(\\mu, \\sigma^2)$.  \n",
    "Its probability density function (**pdf**),\n",
    "$\n",
    "p(x) = \\NormDist(x \\mid \\mu, \\sigma^2)\n",
    "$ for $x \\in (-\\infty, +\\infty)$,\n",
    "is given by\n",
    "$$\\begin{align}\n",
    "\\NormDist(x \\mid \\mu, \\sigma^2) = (2 \\pi \\sigma^2)^{-1/2} e^{-(x-\\mu)^2/2 \\sigma^2} \\,. \\tag{G1}\n",
    "\\end{align}$$\n",
    "\n",
    "Run the cell below to define a function to compute the pdf (G1) using the `scipy` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3be2918",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def pdf_G1(x, mu, sigma2):\n",
    "    \"Univariate Gaussian pdf\"\n",
    "    pdf_values = sp.stats.norm.pdf(x, loc=mu, scale=np.sqrt(sigma2))\n",
    "    return pdf_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069911f0",
   "metadata": {},
   "source": [
    "Computers typically represent functions *numerically* by their values on a grid\n",
    "of points (nodes), an approach called ***discretisation***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80586aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = -20, 20\n",
    "N = 201                         # num of grid points\n",
    "grid1d = np.linspace(*bounds,N) # grid\n",
    "dx = grid1d[1] - grid1d[0]      # grid spacing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e40b3ce",
   "metadata": {},
   "source": [
    "Feel free to come back here later and change the grid resolution to see how\n",
    "it affects the cells below (upon re-running them).\n",
    "\n",
    "The following code plots the Gaussian pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0442e503",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "hist = []\n",
    "@interact(mu=bounds, sigma=(.1, 10, 1))\n",
    "def plot_pdf(mu=0, sigma=5):\n",
    "    plt.figure(figsize=(6, 2))\n",
    "    colors = plt.get_cmap('hsv')([(k-len(hist))%9/9 for k in range(9)])\n",
    "    plt.xlim(*bounds)\n",
    "    plt.ylim(0, .2)\n",
    "    hist.insert(0, pdf_G1(grid1d, mu, sigma**2))\n",
    "    for density_values, color in zip(hist, colors):\n",
    "        plt.plot(grid1d, density_values, c=color)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0059c2",
   "metadata": {},
   "source": [
    "#### Exc -- parameter influence\n",
    "Play around with `mu` and `sigma` to answer these questions:\n",
    " * How does the pdf curve change when `mu` changes? Options (several might be right/wrong)\n",
    "   1. It changes the curve into a uniform distribution.\n",
    "   1. It changes the width of the curve.\n",
    "   1. It shifts the peak of the curve to the left or right.\n",
    "   1. It changes the height of the curve.\n",
    "   1. It transforms the curve into a binomial distribution.\n",
    "   1. It makes the curve wider or narrower.\n",
    "   1. It modifies the skewness (asymmetry) of the curve.\n",
    "   1. It causes the curve to expand vertically while keeping the width the same.\n",
    "   1. It translates the curve horizontally.\n",
    "   1. It alters the kurtosis (peakedness) of the curve.\n",
    "   1. It rotates the curve around the origin.\n",
    "   1. It makes the curve a straight line.\n",
    " * How does the pdf curve change when you increase `sigma`?  \n",
    "   Refer to the same options as previous question.\n",
    " * In a few words, describe the shape of the Gaussian pdf curve.\n",
    "   Does this ring a bell? *Hint: it should be clear as a bell!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d266ac66",
   "metadata": {},
   "source": [
    "**Exc -- Implementation:** Change the implementation of `pdf_G1` so as to not use `scipy`, but your own code (using `numpy` only). Re-run all of the above cells and check that you get the same plots as before.  \n",
    "*Hint: `**` is the exponentiation/power operator, but $e^x$ is more efficiently computed with `np.exp(x)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e1106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('pdf_G1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b6d541",
   "metadata": {},
   "source": [
    "**Exc -- Derivatives:** Recall $p(x) = \\NormDist(x \\mid \\mu, \\sigma^2)$ from eqn (G1).  \n",
    "Use pen, paper, and calculus to answer the following questions,  \n",
    "which derive some helpful mnemonics about the distribution.\n",
    "\n",
    " * (i) Find $x$ such that $p(x) = 0$.\n",
    " * (ii) Where is the location of the **mode (maximum)** of the density?  \n",
    "   I.e. find $x$ such that $\\frac{d p}{d x}(x) = 0$.\n",
    "   *Hint: begin by writing $p(x)$ as $c e^{- J(x)}$ for some $J(x)$.*\n",
    " * (iii) Where is the **inflection point**? I.e. where $\\frac{d^2 p}{d x^2}(x) = 0$.\n",
    " * (iv) *Optional*: Some forms of *sensitivity analysis* (typically for non-Gaussian $p$) consist in estimating/approximating the Hessian, i.e. $\\frac{d^2 \\log p}{d x^2}$. Explain what this has to do with *uncertainty quantification*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e3d08a",
   "metadata": {},
   "source": [
    "#### Exc (optional) -- Change of variables, Expectation\n",
    "Let $z = \\phi(x)$ for some monotonic function $\\phi$,\n",
    "and $p_x$ and $p_z$ be their probability density functions (pdf).\n",
    "- (a): Show that $p_z(z) = p_x\\big(\\phi^{-1}(z)\\big) \\frac{1}{|\\phi'(z)|}$,\n",
    "- (b): The **expected value** of a random variable is its long (infinite)-run average value. Formally, the expectation of $x$ is $\\Expect[x] ≔ \\int  x \\, p_x(x) \\, d x $, where ***the domain of integration is over all values of $x$***\n",
    "  (i.e. from $-\\infty$ to $+\\infty$ in the case of Gaussian distributions).\n",
    "  Show that you don't need to derive the density of $z$ in order to compute its expectation, i.e. that\n",
    "  $$ \\Expect[z] = \\int  \\phi(x) \\, p_x(x) \\, d x ≕ \\Expect[\\phi(x)] \\,,$$\n",
    "  *Hint: while the proof is convoluted, the result itself is [pretty intuitive](https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92777530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('CVar in proba')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8311df4c",
   "metadata": {},
   "source": [
    "#### Exc (optional) -- Integrals\n",
    "Recall $p(x) = \\NormDist(x \\mid \\mu, \\sigma^2)$ from eqn (G1). Abbreviate it using $c = (2 \\pi \\sigma^2)^{-1/2}$.  \n",
    "Use pen, paper, and calculus to show that\n",
    " - (i) the first parameter, $\\mu$, indicates its **mean**, i.e. that $$\\mu = \\Expect[x] \\,.$$\n",
    "   *Hint: you can rely on the result of (iii)*\n",
    " - (ii) the second parameter, $\\sigma^2>0$, indicates its **variance**,\n",
    "   i.e. that $$\\sigma^2 = \\mathbb{Var}(x) \\mathrel{≔} \\Expect[(x-\\mu)^2] \\,.$$\n",
    "   *Hint: use $x^2 = x x$ to enable integration by parts.*\n",
    " - (iii) $E[1] = 1$,  \n",
    "   thus proving that (G1) indeed uses the right normalising constant.  \n",
    "   *Hint: Neither Bernoulli and Laplace managed this,\n",
    "   until Gauss did by first deriving $(E[1])^2$.  \n",
    "   For more (visual) help, watch [3Blue1Brown](https://www.youtube.com/watch?v=cy8r7WSuT1I&t=3m52s).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5470edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('Gauss integrals')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce40fa45",
   "metadata": {},
   "source": [
    "**Exc -- The uniform pdf**:\n",
    "Below is the pdf of the [uniform/flat/box distribution](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous))\n",
    "for a given mean and variance.\n",
    "- Replace `_G1` by `_U1` in the code generating the above interactive plot.\n",
    "- Why are the walls (ever so slightly) inclined?\n",
    "- Write your own implementation below, and check that it reproduces the `scipy` version already in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c1e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_U1(x, mu, sigma2):\n",
    "    a = mu - np.sqrt(3*sigma2)\n",
    "    b = mu + np.sqrt(3*sigma2)\n",
    "    pdf_values = sp.stats.uniform(loc=a, scale=(b-a)).pdf(x)\n",
    "    # Your own implementation:\n",
    "    # height = ...\n",
    "    # pdf_values = height * np.ones_like(x)\n",
    "    # pdf_values[x<a] = ...\n",
    "    # pdf_values[x>b] = ...\n",
    "    return pdf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e09bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('pdf_U1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28223c0",
   "metadata": {},
   "source": [
    "## The multivariate (i.e. vector) case\n",
    "Here's the pdf of the *multivariate* Gaussian (for any dimension $\\ge 1$):\n",
    "$$\\begin{align}\n",
    "\\NormDist(\\x \\mid  \\mathbf{\\mu}, \\mathbf{\\Sigma})\n",
    "&=\n",
    "|2 \\pi \\mathbf{\\Sigma}|^{-1/2} \\, \\exp\\Big(-\\frac{1}{2}\\|\\x-\\mathbf{\\mu}\\|^2_\\mathbf{\\Sigma} \\Big) \\,, \\tag{GM}\n",
    "\\end{align}$$\n",
    "where $|.|$ represents the matrix determinant,  \n",
    "and $\\|.\\|_\\mathbf{W}$ represents a weighted 2-norm: $\\|\\x\\|^2_\\mathbf{W} = \\x^T \\mathbf{W}^{-1} \\x$.  \n",
    "*PS: The norm (quadratic form) is invariant to antisymmetry in the weight matrix,\n",
    "so we take $\\mathbf{\\Sigma}$ to be symmetric.\n",
    "Further, the density (GM) is only integrable over $\\Reals^{\\xDim}$ if $\\mathbf{\\Sigma}$ is positive-definite.*\n",
    "\n",
    "It is important to recognize how similar eqn. (GM) is to the univariate (scalar) case (G1).\n",
    "Moreover, [as above](#Exc-(optional)----Integrals) it can be shown that\n",
    "- $\\mathbf{\\mu} = \\Expect[\\x]$,\n",
    "- $\\mathbf{\\Sigma} = \\Expect[(\\x-\\mu)(\\x-\\mu)\\tr]$.\n",
    "\n",
    "Note that that the elements of $\\mathbf{\\Sigma}$ are individual covariances,\n",
    "$\\Sigma_{i,j} = \\Expect[(x_i-\\mu_i)(x_j-\\mu_j)] = \\mathbb{Cov}(x_i, x_j)$.\n",
    "Therefore $\\mathbf{\\Sigma}$ is called the *covariance (matrix)*.\n",
    "and its diagonal entries are simply variances, $\\Sigma_{i,i} = \\mathbb{Var}(x_i)$.\n",
    "\n",
    "The following implements the pdf (GM). Take a moment to digest the code, but don't worry if you don't understand it all. Hints:\n",
    " * `@` produces matrix multiplication (`*` in `Matlab`);\n",
    " * `*` produces array multiplication (`.*` in `Matlab`);\n",
    " * `axis=-1` makes `np.sum()` work along the last dimension of an ND-array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f75d1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import det, inv\n",
    "\n",
    "def weighted_norm22(points, Wi):\n",
    "    \"Computes the weighted norm of each vector (row in `points`).\"\n",
    "    return np.sum( (points @ inv(Wi)) * points, axis=-1)\n",
    "\n",
    "def pdf_GM(points, mu, Sigma):\n",
    "    \"pdf -- Gaussian, Multivariate: N(x | mu, Sigma) for each x in `points`.\"\n",
    "    c = np.sqrt(det(2*np.pi*Sigma))\n",
    "    return 1/c * np.exp(-0.5*weighted_norm22(points - mu, Sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52569aaf",
   "metadata": {},
   "source": [
    "The following code plots the pdf as contour (iso-density) curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc581fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid2d = np.dstack(np.meshgrid(grid1d, grid1d))\n",
    "\n",
    "@interact(corr=(-1, 1, .001), std_x=(1e-5, 10, 1))\n",
    "def plot_pdf_G2(corr=0.7, std_x=1):\n",
    "    # Form covariance matrix (C) from input and some constants\n",
    "    var_x = std_x**2\n",
    "    var_y = 1\n",
    "    cv_xy = np.sqrt(var_x * var_y) * corr\n",
    "    C = 25 * np.array([[var_x, cv_xy],\n",
    "                       [cv_xy, var_y]])\n",
    "    # Evaluate (compute)\n",
    "    density_values = pdf_GM(grid2d, mu=0, Sigma=C)\n",
    "    # Plot\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    height = 1/np.sqrt(det(2*np.pi*C))\n",
    "    plt.contour(grid1d, grid1d, density_values,\n",
    "               levels=np.linspace(1e-4, height, 11), cmap=\"plasma\")\n",
    "    plt.axis('equal');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bfbdaa",
   "metadata": {},
   "source": [
    "**Exc -- Correlation influence:** How do the contours look? Try to understand why. Cases:\n",
    " * (a) correlation=0.\n",
    " * (b) correlation=0.99.\n",
    " * (c) correlation=0.5. (Note that we've used `plt.axis('equal')`).\n",
    " * (d) correlation=0.5, but with non-equal variances.\n",
    "\n",
    "Finally (optional): why does the code \"crash\" when `corr = +/- 1` ? Is this a good or a bad thing?  \n",
    "*Hint: do you like playing with fire?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae90de7d",
   "metadata": {},
   "source": [
    "**Exc Correlation game:** Play [here](http://guessthecorrelation.com/) until you get a score (gold coins) of 5 or more.  \n",
    "*PS: you can probably tell that the samples are not drawn from Gaussian distributions. However, the quantity $\\mathbb{Cov}(x_i, x_i)$ is well defined and can be estimated from the samples.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f56365",
   "metadata": {},
   "source": [
    "**Exc -- Correlation disambiguation:**\n",
    "* What's the difference between correlation and covariance?\n",
    "* What's the difference between non-zero (C) correlation (or covariance) and (D) dependence?\n",
    "  *Hint: consider this [image](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient#/media/File:Correlation_examples2.svg).*  \n",
    "  - Does $C \\Rightarrow D$ or the converse?  \n",
    "  - What about the negation, $\\neg D \\Rightarrow \\neg C$, or its converse?*  \n",
    "  - What about the (jointly) Gaussian case?\n",
    "* Does correlation (or dependence) imply causation?\n",
    "* Suppose $x$ and $y$ have non-zero correlation, but neither one causes the other.\n",
    "  Does information about $y$ give you information about $x$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fcfd1a",
   "metadata": {},
   "source": [
    "**Exc (optional) -- Gaussian ubiquity:** Why are we so fond of the Gaussian assumption?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43ac120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('Why Gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace12e0d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "The Normal/Gaussian distribution is bell-shaped.\n",
    "Its parameters are the mean and the variance.\n",
    "In the multivariate case, the mean is a vector,\n",
    "while the second parameter becomes a covariance *matrix*,\n",
    "whose off-diagonal elements represent scaled correlation factors,\n",
    "which measure *linear* dependence.\n",
    "\n",
    "### Next: [T3 - Bayesian inference](T3%20-%20Bayesian%20inference.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- ###### Author (1999):\n",
    "<a name=\"Author-(1999):\"></a> \n",
    "  Example T.I. Author, \"More to come\", *Some Journal*, 44(1), 2000."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "jupytext": {
   "formats": "ipynb,scripts//py"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
