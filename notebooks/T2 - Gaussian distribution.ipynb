{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f94593",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote = \"https://raw.githubusercontent.com/nansencenter/DA-tutorials\"\n",
    "!wget -qO- {remote}/master/notebooks/resources/colab_bootstrap.sh | bash -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb7f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources import show_answer, interact\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff70496",
   "metadata": {},
   "source": [
    "We start by reviewing the most useful of probability distributions.\n",
    "\n",
    "# T2 - The Gaussian (Normal) distribution\n",
    "\n",
    "But first, let's refresh some basic theory.\n",
    "$\n",
    "% ######################################## Loading TeX (MathJax)... Please wait ########################################\n",
    "\\newcommand{\\Reals}{\\mathbb{R}} \\newcommand{\\Expect}[0]{\\mathbb{E}} \\newcommand{\\NormDist}{\\mathscr{N}} \\newcommand{\\DynMod}[0]{\\mathscr{M}} \\newcommand{\\ObsMod}[0]{\\mathscr{H}} \\newcommand{\\mat}[1]{{\\mathbf{{#1}}}} \\newcommand{\\bvec}[1]{{\\mathbf{#1}}} \\newcommand{\\trsign}{{\\mathsf{T}}} \\newcommand{\\tr}{^{\\trsign}} \\newcommand{\\ceq}[0]{\\mathrel{≔}} \\newcommand{\\xDim}[0]{D} \\newcommand{\\supa}[0]{^\\text{a}} \\newcommand{\\supf}[0]{^\\text{f}} \\newcommand{\\I}[0]{\\mat{I}} \\newcommand{\\K}[0]{\\mat{K}} \\newcommand{\\bP}[0]{\\mat{P}} \\newcommand{\\bH}[0]{\\mat{H}} \\newcommand{\\bF}[0]{\\mat{F}} \\newcommand{\\R}[0]{\\mat{R}} \\newcommand{\\Q}[0]{\\mat{Q}} \\newcommand{\\B}[0]{\\mat{B}} \\newcommand{\\C}[0]{\\mat{C}} \\newcommand{\\Ri}[0]{\\R^{-1}} \\newcommand{\\Bi}[0]{\\B^{-1}} \\newcommand{\\X}[0]{\\mat{X}} \\newcommand{\\A}[0]{\\mat{A}} \\newcommand{\\Y}[0]{\\mat{Y}} \\newcommand{\\E}[0]{\\mat{E}} \\newcommand{\\U}[0]{\\mat{U}} \\newcommand{\\V}[0]{\\mat{V}} \\newcommand{\\x}[0]{\\bvec{x}} \\newcommand{\\y}[0]{\\bvec{y}} \\newcommand{\\z}[0]{\\bvec{z}} \\newcommand{\\q}[0]{\\bvec{q}} \\newcommand{\\br}[0]{\\bvec{r}} \\newcommand{\\bb}[0]{\\bvec{b}} \\newcommand{\\bx}[0]{\\bvec{\\bar{x}}} \\newcommand{\\by}[0]{\\bvec{\\bar{y}}} \\newcommand{\\barB}[0]{\\mat{\\bar{B}}} \\newcommand{\\barP}[0]{\\mat{\\bar{P}}} \\newcommand{\\barC}[0]{\\mat{\\bar{C}}} \\newcommand{\\barK}[0]{\\mat{\\bar{K}}} \\newcommand{\\D}[0]{\\mat{D}} \\newcommand{\\Dobs}[0]{\\mat{D}_{\\text{obs}}} \\newcommand{\\Dmod}[0]{\\mat{D}_{\\text{obs}}} \\newcommand{\\ones}[0]{\\bvec{1}} \\newcommand{\\AN}[0]{\\big( \\I_N - \\ones \\ones\\tr / N \\big)}\n",
    "$\n",
    "\n",
    "## Probability essentials\n",
    "\n",
    "As stated by James Bernoulli (1713) and elucidated by [Laplace (1812)](#References):\n",
    "\n",
    "> The Probability for an event is the ratio of the number of cases favorable to it, to the number of all\n",
    "> cases possible when nothing leads us to expect that any one of these cases should occur more than any other,\n",
    "> which renders them, for us, equally possible:\n",
    "\n",
    "$$ \\mathbb{P}(\\text{event}) = \\frac{\\text{# favorable outcomes}}{\\text{# possible outcomes}} $$\n",
    "\n",
    "- A *discrete* random variable, $X$, has a probability *mass* function (**pmf**) defined by $p(x) = \\mathbb{P}(X{=}x)$.  \n",
    "  **NB**: despite us casually using the same $p$ symbol, $p(x)$ and $p(y)$ are generally different functions.\n",
    "- The *joint* probability of two random variables $X$ and $Y$ is defined by the intersections:\n",
    "  $p(x, y) = \\mathbb{P}(X{=}x \\cap Y{=}y)$.\n",
    "- The *conditional* probability of $X$ given $Y$ is defined by $p(x|y) = p(x,y)/p(y)$.\n",
    "- A *continuous* random variable has a probability *density* function (**pdf**) defined by\n",
    "  $p(x) = \\mathbb{P}(X \\in [x, x+\\delta x])/\\delta x$, with $\\delta x \\to 0$.  \n",
    "  Equivalently, $p(x) = F'(x)$, where $F$ is the cumulative distribution function (**cdf**), $F(x) = \\mathbb{P}(X \\le x)$.\n",
    "\n",
    "A **sample average** based on draws from a random variable $x$ (we no longer use uppercase for random variables!)\n",
    "is denoted with an overhead bar:\n",
    "$$ \\bar{x} := \\frac{1}{N} \\sum_{n=1}^{N} x_n \\,. $$\n",
    "By the *law of large numbers (LLN)*, the sample average converges for $N \\to \\infty$ to the **expected value** (*sometimes* called the **mean**):\n",
    "$$ \\Expect[x] ≔ \\int x \\, p(x) \\, d x \\,, $$\n",
    "where the domain of integration is over *all possible values of $x$*.\n",
    "\n",
    "## The univariate (a.k.a. 1-dimensional, scalar) Gaussian\n",
    "\n",
    "If $x$ is Gaussian (a.k.a. \"Normal\"), we write\n",
    "$x \\sim \\NormDist(\\mu, \\sigma^2)$, or $p(x) = \\NormDist(x \\mid \\mu, \\sigma^2)$,\n",
    "where the parameters $\\mu$ and $\\sigma^2$ are called the mean and variance\n",
    "(for reasons that will become clear below).\n",
    "The Gaussian pdf is, for $x \\in (-\\infty, +\\infty)$,\n",
    "$$ \\large \\NormDist(x \\mid \\mu, \\sigma^2) = (2 \\pi \\sigma^2)^{-1/2} e^{-(x-\\mu)^2/2 \\sigma^2} \\,. \\tag{G1} $$\n",
    "\n",
    "Run the cell below to define a function to compute the pdf (G1) using the `scipy` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3be2918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_G1(x, mu, sigma2):\n",
    "    \"Univariate Gaussian pdf\"\n",
    "    pdf_values = sp.stats.norm.pdf(x, loc=mu, scale=np.sqrt(sigma2))\n",
    "    return pdf_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069911f0",
   "metadata": {},
   "source": [
    "Computers typically represent functions *numerically* by their values on a grid\n",
    "of points (nodes), an approach called ***discretisation***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80586aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = -20, 20\n",
    "N = 201                         # num of grid points\n",
    "grid1d = np.linspace(*bounds,N) # grid\n",
    "dx = grid1d[1] - grid1d[0]      # grid spacing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e40b3ce",
   "metadata": {},
   "source": [
    "Feel free to come back here later and change the grid resolution to see how\n",
    "it affects the cells below (upon re-running them).\n",
    "\n",
    "The following code plots the Gaussian pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0442e503",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = []\n",
    "@interact(mu=bounds, sigma=(.1, 10, 1))\n",
    "def plot_pdf(mu=0, sigma=5):\n",
    "    plt.figure(figsize=(6, 2))\n",
    "    colors = plt.get_cmap('hsv')([(k-len(hist))%9/9 for k in range(9)])\n",
    "    plt.xlim(*bounds)\n",
    "    plt.ylim(0, .2)\n",
    "    hist.insert(0, pdf_G1(grid1d, mu, sigma**2))\n",
    "    for density_values, color in zip(hist, colors):\n",
    "        plt.plot(grid1d, density_values, c=color)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0059c2",
   "metadata": {},
   "source": [
    "#### Exc -- parameter influence\n",
    "\n",
    "Play around with `mu` and `sigma` to answer these questions:\n",
    "\n",
    "- How does the pdf curve change when `mu` changes? Options (several might be right/wrong)\n",
    "  1. It changes the curve into a uniform distribution.\n",
    "  1. It changes the width of the curve.\n",
    "  1. It shifts the peak of the curve to the left or right.\n",
    "  1. It changes the height of the curve.\n",
    "  1. It transforms the curve into a binomial distribution.\n",
    "  1. It makes the curve wider or narrower.\n",
    "  1. It modifies the skewness (asymmetry) of the curve.\n",
    "  1. It causes the curve to expand vertically while keeping the width the same.\n",
    "  1. It translates the curve horizontally.\n",
    "  1. It alters the kurtosis (peakedness) of the curve.\n",
    "  1. It rotates the curve around the origin.\n",
    "  1. It makes the curve a straight line.\n",
    "- How does the pdf curve change when you increase `sigma`?  \n",
    "  Refer to the same options as previous question.\n",
    "- In a few words, describe the shape of the Gaussian pdf curve.\n",
    "  Does this ring a bell? *Hint: it should be clear as a bell!*\n",
    "\n",
    "**Exc -- Implementation:** Change the implementation of `pdf_G1` so as to not use `scipy`, but your own code (using `numpy` only). Re-run all of the above cells and check that you get the same plots as before.  \n",
    "*Hint: `**` is the exponentiation/power operator, but $e^x$ is more efficiently computed with `np.exp(x)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e1106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('pdf_G1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b6d541",
   "metadata": {},
   "source": [
    "**Exc -- Derivatives:** Recall $p(x) = \\NormDist(x \\mid \\mu, \\sigma^2)$ from eqn. (G1).  \n",
    "Use pen, paper, and calculus to answer the following questions,  \n",
    "which derive some helpful mnemonics about the distribution.\n",
    "\n",
    "- (i) Find $x$ such that $p(x) = 0$.\n",
    "- (ii) Where is the location of the **mode (maximum)** of the density?  \n",
    "  I.e. find $x$ such that $\\frac{d p}{d x}(x) = 0$.\n",
    "  *Hint: begin by writing $p(x)$ as $c e^{- J(x)}$ for some $J(x)$.*\n",
    "- (iii) Where is the **inflection point**? I.e. where $\\frac{d^2 p}{d x^2}(x) = 0$.\n",
    "- (iv) *Optional*: Some forms of *sensitivity analysis* (typically for non-Gaussian $p$) consist in estimating/approximating the Hessian, i.e. $\\frac{d^2 \\log p}{d x^2}$. Explain what this has to do with *uncertainty quantification*.\n",
    "\n",
    "<a name=\"Exc-(optional)----Change-of-variables\"></a>\n",
    "\n",
    "#### Exc (optional) -- Change of variables\n",
    "\n",
    "Let $z = \\phi(x)$ for some monotonic function $\\phi$,\n",
    "and $p_x$ and $p_z$ be their probability density functions (pdf).\n",
    "\n",
    "- (a): Show that $p_z(z) = p_x\\big(\\phi^{-1}(z)\\big) \\frac{1}{|\\phi'(z)|}$,\n",
    "- (b): Show that you don't need to derive the density of $z$ in order to compute its expectation, i.e. that\n",
    "  $$ \\Expect[z] = \\int  \\phi(x) \\, p_x(x) \\, d x ≕ \\Expect[\\phi(x)] \\,,$$\n",
    "  *Hint: while the proof is convoluted, the result itself is [pretty intuitive](https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92777530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('CVar in proba')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8311df4c",
   "metadata": {},
   "source": [
    "<a name=\"Exc-(optional)----Integrals\"></a>\n",
    "\n",
    "#### Exc (optional) -- Integrals\n",
    "\n",
    "Recall $p(x) = \\NormDist(x \\mid \\mu, \\sigma^2)$ from eqn. (G1). Abbreviate it using $c = (2 \\pi \\sigma^2)^{-1/2}$.  \n",
    "Use pen, paper, and calculus to show that\n",
    "\n",
    "- (i) the first parameter, $\\mu$, indicates its **mean**, i.e. that $$\\mu = \\Expect[x] \\,.$$\n",
    "  *Hint: you can rely on the result of (iii)*\n",
    "- (ii) the second parameter, $\\sigma^2>0$, indicates its **variance**,\n",
    "  i.e. that $$\\sigma^2 = \\mathbb{Var}(x) \\mathrel{≔} \\Expect[(x-\\mu)^2] \\,.$$\n",
    "  *Hint: use $x^2 = x x$ to enable integration by parts.*\n",
    "- (iii) $E[1] = 1$,  \n",
    "  thus proving that (G1) indeed uses the right normalising constant.  \n",
    "  *Hint: Neither Bernoulli and Laplace managed this,\n",
    "  until [Gauss (1809)](#References) did by first deriving $(E[1])^2$.  \n",
    "  For more (visual) help, watch [3Blue1Brown](https://www.youtube.com/watch?v=cy8r7WSuT1I&t=3m52s).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5470edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('Gauss integrals')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce40fa45",
   "metadata": {},
   "source": [
    "**Exc -- The uniform pdf**:\n",
    "Below is the pdf of the [uniform/flat/box distribution](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous))\n",
    "for a given mean and variance.\n",
    "\n",
    "- Replace `_G1` by `_U1` in the code generating the above interactive plot.\n",
    "- Why are the walls (ever so slightly) inclined?\n",
    "- Write your own implementation below, and check that it reproduces the `scipy` version already in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c1e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_U1(x, mu, sigma2):\n",
    "    a = mu - np.sqrt(3*sigma2)\n",
    "    b = mu + np.sqrt(3*sigma2)\n",
    "    pdf_values = sp.stats.uniform(loc=a, scale=(b-a)).pdf(x)\n",
    "    # Your own implementation:\n",
    "    # height = ...\n",
    "    # pdf_values = height * np.ones_like(x)\n",
    "    # pdf_values[x<a] = ...\n",
    "    # pdf_values[x>b] = ...\n",
    "    return pdf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e09bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('pdf_U1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28223c0",
   "metadata": {},
   "source": [
    "## The multivariate (i.e. vector) Gaussian\n",
    "\n",
    "A *multivariate* random variable, i.e. **vector**, is simply a collection of scalar variables (on the same probability space).\n",
    "I.e. its density is the joint density of its components.\n",
    "The pdf of the multivariate Gaussian (for any dimension $\\ge 1$) is\n",
    "\n",
    "$$\\large \\NormDist(\\x \\mid  \\mathbf{\\mu}, \\mathbf{\\Sigma}) = |2 \\pi \\mathbf{\\Sigma}|^{-1/2} \\, \\exp\\Big(-\\frac{1}{2}\\|\\x-\\mathbf{\\mu}\\|^2_\\mathbf{\\Sigma} \\Big) \\,, \\tag{GM} $$\n",
    "where $|.|$ represents the matrix determinant,  \n",
    "and $\\|.\\|_\\mathbf{W}$ represents a weighted 2-norm: $\\|\\x\\|^2_\\mathbf{W} = \\x^T \\mathbf{W}^{-1} \\x$.  \n",
    "\n",
    "<details style=\"border: 1px solid #aaaaaa; border-radius: 4px; padding: 0.5em 0.5em 0;\">\n",
    "<summary style=\"font-weight: normal; font-style: italic; margin: -0.5em -0.5em 0; padding: 0.5em;\">\n",
    "  🔍 $\\mathbf{W}$ must be symmetric-positive-definite (SPD) because ... 👇\n",
    "</summary>\n",
    "\n",
    "- The norm (a quadratic form) is invariant to any asymmetry in the weight matrix.\n",
    "- The density (GM) would not be integrable (over $\\Reals^{\\xDim}$) if $\\x\\tr \\mathbf{\\Sigma} \\x > 0$.\n",
    "\n",
    "- - -\n",
    "</details>\n",
    "\n",
    "It is important to recognize how similar eqn. (GM) is to the univariate (scalar) case (G1).\n",
    "Moreover, [similarly as above](#Exc-(optional)----Integrals), it can be shown that\n",
    "\n",
    "- $\\mathbf{\\mu} = \\Expect[\\x]$,\n",
    "- $\\mathbf{\\Sigma} = \\Expect[(\\x-\\mu)(\\x-\\mu)\\tr]$,\n",
    "\n",
    "I.e. the elements of $\\mathbf{\\Sigma}$ are the individual covariances,\n",
    "$\\Sigma_{i,j} = \\Expect[(x_i-\\mu_i)(x_j-\\mu_j)] =: \\mathbb{Cov}(x_i, x_j)$\n",
    "and, on the diagonal ($i=j$), variances: $\\Sigma_{i,i} = \\mathbb{Var}(x_i)$.\n",
    "Therefore $\\mathbf{\\Sigma}$ is called the *covariance (matrix)*.\n",
    "\n",
    "The following implements the pdf (GM). Take a moment to digest the code, but don't worry if you don't understand it all. Hints:\n",
    "\n",
    "- `@` produces matrix multiplication (`*` in `Matlab`);\n",
    "- `*` produces array multiplication (`.*` in `Matlab`);\n",
    "- `axis=-1` makes `np.sum()` work along the last dimension of an ND-array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f75d1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import det, inv\n",
    "\n",
    "def weighted_norm22(points, Wi):\n",
    "    \"Computes the weighted norm of each vector (row in `points`).\"\n",
    "    return np.sum( (points @ inv(Wi)) * points, axis=-1)\n",
    "\n",
    "def pdf_GM(points, mu, Sigma):\n",
    "    \"pdf -- Gaussian, Multivariate: N(x | mu, Sigma) for each x in `points`.\"\n",
    "    c = np.sqrt(det(2*np.pi*Sigma))\n",
    "    return 1/c * np.exp(-0.5*weighted_norm22(points - mu, Sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52569aaf",
   "metadata": {},
   "source": [
    "The following code plots the pdf as contour (level) curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc581fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid2d = np.dstack(np.meshgrid(grid1d, grid1d))\n",
    "\n",
    "@interact(corr=(-1, 1, .001), std_x=(1e-5, 10, 1))\n",
    "def plot_pdf_G2(corr=0.7, std_x=1):\n",
    "    # Form covariance matrix (C) from input and some constants\n",
    "    var_x = std_x**2\n",
    "    var_y = 1\n",
    "    cv_xy = np.sqrt(var_x * var_y) * corr\n",
    "    C = 25 * np.array([[var_x, cv_xy],\n",
    "                       [cv_xy, var_y]])\n",
    "    # Evaluate (compute)\n",
    "    density_values = pdf_GM(grid2d, mu=0, Sigma=C)\n",
    "    # Plot\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    height = 1/np.sqrt(det(2*np.pi*C))\n",
    "    plt.contour(grid1d, grid1d, density_values,\n",
    "               levels=np.linspace(1e-4, height, 11), cmap=\"plasma\")\n",
    "    plt.axis('equal');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bfbdaa",
   "metadata": {},
   "source": [
    "The code defines the covariance `cv_xy` from the input ***correlation*** `corr`.\n",
    "This is a coefficient (number), defined for any two random variables $x$ and $y$ (not necessarily Gaussian) by\n",
    "$$ \\rho[x,y]=\\frac{\\mathbb{Cov}[x,y]}{\\sigma_x \\sigma_y} \\,. $$\n",
    "This correlation quantifies (defines) the ***linear dependence*** between $x$ and $y$. Indeed,\n",
    "\n",
    "- $-1\\leq \\rho \\leq 1$ (by Cauchy-Swartz)\n",
    "- **If** $X$ and $Y$ are *independent*, i.e. $p(x,y) = p(x) \\, p(y)$ for all $x, y$, then $\\rho[X,Y]=0$.\n",
    "\n",
    "**Exc -- Correlation influence:** How do the contours look? Try to understand why. Cases:\n",
    "\n",
    "- (a) correlation=0.\n",
    "- (b) correlation=0.99.\n",
    "- (c) correlation=0.5. (Note that we've used `plt.axis('equal')`).\n",
    "- (d) correlation=0.5, but with non-equal variances.\n",
    "\n",
    "Finally (optional): why does the code \"crash\" when `corr = +/- 1` ? Is this a good or a bad thing?  \n",
    "*Hint: do you like playing with fire?*\n",
    "\n",
    "**Exc Correlation game:** [Play](http://guessthecorrelation.com/) until you get a score (gold coins) of 5 or more.  \n",
    "\n",
    "**Exc -- Correlation disambiguation:**\n",
    "\n",
    "- What's the difference between correlation and covariance (in words)?\n",
    "- What's the difference between non-zero (C) correlation (or covariance) and (D) dependence?\n",
    "  *Hint: consider this [image](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient#/media/File:Correlation_examples2.svg).*  \n",
    "  - Does $C \\Rightarrow D$ or the converse?  \n",
    "  - What about the negation, $\\neg D \\Rightarrow \\neg C$, or its converse?*  \n",
    "  - What about the (jointly) Gaussian case?\n",
    "- Does correlation (or dependence) imply causation?\n",
    "- Suppose $x$ and $y$ have non-zero correlation, but neither one causes the other.\n",
    "  Does information about $y$ give you information about $x$?\n",
    "\n",
    "**Exc (optional) -- Gaussian ubiquity:** Why are we so fond of the Gaussian assumption?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43ac120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('Why Gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace12e0d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The Normal/Gaussian distribution is bell-shaped.\n",
    "Its parameters are the mean and the variance.\n",
    "In the multivariate case, the mean is a vector,\n",
    "while the second parameter becomes a covariance *matrix*,\n",
    "whose off-diagonal elements represent scaled correlation factors,\n",
    "which measure *linear* dependence.\n",
    "\n",
    "### Next: [T3 - Bayesian inference](T3%20-%20Bayesian%20inference.ipynb)\n",
    "\n",
    "<a name=\"References\"></a>\n",
    "\n",
    "### References\n",
    "\n",
    "- **Laplace (1812)**: P. S. Laplace, \"Théorie Analytique des Probabilités\", 1812.\n",
    "- **Gauss (1809)**: Gauss, C. F. (1809). *Theoria Motus Corporum Coelestium in Sectionibus Conicis Solem Ambientium*. Specifically, Book II, Section 3, Art. 177-179, where he presents the method of least squares (which will be very relevant to us) and its probabilistic justification based on the normal distribution of errors)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "jupytext": {
   "formats": "ipynb,scripts//py:light,scripts//md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
