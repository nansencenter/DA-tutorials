{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6de1e4a8",
   "metadata": {},
   "source": [
    "# T5 - The Kalman filter (KF) -- multivariate\n",
    "Dealing with vectors and matrices is a lot like plain numbers. But some things get more complicated.\n",
    "We begin by illustrating Bayes' rule in the 2D, i.e. multivariate, case.\n",
    "$\n",
    "% START OF MACRO DEF\n",
    "% DO NOT EDIT IN INDIVIDUAL NOTEBOOKS, BUT IN macros.py\n",
    "%\n",
    "\\newcommand{\\Reals}{\\mathbb{R}}\n",
    "\\newcommand{\\Expect}[0]{\\mathbb{E}}\n",
    "\\newcommand{\\NormDist}{\\mathcal{N}}\n",
    "%\n",
    "\\newcommand{\\DynMod}[0]{\\mathscr{M}}\n",
    "\\newcommand{\\ObsMod}[0]{\\mathscr{H}}\n",
    "%\n",
    "\\newcommand{\\mat}[1]{{\\mathbf{{#1}}}}\n",
    "%\\newcommand{\\mat}[1]{{\\pmb{\\mathsf{#1}}}}\n",
    "\\newcommand{\\bvec}[1]{{\\mathbf{#1}}}\n",
    "%\n",
    "\\newcommand{\\trsign}{{\\mathsf{T}}}\n",
    "\\newcommand{\\tr}{^{\\trsign}}\n",
    "\\newcommand{\\tn}[1]{#1}\n",
    "\\newcommand{\\ceq}[0]{\\mathrel{≔}}\n",
    "%\n",
    "\\newcommand{\\I}[0]{\\mat{I}}\n",
    "\\newcommand{\\K}[0]{\\mat{K}}\n",
    "\\newcommand{\\bP}[0]{\\mat{P}}\n",
    "\\newcommand{\\bH}[0]{\\mat{H}}\n",
    "\\newcommand{\\bF}[0]{\\mat{F}}\n",
    "\\newcommand{\\R}[0]{\\mat{R}}\n",
    "\\newcommand{\\Q}[0]{\\mat{Q}}\n",
    "\\newcommand{\\B}[0]{\\mat{B}}\n",
    "\\newcommand{\\C}[0]{\\mat{C}}\n",
    "\\newcommand{\\Ri}[0]{\\R^{-1}}\n",
    "\\newcommand{\\Bi}[0]{\\B^{-1}}\n",
    "\\newcommand{\\X}[0]{\\mat{X}}\n",
    "\\newcommand{\\A}[0]{\\mat{A}}\n",
    "\\newcommand{\\Y}[0]{\\mat{Y}}\n",
    "\\newcommand{\\E}[0]{\\mat{E}}\n",
    "\\newcommand{\\U}[0]{\\mat{U}}\n",
    "\\newcommand{\\V}[0]{\\mat{V}}\n",
    "%\n",
    "\\newcommand{\\x}[0]{\\bvec{x}}\n",
    "\\newcommand{\\y}[0]{\\bvec{y}}\n",
    "\\newcommand{\\z}[0]{\\bvec{z}}\n",
    "\\newcommand{\\q}[0]{\\bvec{q}}\n",
    "\\newcommand{\\br}[0]{\\bvec{r}}\n",
    "\\newcommand{\\bb}[0]{\\bvec{b}}\n",
    "%\n",
    "\\newcommand{\\bx}[0]{\\bvec{\\bar{x}}}\n",
    "\\newcommand{\\by}[0]{\\bvec{\\bar{y}}}\n",
    "\\newcommand{\\barB}[0]{\\mat{\\bar{B}}}\n",
    "\\newcommand{\\barP}[0]{\\mat{\\bar{P}}}\n",
    "\\newcommand{\\barC}[0]{\\mat{\\bar{C}}}\n",
    "\\newcommand{\\barK}[0]{\\mat{\\bar{K}}}\n",
    "%\n",
    "\\newcommand{\\D}[0]{\\mat{D}}\n",
    "\\newcommand{\\Dobs}[0]{\\mat{D}_{\\text{obs}}}\n",
    "\\newcommand{\\Dmod}[0]{\\mat{D}_{\\text{obs}}}\n",
    "%\n",
    "\\newcommand{\\ones}[0]{\\bvec{1}}\n",
    "\\newcommand{\\AN}[0]{\\big( \\I_N - \\ones \\ones\\tr / N \\big)}\n",
    "%\n",
    "% END OF MACRO DEF\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca4eab",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import resources.workspace as ws\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b5f3cb",
   "metadata": {},
   "source": [
    "## Multivariate Bayes' rule\n",
    "\n",
    "#### Exc 2 (The likelihood):\n",
    "Suppose the observation, $\\y$, is related to the true state, $\\x$, via an \"observation (forward) model\", $\\ObsMod$:\n",
    "\\begin{align*}\n",
    "\\y &= \\ObsMod(\\x) + \\br \\, , \\;\\; \\qquad (2)\n",
    "\\end{align*}\n",
    "where the noise follows the law $\\br \\sim \\NormDist(\\bvec{0}, \\R)$ for some $\\R>0$ (meaning $\\R$ is symmetric-positive-definite).\n",
    "\n",
    "\n",
    "Derive the expression for the likelihood, $p(\\y|\\x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064c398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Likelihood for additive noise')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba6b29b",
   "metadata": {},
   "source": [
    "Unlike [T2](T2%20-%20Gaussian%20distribution.ipynb), which implemented the Gaussian pdf,\n",
    "we here take it straight from `scipy.stats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06046d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "def pdf_GM(points, mu, Sigma):\n",
    "    diff = points - mu  # broadcasts both\n",
    "    zero = np.zeros(len(Sigma))\n",
    "    return multivariate_normal(zero, Sigma).pdf(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4212822",
   "metadata": {},
   "source": [
    "The following code shows Bayes' rule in action.\n",
    "Notice that the implementation (`Bayes_rule`) is the very same as in the 1D case\n",
    "-- simply the pointwise multiplication of two arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af335f4",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def Bayes_rule(prior_values, lklhd_values, dx):\n",
    "    prod = prior_values * lklhd_values         # pointwise multiplication\n",
    "    posterior_values = prod/(np.sum(prod)*dx)  # normalization\n",
    "    return posterior_values\n",
    "\n",
    "bounds = -15, 15\n",
    "grid1d = np.linspace(*bounds, 201)\n",
    "dx = grid1d[1]  - grid1d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245fb965",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "grid2d = np.dstack(np.meshgrid(grid1d, grid1d))\n",
    "\n",
    "@ws.interact(corr_R=(-0.999, 0.999, .01),\n",
    "             corr_B=(-0.999, 0.999, .01),\n",
    "             y1=bounds,\n",
    "             y2=bounds,\n",
    "             R1=(0.01, 36, 0.2),\n",
    "             R2=(0.01, 36, 0.2),\n",
    "             top=[['corr_B', 'corr_R'], 'y1_only'],\n",
    "             bottom=[['y1', 'R1']],\n",
    "             vertical=['y2', 'R2'],\n",
    "             right=[['y2', 'R2']],\n",
    "             )\n",
    "def Bayes2(corr_B=.6, corr_R=.6, y1=3, y2=-12, R1=4**2, R2=1, y1_only=False):\n",
    "    x = grid2d\n",
    "    # Prior\n",
    "    mu = np.zeros(2)\n",
    "    B = 25 * np.array([[1, corr_B],\n",
    "                       [corr_B, 1]])\n",
    "    # Likelihood\n",
    "    cov_R = np.sqrt(R1*R2)*corr_R\n",
    "    R = np.array([[R1, cov_R],\n",
    "                  [cov_R, R2]])\n",
    "    y = np.array([y1, y2])\n",
    "    Hx = x\n",
    "    #Hx = x**2/4\n",
    "    #Hx = x**3/36\n",
    "\n",
    "    #Hx = x[..., :1] * x[..., 1:2]\n",
    "    #y1_only = True\n",
    "\n",
    "    if y1_only:\n",
    "        y = y[:1]\n",
    "        R = R[:1, :1]\n",
    "        Hx = Hx[..., :1]\n",
    "\n",
    "    # Compute\n",
    "    lklhd = pdf_GM(y, Hx, R)\n",
    "    prior = pdf_GM(x, mu, B)\n",
    "    postr = Bayes_rule(prior, lklhd, dx**2)\n",
    "\n",
    "    ax, plot = ws.get_jointplotter(grid1d)\n",
    "    contours = [plot(prior, 'blue'),\n",
    "                plot(lklhd, 'green'),\n",
    "                plot(postr, 'red', linewidths=2)]\n",
    "    ax.legend(contours, ['prior', 'lklhd', 'postr'], loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c43fb2c",
   "metadata": {},
   "source": [
    "#### Exc\n",
    "- Does the posterior (pdf) generally lie \"between\" the prior and likelihood?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb27acd8",
   "metadata": {},
   "source": [
    "#### Exc: Observation models\n",
    "- Implement $\\mathcal{H}(\\x) = \\x_1$.\n",
    "- Implement $\\mathcal{H}(\\x) = \\frac{1}{M} \\sum_{i=1}^M \\x_i$.\n",
    "- Implement $\\mathcal{H}(\\x) = \\x_2 - \\x_1$.\n",
    "- Implement $\\mathcal{H}(\\x) = (\\x_1^2, \\x_2^2)$.\n",
    "- Implement $\\mathcal{H}(\\x) = \\x_1 \\x_2$.\n",
    "- For those of the above models that are linear,\n",
    "  find the (possibly rectangular) matrix $\\bH$ such that $\\mathcal{H}(\\x) = \\bH \\x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa63694c",
   "metadata": {},
   "source": [
    "## The KF analysis step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e358f65",
   "metadata": {},
   "source": [
    "The following exercise derives the analysis step.\n",
    "\n",
    "#### Exc 4 (The 'precision' form of the KF):\n",
    "Similarly to [Exc 2.18](T3%20-%20Bayesian%20inference.ipynb#Exc--2.18-'Gaussian-Gaussian-Bayes':),\n",
    "it may be shown that the prior $p(\\x) = \\NormDist(\\x \\mid \\bb,\\B)$\n",
    "and likelihood $p(\\y|\\x) = \\NormDist(\\y \\mid \\bH \\x,\\R)$,\n",
    "yield the posterior:\n",
    "\\begin{align}\n",
    "p(\\x|\\y)\n",
    "&= \\NormDist(\\x \\mid \\hat{\\x}, \\bP) \\tag{4}\n",
    "\\, ,\n",
    "\\end{align}\n",
    "where the posterior/analysis mean (vector) and covariance (matrix) are given by:\n",
    "\\begin{align}\n",
    "\t\t\t\\bP &= (\\bH\\tr \\Ri \\bH + \\Bi)^{-1} \\, , \\tag{5} \\\\\n",
    "\t\t\t\\hat{\\x} &= \\bP\\left[\\bH\\tr \\Ri \\y + \\Bi \\bb\\right] \\tag{6} \\, ,\n",
    "\\end{align}\n",
    "Prove eqns (4-6).  \n",
    "*Hint: like the last time, the main part lies in \"completing the square\" in $\\x$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad586834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('KF precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8560599",
   "metadata": {},
   "source": [
    "However, the computations can be pretty expensive...\n",
    "\n",
    "#### Exc 5 (optional): Suppose $\\x$ is $M$-dimensional and has a covariance matrix $\\B$.\n",
    " * (a). What's the size of $\\B$?\n",
    " * (b). How many \"flops\" (approximately, i.e. to leading order) are required  \n",
    " to compute the \"precision form\" of the KF update equation, eqn (5) ?\n",
    " * (c). How much memory (bytes) is required to hold its covariance matrix $\\B$ ?\n",
    " * (d). How many megabytes is this if $M$ is a million?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692ae96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Cov memory')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c77009",
   "metadata": {},
   "source": [
    "This is one of the principal reasons why basic extended KF is infeasible for DA.  \n",
    "The following derives another, often more practical, form of the KF analysis update."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892416e0",
   "metadata": {},
   "source": [
    "#### Exc 6 (The \"Woodbury\" matrix inversion identity):\n",
    "The following is known as the Sherman-Morrison-Woodbury lemma/identity,\n",
    "$$\\begin{align}\n",
    "    \\bP = \\left( \\B^{-1} + \\V\\tr \\R^{-1} \\U \\right)^{-1}\n",
    "    =\n",
    "    \\B - \\B \\V\\tr \\left( \\R + \\U \\B \\V\\tr \\right)^{-1} \\U \\B \\, ,\n",
    "    \\tag{W}\n",
    "\\end{align}$$\n",
    "which holds for any (suitably shaped matrices)\n",
    "$\\B$, $\\R$, $\\V,\\U$ *such that the above exists*.\n",
    "\n",
    "Prove the identity. Hint: don't derive it, just prove it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c0868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Woodbury')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716e7169",
   "metadata": {},
   "source": [
    "#### Exc 7 (optional):\n",
    "- Show that $\\B$ and $\\R$ must be square.\n",
    "- Show that $\\U$ and $\\V$ are not necessarily square, but must have the same dimensions.\n",
    "- Show that $\\B$ and $\\R$ are not necessarily of equal size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe80342",
   "metadata": {},
   "source": [
    "Exc 7 makes it clear that the Woodbury identity may be used to compute $\\bP$ by inverting matrices of the size of $\\R$ rather than the size of $\\B$.\n",
    "Of course, if $\\R$ is bigger than $\\B$, then the identity is useful the other way around."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5830457",
   "metadata": {},
   "source": [
    "#### Exc 8 (Corollary 1 -- optional):\n",
    "Prove that, for any symmetric, positive-definite (SPD) matrices $\\R$ and $\\B$, and any matrix $\\bH$,\n",
    "$$\\begin{align}\n",
    " \t\\left(\\bH\\tr \\R^{-1} \\bH + \\B^{-1}\\right)^{-1}\n",
    "    &=\n",
    "    \\B - \\B \\bH\\tr \\left( \\R + \\bH \\B \\bH\\tr \\right)^{-1} \\bH \\B \\tag{C1}\n",
    "    \\, .\n",
    "\\end{align}$$\n",
    "Hint: consider the properties of [SPD](https://en.wikipedia.org/wiki/Definiteness_of_a_matrix#Properties) matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83409d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Woodbury C1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dcda51",
   "metadata": {},
   "source": [
    "#### Exc 10 (Corollary 2 -- optional):\n",
    "Prove that, for the same matrices as for Corollary C1,\n",
    "$$\\begin{align}\n",
    "\t\\left(\\bH\\tr \\R^{-1} \\bH + \\B^{-1}\\right)^{-1}\\bH\\tr \\R^{-1}\n",
    "    &= \\B \\bH\\tr \\left( \\R + \\bH \\B \\bH\\tr \\right)^{-1}\n",
    "    \\tag{C2}\n",
    "    \\, .\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82abd4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Woodbury C2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71e92d6",
   "metadata": {},
   "source": [
    "#### Exc 12 (The \"gain\" form of the KF):\n",
    "Now, let's go back to the KF, eqns (5) and (6). Since $\\B$ and $\\R$ are covariance matrices, they are symmetric-positive. In addition, we will assume that they are full-rank, making them SPD and invertible.  \n",
    "\n",
    "Define the Kalman gain by:\n",
    " $$\\begin{align}\n",
    "    \\K &= \\B \\bH\\tr \\big(\\bH \\B \\bH\\tr + \\R\\big)^{-1} \\, . \\tag{K1}\n",
    "\\end{align}$$\n",
    " * (a) Apply (C1) to eqn (5) to obtain the Kalman gain form of analysis/posterior covariance matrix:\n",
    "$$\\begin{align}\n",
    "    \\bP &= [\\I_M - \\K \\bH]\\B \\, . \\tag{8}\n",
    "\\end{align}$$\n",
    "\n",
    "* (b) Apply (C2)  to (5) to obtain the identity\n",
    "$$\\begin{align}\n",
    "    \\K &= \\bP \\bH\\tr \\R^{-1}  \\, . \\tag{K2}\n",
    "\\end{align}$$\n",
    "\n",
    "* (c) Show that $\\bP \\Bi = [\\I_M - \\K \\bH]$.\n",
    "* (d) Use (b) and (c) to obtain the Kalman gain form of analysis/posterior covariance\n",
    "$$\\begin{align}\n",
    "     \\hat{\\x} &= \\bb + \\K\\left[\\y - \\bH \\bb\\right] \\, . \\tag{9}\n",
    "\\end{align}$$\n",
    "\n",
    "Together, eqns (8) and (9) define the Kalman gain form of the KF update.\n",
    "The inversion (eqn 7) involved is of the size of $\\R$, while in eqn (5) it is of the size of $\\B$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923b8b9c",
   "metadata": {},
   "source": [
    "## The KF forecast step\n",
    "\n",
    "The forecast step remains essentially unchanged from the [univariate case](T3%20-%20Bayesian%20inference.ipynb).\n",
    "The only difference is that $\\DynMod$ is now a matrix, as well as the use of the transpose ${}^T$ in the covariance equation:\n",
    "$\\begin{align}\n",
    "\\bb_k\n",
    "&= \\DynMod_{k-1} \\hat{\\x}_{k-1} \\, , \\tag{1a} \\\\\\\n",
    "\\B_k\n",
    "&= \\DynMod_{k-1} \\bP_{k-1} \\DynMod_{k-1}^T + \\Q_{k-1} \\, . \\tag{1b}\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596d515f",
   "metadata": {},
   "source": [
    "<mark><font size=\"-1\">\n",
    "We have now derived the Kalman filter, also in the multivariate case. We know how to:\n",
    "</font></mark>\n",
    "- Propagate our estimate of $\\x$ to the next time step using eqns (1a) and (1b).\n",
    "- Update our estimate of $\\x$ by assimilating the latest observation $\\y$, using eqns (5) and (6)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feda02f3",
   "metadata": {},
   "source": [
    "## Another time series problem, but this time multivariate\n",
    "In the previous tutorial we saw that the Kalman filter (KF) works well\n",
    "for univariate (scalar) time series problem. Let us try it on a multivariate problem.\n",
    "\n",
    "### The model\n",
    "- The straight line example (of the tutorial T3) could result from discretizing the model:\n",
    "$\n",
    "\\frac{d^2 x}{dt^2} = 0 \\, .\n",
    "$\n",
    "- Here we're going to consider the M-th order model:\n",
    "$ \\frac{d^M x}{dt^M} = 0 \\, .$\n",
    "- This can be rewritten as a 1-st order vector (i.e. coupled system of) ODE:\n",
    "$\\frac{d x_m}{dt} = x_{m+1} \\, ,$\n",
    "where the subscript $1,\\ldots,M$ is now instead the *index* of the state vector element,\n",
    "and $x_{M+1} = 0$.\n",
    "- To make it more interesting, we'll add two terms to this evolution model:  \n",
    "    - damping: $\\beta x_m$, with $\\beta < 0$;\n",
    "    - noise: $\\frac{d q_m}{dt}$.  \n",
    "\n",
    "Thus,\n",
    "$$ \\frac{d x_m}{dt} = \\beta x_m + x_{m+1} + \\frac{d q_m}{dt} \\, ,$$\n",
    "where $q_m$ is the noise process, and $\\beta = \\log(0.9)$.\n",
    "Discretized by explicit-Euler, with a time step `dt=1`, this yields\n",
    "$$ x_{k+1, m} = 0.9 x_{k, m} + x_{k, m+1} + q_{k, m}\\, ,$$\n",
    "\n",
    "In summary, $\\x_{k+1} = \\DynMod \\x_k + \\q_k$, with $\\DynMod$ as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f2b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 4 # model order (and also ndim)\n",
    "M_matrix = 0.9*np.eye(M) + np.diag(np.ones(M-1), 1)\n",
    "print(M_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07755d38",
   "metadata": {},
   "source": [
    "### Estimation by the Kalman filter (and smoother) with DAPPER\n",
    "\n",
    "Note that this is an $M$-dimensional time series.\n",
    "However, we'll only observe the first (0th) component.\n",
    "\n",
    "We shall not write the code for the multivariate Kalman filter,\n",
    "because it already exists in DAPPER in `da_methods.py` and is called `ExtKF()`.\n",
    "\n",
    "The following code configures an experiment based on the above model. Don't worry about the specifics. We'll get back to how to use DAPPER later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a3d91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dapper.mods as modelling\n",
    "from dapper.mods.utils import linear_model_setup, partial_Id_Obs\n",
    "\n",
    "# Forecast dynamics\n",
    "Dyn = linear_model_setup(M_matrix, dt0=1)\n",
    "Dyn['noise'] = 0.0001*(1+np.arange(M))\n",
    "\n",
    "# Initial conditions\n",
    "X0 = modelling.GaussRV(M=M, C=0.02*np.arange(M))\n",
    "\n",
    "# Observe 0th component only\n",
    "Obs = partial_Id_Obs(M, [0])\n",
    "Obs['noise'] = 1000\n",
    "\n",
    "# Time settings\n",
    "t = modelling.Chronology(dt=1, dto=5, K=250)\n",
    "\n",
    "# Wrap-up\n",
    "HMM = modelling.HiddenMarkovModel(Dyn, Obs, t, X0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a7c8f5",
   "metadata": {},
   "source": [
    "This generates (simulates) a synthetic truth (xx) and observations (yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e17857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = HMM.simulate()\n",
    "for m, x in enumerate(xx.T):\n",
    "    plt.plot(x, label=\"x^%d\"%m)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4ea9f7",
   "metadata": {},
   "source": [
    "Now we'll run assimilation methods on the data. Firstly, the KF, available as `ExtKF` in DAPPER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02472b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dapper.da_methods as da\n",
    "ExtKF = da.ExtKF()\n",
    "ExtKF.assimilate(HMM, xx, yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4005d114",
   "metadata": {},
   "source": [
    "We'll also run the \"Kalman smoother\" available as `ExtRTS`.\n",
    "Without going into details, this method is based on the Kalman *filter* but,\n",
    "being a *smoother*,\n",
    "it also goes backwards and updates previous estimates with future (relatively speaking) observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f23e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "ExtRTS = da.ExtRTS()\n",
    "ExtRTS.assimilate(HMM, xx, yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad0abd9",
   "metadata": {},
   "source": [
    "### Estimation by \"time series analysis\"\n",
    "The following methods perform time series analysis of the observations, and are mainly derived from signal processing theory.\n",
    "Considering that derivatives can be approximated by differentials, it is plausible that the above model could also be written as an AR(M) process. Thus these methods should perform quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f67236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools\n",
    "import scipy as sp\n",
    "import scipy.signal as sp_sp\n",
    "normalize = lambda x: x / x.sum()\n",
    "truncate  = lambda x, n: np.hstack([x[:n], np.zeros(len(x)-n)])\n",
    "\n",
    "# We only estimate the 0-th component.\n",
    "signal = yy[:, 0]\n",
    "\n",
    "# Estimated signals\n",
    "ESig = {}\n",
    "ESig['Gaussian'] = sp_sp.convolve(signal, normalize(sp_sp.gaussian(30, 3)), 'same')\n",
    "ESig['Wiener']   = sp_sp.wiener(signal)\n",
    "ESig['Butter']   = sp_sp.filtfilt(*sp_sp.butter(10, 0.12), signal, padlen=len(signal)//10)\n",
    "ESig['Spline']   = sp.interpolate.UnivariateSpline(t.kko, signal, s=1e4)(t.kko)\n",
    "ESig['Low-pass'] = np.fft.irfft(truncate(np.fft.rfft(signal), len(signal)//14))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262e3434",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "The following code plots the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4012fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ws.interact(Visible=ws.SelectMultiple(options=['Truth', *ESig, 'My Method',\n",
    "                                                'Kalman smoother', 'Kalman filter', 'Kalman filter a']))\n",
    "def plot_results(Visible):\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    plt.plot(t.kko, yy, 'k.', alpha=0.4, label=\"Obs\")\n",
    "    if 'Truth'           in Visible: plt.plot(t.kk , xx[:, 0]               , 'k', label=\"Truth\")\n",
    "    if 'Kalman smoother' in Visible: plt.plot(t.kk , ExtRTS.stats.mu.u[:, 0], 'm', label=\"K. smoother\")\n",
    "    if 'Kalman filter'   in Visible: plt.plot(t.kk , ExtKF .stats.mu.u[:, 0], 'b', label=\"K. filter\")\n",
    "    if 'Kalman filter a' in Visible: plt.plot(t.kko, ExtKF .stats.mu.a[:, 0], 'b', label=\"K. filter (a)\")\n",
    "    if 'My Method'       in Visible: plt.plot(t.kk , MyMeth.stats.mu.u[:, 0], 'b', label=\"My method\")\n",
    "    for method, estimate in ESig.items():\n",
    "        if method in Visible: plt.plot(t.kko, estimate, label=method)\n",
    "\n",
    "    plt.ylabel('$x^0$, $y$, and $\\hat{x}^0$')\n",
    "    plt.xlabel('Time index ($k$)')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca6e005",
   "metadata": {},
   "source": [
    "Visually, it's hard to imagine better performance than from the Kalman smoother.\n",
    "However, recall the advantage of the Kalman filter (and smoother): *they know the forecast model that generated the truth*.\n",
    "\n",
    "Since the noise levels Q and R are given to the DA methods (but they don't know the actual outcomes/realizations of the random noises), they also do not need any *tuning*, compared to signal processing filters, or choosing between the myriad of signal processing filters [out there](https://docs.scipy.org/doc/scipy/reference/signal.html#module-scipy.signal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f289e730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_error(estimate_at_obs_times):\n",
    "    return np.mean(np.abs(xx[t.kko, 0] - estimate_at_obs_times))\n",
    "\n",
    "for method, estimate in {**ESig,\n",
    "                         'K. smoother': ExtRTS.stats.mu.u[t.kko, 0],\n",
    "                         'K. filter'  : ExtKF .stats.mu.a[:, 0],\n",
    "                         # 'My Method'  : MyMeth.stats.mu.a[:, 0], # uncomment after Exc 8\n",
    "                        }.items():\n",
    "    print(\"%20s\" % method, \"%.4f\" % average_error(estimate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb150da",
   "metadata": {},
   "source": [
    "**Exc 14:** Theoretically, in the long run, the Kalman smoother should yield the optimal result. Verify this by increasing the experiment length to `K=10**4`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60b11b6",
   "metadata": {},
   "source": [
    "**Exc 16:** Re-run the experiment with different parameters, for example the observation noise strength or `dko`.  \n",
    "[Results will differ even if you changed nothing because the truth noises (and obs) are stochastic.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9638cd94",
   "metadata": {},
   "source": [
    "**Exc 18:** Right before executing the assimilations (but after simulating the truth and obs), change $R$ by inserting:\n",
    "\n",
    "    HMM.Obs.noise = GaussRV(C=0.01*np.eye(1))\n",
    "\n",
    "What happens to the estimates of the Kalman filter and smoother?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18788a2",
   "metadata": {},
   "source": [
    "**Exc 20 (optional):** Try out different methods from DAPPER by replacing `MyMethod` below with one of the following:\n",
    " - Climatology\n",
    " - Var3D\n",
    " - OptInterp\n",
    " - EnKF\n",
    " - EnKS\n",
    " - PartFilt\n",
    "\n",
    "You typically also need to set (and possibly tune) some method parameters. Otherwise you will get an error (or possibly the method will perform very badly). You may find (some) documentation for each method in its source code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac263dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyMeth = da.MyMethod(param1=val1, ...)\n",
    "MyMeth.assimilate(HMM, xx, yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd649c0d",
   "metadata": {},
   "source": [
    "### Summary\n",
    "We have derived two forms of the multivariate KF analysis update step: the \"precision matrix\" form, and the \"Kalman gain\" form. The latter is especially practical when the number of observations is smaller than the length of the state vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67a8462",
   "metadata": {},
   "source": [
    "As a subset of state estimation we can do time series estimation\n",
    "[(wherein state-estimation is called state-space approach)](https://www.google.com/search?q=\"We+now+demonstrate+how+to+put+these+models+into+state+space+form\").\n",
    "Moreover, DA methods produce uncertainty quantification, something which is usually more obscure with time series analysis methods.\n",
    "Still, the best is yet to come: the ability to handle very large and chaotic systems\n",
    "(which are more fun than stochastically driven signals such as above)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4b1a71",
   "metadata": {},
   "source": [
    "### Next: [T6 - Spatial statistics (\"geostatistics\") & Kriging](T6%20-%20Geostatistics%20%26%20Kriging.ipynb)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
