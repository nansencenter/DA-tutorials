{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3703145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote = \"https://raw.githubusercontent.com/nansencenter/DA-tutorials\"\n",
    "!wget -qO- {remote}/master/notebooks/resources/colab_bootstrap.sh | bash -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea12014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources import show_answer, interact, import_from_nb\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import scipy.stats as ss\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "plt.ion();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad1ac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "(pdf_G1, grid1d) = import_from_nb(\"T2\", (\"pdf_G1\", \"grid1d\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e48e86",
   "metadata": {},
   "source": [
    "In [T5](T5%20-%20Multivariate%20Kalman%20filter.ipynb#Exc----The-%22Gain%22-form-of-the-KF) we derived the classical Kalman filter (KF),\n",
    "$\n",
    "\\newcommand{\\Expect}[0]{\\mathbb{E}}\n",
    "\\newcommand{\\NormDist}{\\mathscr{N}}\n",
    "\\newcommand{\\DynMod}[0]{\\mathscr{M}}\n",
    "\\newcommand{\\ObsMod}[0]{\\mathscr{H}}\n",
    "\\newcommand{\\mat}[1]{{\\mathbf{{#1}}}}\n",
    "\\newcommand{\\vect}[1]{{\\mathbf{#1}}}\n",
    "\\newcommand{\\trsign}{{\\mathsf{T}}}\n",
    "\\newcommand{\\tr}{^{\\trsign}}\n",
    "\\newcommand{\\ceq}[0]{\\mathrel{≔}}\n",
    "\\newcommand{\\xDim}[0]{D}\n",
    "\\newcommand{\\ta}[0]{\\text{a}}\n",
    "\\newcommand{\\tf}[0]{\\text{f}}\n",
    "\\newcommand{\\I}[0]{\\mat{I}}\n",
    "\\newcommand{\\X}[0]{\\mat{X}}\n",
    "\\newcommand{\\Y}[0]{\\mat{Y}}\n",
    "\\newcommand{\\E}[0]{\\mat{E}}\n",
    "\\newcommand{\\x}[0]{\\vect{x}}\n",
    "\\newcommand{\\y}[0]{\\vect{y}}\n",
    "\\newcommand{\\z}[0]{\\vect{z}}\n",
    "\\newcommand{\\bx}[0]{\\vect{\\bar{x}}}\n",
    "\\newcommand{\\by}[0]{\\vect{\\bar{y}}}\n",
    "\\newcommand{\\bP}[0]{\\mat{P}}\n",
    "\\newcommand{\\barC}[0]{\\mat{\\bar{C}}}\n",
    "\\newcommand{\\ones}[0]{\\vect{1}}\n",
    "\\newcommand{\\AN}[0]{\\big( \\I_N - \\ones \\ones\\tr / N \\big)}\n",
    "$\n",
    "wherein the dynamics (and measurements) are assumed linear,\n",
    "i.e. $\\DynMod, \\ObsMod$ are matrices.\n",
    "But [T6](T6%20-%20Chaos%20%26%20Lorenz%20[optional].ipynb)\n",
    "illustrated several *non-linear* dynamical systems\n",
    "that we would like to be able track (estimate).\n",
    "The classical approach to handle non-linearity\n",
    "is called the *extended* KF (**EKF**), and its derivation is straightforward:\n",
    "replace $\\DynMod \\x^a$ by $\\DynMod(\\x^a)$,\n",
    "and $\\DynMod \\, \\bP^a$ by $\\frac{\\partial \\DynMod}{\\partial \\x}(\\x^a) \\, \\bP^a$\n",
    "(where the Jacobian is the integrated TLM seen in [T6](T6%20-%20Chaos%20%26%20Lorenz%20[optional].ipynb#Error/perturbation-propagation))\n",
    "and do likewise for $\\ObsMod$ with $\\x^f$ and $\\bP^f$.\n",
    "The EKF is still highly useful in many engineering problems,\n",
    "but for the class of problems generally found in geoscience,\n",
    "the TLM linearisation is sometimes too inaccurate (or insufficiently robust to the uncertainty),\n",
    "and the process of deriving and coding up the TLM too arduous\n",
    "(several PhD years, unless auto-differentiable frameworks have been used)\n",
    "or downright illegal (proprietary software).\n",
    "Therefore, another approach is needed...\n",
    "\n",
    "# T7 - The ensemble (Monte-Carlo) approach\n",
    "\n",
    "**Monte-Carlo (M-C) methods** are a class of computational algorithms that rely on random/stochastic sampling.\n",
    "They generally trade off higher (though random!) error for lower technical complexity [<sup>[1]</sup>](#Footnote-1:).\n",
    "Examples from optimisation include randomly choosing search directions, swarms,\n",
    "evolutionary mutations, or perturbations for gradient approximation.\n",
    "But the main application area is the computation of (deterministic) integrals via sample averages,\n",
    "which is rooted in the fact that any integral can be formulated as expectations,\n",
    "as well as the law of large numbers (LLN).\n",
    "Thus M-C methods apply to surprisingly large class of problems, including for\n",
    "example a way to [inefficiently approximate the value of\n",
    "$\\pi$](https://en.wikipedia.org/wiki/Monte_Carlo_method#Overview).\n",
    "Indeed, many of the integrals of interest are inherently expectations.\n",
    "But arising from complicated processes, they are often intractable [<sup>[2]</sup>](#Footnote-2:),\n",
    "whereas a Monte-Carlo sample thereof is obtained simply by repeated simulation.\n",
    "Indeed, the forecast distribution can be expressed precisely by such an integral\n",
    "[[T4]](T4%20-%20Time%20series%20filtering.ipynb#The-(general)-Bayesian-filtering-recursions).\n",
    "\n",
    "Then, similarly to the EKF, the ensemble Kalman filter (**EnKF**) can be derived by replacing\n",
    "$\\DynMod \\x^a$ and $\\DynMod \\, \\bP^a$ by the appropriate ensemble moments (statistics)[<sup>[3]</sup>](#Footnote-3:).\n",
    "The EnKF will be developed in full later – at present, our purpose is to focus on the generation of Monte-Carlo ensembles,\n",
    "and their use to reconstruct (estimate) the underlying distribution.\n",
    "\n",
    "**An ensemble** is an *i.i.d.* sample. I.e. a set of \"members\"\n",
    "(\"particles\", \"realizations\", or \"sample points\") that have been drawn (\"sampled\")\n",
    "independently from the same distribution.\n",
    "With the EnKF, these assumptions are generally tenuous, but pragmatic.\n",
    "In particular, an ensemble can be used to characterize uncertainty:\n",
    "either by using it to compute (estimate) *statistics* thereof, such as the mean, median,\n",
    "variance, covariance, skewness, confidence intervals, etc\n",
    "(any function of the ensemble can be seen as a \"statistic\"),\n",
    "or by using it to reconstruct the distribution/density from which it is sampled.\n",
    "The latter is illustrated by the plot below.\n",
    "Take a moment to digest its code.\n",
    "Note:\n",
    "\n",
    "- The sample/ensemble is plotted as thin narrow lines.\n",
    "  Note that it is generated via `randn`, which samples from $\\NormDist(0, 1)$.\n",
    "- The \"Parametric\" density estimate is defined by estimating the mean and the variance,\n",
    "  and using those estimates to define a Gaussian density (with those parameters).\n",
    "- We will not detail the KDE method, but it can be considered as a \"continuous\" version of a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb570908",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0\n",
    "sigma2 = 25\n",
    "N = 80\n",
    "\n",
    "@interact(              seed=(1, 10), nbins=(2, 60), bw=(0.1, 1))\n",
    "def pdf_reconstructions(seed=5,       nbins=10,      bw=.3):\n",
    "    rnd.seed(seed)\n",
    "    E = mu + np.sqrt(sigma2)*rnd.randn(N)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(grid1d, pdf_G1(grid1d, mu, sigma2), lw=5,                      label=\"True\")\n",
    "    ax.plot(E, np.zeros(N), '|k', ms=100, mew=.4,                          label=\"_raw ens\")\n",
    "    ax.hist(E, nbins, density=1, alpha=.7, color=\"C5\",                     label=\"Histogram\")\n",
    "    ax.plot(grid1d, pdf_G1(grid1d, np.mean(E), np.var(E)), lw=5,           label=\"Parametric\")\n",
    "    ax.plot(grid1d, gaussian_kde(E.ravel(), bw**2).evaluate(grid1d), lw=5, label=\"KDE\")\n",
    "    ax.set_ylim(top=(3*sigma2)**-.5)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972fc3c6",
   "metadata": {},
   "source": [
    "**Exc – A matter of taste?:**\n",
    "- Which approximation to the true pdf looks better?\n",
    "- Which approximation starts with more information?  \n",
    "  What is the downside of making such assumptions?\n",
    "- What value of `bw` causes the \"KDE\" method to most closely\n",
    "  reproduce/recover the \"Parametric\" method?\n",
    "  What about the \"Histogram\" method?  \n",
    "  *PS: we might say that the KDE method \"bridges\" the other two.*.\n",
    "\n",
    "The widget above illustrated how to estimate or reconstruct a distribution on the basis of a sample.\n",
    "But for the EnKF, we also need to know how to go the other way: drawing a sample from a (multivariate) Gaussian distribution...\n",
    "\n",
    "**Exc – Multivariate Gaussian sampling:**\n",
    "Suppose $\\z$ is a standard Gaussian,\n",
    "i.e. $p(\\z) = \\NormDist(\\z \\mid \\vect{0},\\I_{\\xDim})$,\n",
    "where $\\I_{\\xDim}$ is the $\\xDim$-dimensional identity matrix.  \n",
    "Let $\\x = \\mat{L}\\z + \\mu$.\n",
    "\n",
    "- (a – optional) Refer to the exercise on\n",
    "  [change of variables](T2%20-%20Gaussian%20distribution.ipynb#Exc-(optional)----Change-of-variables)\n",
    "  to show that $p(\\x) = \\NormDist(\\x \\mid \\mu, \\mat{C})$,\n",
    "  where $\\mat{C} = \\mat{L}^{}\\mat{L}^T$.\n",
    "- (b) The code below samples $N = 100$ realizations of $\\x$\n",
    "  and collects them in an ${\\xDim}$-by-$N$ \"ensemble matrix\" $\\E$.\n",
    "  But `for` loops are slow in plain Python (and Matlab).\n",
    "  Replace it with something akin to `E = mu + L@Z`.\n",
    "  *Hint: this code snippet fails because it's trying to add a vector to a matrix.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bec352",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.array([1, 100, 5])\n",
    "xDim = len(mu)\n",
    "L = np.diag(1+np.arange(xDim))\n",
    "C = L @ L.T\n",
    "Z = rnd.randn(xDim, N)\n",
    "\n",
    "# Using a loop (\"slow\")\n",
    "E = np.zeros((xDim, N))\n",
    "for n in range(N):\n",
    "    E[:, n] = mu + L@Z[:, n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d254ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('Gaussian sampling', 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f46e7f6",
   "metadata": {},
   "source": [
    "The following prints some numbers that can be used to ascertain if you got it right.\n",
    "Note that the estimates will never be exact:\n",
    "they contain some amount of random error, a.k.a. ***sampling error***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ffb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.printoptions(precision=1, suppress=True):\n",
    "    print(\"Estimated mean =\", np.mean(E, axis=1))\n",
    "    print(\"Estimated cov =\", np.cov(E), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809f7a8a",
   "metadata": {},
   "source": [
    "**Exc – Moment estimation code:** Above, we used numpy's (`np`) functions to compute the sample-estimated mean and covariance matrix,\n",
    "$\\bx$ and $\\barC$,\n",
    "from the ensemble matrix $\\E$.\n",
    "Now, instead, implement these estimators yourself:\n",
    "$$\\begin{align}\\bx &\\ceq \\frac{1}{N}   \\sum_{n=1}^N \\x_n \\,, \\\\\n",
    "   \\barC &\\ceq \\frac{1}{N-1} \\sum_{n=1}^N (\\x_n - \\bx) (\\x_n - \\bx)^T \\,. \\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7227350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't use numpy's mean, cov, but feel free to use a `for` loop.\n",
    "def estimate_mean_and_cov(E):\n",
    "    xDim, N = E.shape\n",
    "\n",
    "    ### FIX THIS ###\n",
    "    x_bar = np.zeros(xDim)\n",
    "    C_bar = np.zeros((xDim, xDim))\n",
    "\n",
    "    return x_bar, C_bar\n",
    "\n",
    "x_bar, C_bar = estimate_mean_and_cov(E)\n",
    "with np.printoptions(precision=1):\n",
    "    print(\"Mean =\", x_bar)\n",
    "    print(\"Covar =\", C_bar, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b3983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('ensemble moments, loop')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd7960a",
   "metadata": {},
   "source": [
    "**Exc – An obsession?:** Why do we normalize by $(N-1)$ for the covariance computation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994e9487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('Why (N-1)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471bc6ef",
   "metadata": {},
   "source": [
    "It can be shown that the above estimators for the mean and the covariance are *consistent and unbiased*.\n",
    "***Consistent*** means that if we let $N \\rightarrow \\infty$, their sampling error will vanish (\"almost surely\").\n",
    "***Unbiased*** means that if we repeat the estimation experiment many times (but use a fixed, finite $N$),\n",
    "then the average of sampling errors will also vanish.\n",
    "Under relatively mild regularity conditions, the [absence of bias implies consistency](https://en.wikipedia.org/wiki/Consistent_estimator#Bias_versus_consistency)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279989f1",
   "metadata": {},
   "source": [
    "The following computes a large number ($K$) of $\\barC$ and $1/\\barC$, estimated with a given ensemble size ($N$).\n",
    "Note that the true variance is $C = 1$.\n",
    "The histograms of the estimates is plotted, along with vertical lines displaying the mean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57e0d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10000\n",
    "@interact(N=(2, 30), bottom=True)\n",
    "def var_and_precision_estimates(N=4):\n",
    "    E = rnd.randn(K, N)\n",
    "    estims = np.var(E, ddof=1, axis=-1)\n",
    "    bins = np.linspace(0, 6, 40)\n",
    "    plt.figure()\n",
    "    plt.hist(estims,   bins, alpha=.6, density=1)\n",
    "    plt.hist(1/estims, bins, alpha=.6, density=1)\n",
    "    plt.axvline(np.mean(estims),   color=\"C0\", label=\"C\")\n",
    "    plt.axvline(np.mean(1/estims), color=\"C1\", label=\"1/C\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a66ff5",
   "metadata": {},
   "source": [
    "**Exc – There's bias, and then there's bias:**\n",
    "- Note that $1/\\barC$ does not appear to be an unbiased estimate of $1/C = 1$.  \n",
    "  Explain this by referring to a well-known property of the expectation, $\\Expect$.  \n",
    "  In view of this, consider the role and utility of \"unbiasedness\" in estimation.\n",
    "- What, roughly, is the dependence of the mean values (vertical lines) on the ensemble size?  \n",
    "  What do they tend to as $N$ goes to $0$?  \n",
    "  What about $+\\infty$ ?\n",
    "- Optional: What are the theoretical distributions of $\\barC$ and $1/\\barC$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb10b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('variance estimate statistics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b387e8",
   "metadata": {},
   "source": [
    "**Exc (optional) – Error notions:**\n",
    " * (a). What's the difference between error and residual?\n",
    " * (b). What's the difference between error and bias?\n",
    " * (c). Show that `\"mean-square-error\" (RMSE^2) = Bias^2 + Var`.  \n",
    "   *Hint: Let $e = \\hat{\\theta} - \\theta$ be the random \"error\" referred to above.\n",
    "   Express each term using the expectation $\\Expect$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ee124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35f757e",
   "metadata": {},
   "source": [
    "**Exc – Vectorization:** Python (numpy) is quicker if you \"vectorize\" loops (similar to Matlab and other high-level languages).\n",
    "This is eminently possible with computations of ensemble moments:\n",
    "Let $\\X \\ceq\n",
    "\\begin{bmatrix}\n",
    "\t\t\\x_1 -\\bx, & \\ldots & \\x_N -\\bx\n",
    "\t\\end{bmatrix} \\,.$\n",
    " * (a). Show that $\\X = \\E \\AN$, where $\\ones$ is the column vector of length $N$ with all elements equal to $1$.  \n",
    "   *Hint: consider column $n$ of $\\X$.*  \n",
    "   *PS: it can be shown that $\\ones \\ones\\tr / N$ and its complement is a \"projection matrix\".*\n",
    " * (b). Show that $\\barC = \\X \\X^T /(N-1)$.\n",
    " * (c). Code up this, latest, formula for $\\barC$ and insert it in `estimate_mean_and_cov(E)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e2f553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('ensemble moments vectorized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584a9ac3",
   "metadata": {},
   "source": [
    "**Exc – Moment estimation code, part 2:** The cross-covariance between two random vectors, $\\bx$ and $\\by$, is given by\n",
    "$$\\begin{align}\n",
    "\\barC_{\\x,\\y}\n",
    "&\\ceq \\frac{1}{N-1} \\sum_{n=1}^N\n",
    "(\\x_n - \\bx) (\\y_n - \\by)^T \\\\\\\n",
    "&= \\X \\Y^T /(N-1)\n",
    "\\end{align}$$\n",
    "where $\\Y$ is, similar to $\\X$, the matrix whose columns are $\\y_n - \\by$ for $n=1,\\ldots,N$.  \n",
    "Note that this is simply the covariance formula, but for two different variables.  \n",
    "I.e. if $\\Y = \\X$, then $\\barC_{\\x,\\y} = \\barC_{\\x}$ (which we have denoted $\\barC$ in the above).\n",
    "\n",
    "Implement the cross-covariance estimator in the code-cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9eb4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_cross_cov(Ex, Ey):\n",
    "    Cxy = np.zeros((len(Ex), len(Ey)))  ### INSERT ANSWER ###\n",
    "    return Cxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45524f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('estimate cross')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75527f06",
   "metadata": {},
   "source": [
    "### What about linearisation?\n",
    "\n",
    "We began this tutorial mentioning that M-C can improve on the TLM\n",
    "for propagating uncertainty (represented by covariances, or an ensemble).\n",
    "But we yet to tie this back into the discussion of linearisation.\n",
    "More specifically, what can we say about the way non-linearity is handled by the EnKF?\n",
    "As it turns out, the EnKF is doing linear least-squares regression [[Anderson (2001)]](#References)\n",
    "and therefore one can reference the Gauss-Markov theorem to make certain optimality (e.g., BLUE) claims.\n",
    "Meanwhile, by viewing the ensemble as a set of finite difference perturbations (with large, pseudo-random spread),\n",
    "ensemble methods were intuitively but heuristically thought to compute an **\"average\"** linear model somewhat.\n",
    "This was formalized by [[Raanes (2019)]](#References),\n",
    "and could have made [[Raanes (2017)]](#References) much shorter (chain rule applies for LLS regression).\n",
    "\n",
    "#### Exc: Stein's lemma\n",
    "\n",
    "TODO:\n",
    "- Univariate\n",
    "- Proof\n",
    "  It is important to note that this derivation of the ensemble linearisation\n",
    "  shows that errors (from different members) cancel out,\n",
    "  and shows exactly the linearisation converges to,\n",
    "  both of which are not present in any derivation starting with Taylor-series expansions.\n",
    "- A similar result was recognized by [[Stordal (2016)]](#References).\n",
    "\n",
    "## Summary\n",
    "\n",
    "Monte-Carlo methods use random sampling to estimate expectations and distributions,\n",
    "making them powerful for complex or nonlinear problems.\n",
    "Ensembles – i.i.d. samples – allow us to estimate statistics and reconstruct distributions,\n",
    "with accuracy improving as the ensemble size grows.\n",
    "Parametric assumptions (e.g. assuming Gaussianity) can be useful in approximating distributions.\n",
    "Sample mean and covariance estimators are consistent and unbiased,\n",
    "but nonlinear functions of these (like the inverse covariance) may be biased.\n",
    "Vectorized computation of ensemble statistics is both efficient and essential for practical use.\n",
    "The ensemble approach naturally handles nonlinearity by simulating the full system,\n",
    "forming the basis for methods like the EnKF.\n",
    "\n",
    "### Next: [T8 - Spatial statistics (\"geostatistics\") & Kriging](T8%20-%20Geostats%20%26%20Kriging%20[optional].ipynb)\n",
    "\n",
    "- - -\n",
    "\n",
    "- ###### Footnote 1:\n",
    "<a name=\"Footnote-1:\"></a>\n",
    "Essentially its (pseudo) randomness means that it is easy to avoid nefarious or hard-to-detect biases.\n",
    "For example, the Monte-Carlo approach is particularly useful\n",
    "when grid-based quadrature is difficult, as is often the case for high-dimensional problems.\n",
    "A common misconception in DA is that M-C is somehow more efficient\n",
    "than deterministic quadrature in high dimensions, $D$.\n",
    "The confusion arises because, from Chebyshev inequality, we know that\n",
    "the error of the M-C approximation asymptotically converges to zero at a rate proportional to $1/\\sqrt{N}$,\n",
    "while that of quadrature methods typically converges proportional to $1 / N^{1/D}$.\n",
    "But not only is the \"starting\" coefficient (not shown) dependent on $D$ (and worse for M-C),\n",
    "also (conjecture:) for any $D$ and $N$ you can always find a gridding strategy that has lower error\n",
    "(for example, quasi-random methods such as latin hypercube sampling are easy to recommended\n",
    "in the pure context of hypercube integrals).\n",
    "- ###### Footnote 2:\n",
    "<a name=\"Footnote-2:\"></a>\n",
    "The corresponding density might involve high-dimensional Jacobians for the change-of-variables formula,\n",
    "or require the Chapman-Kolmogorov equations (or Fokker-Planck in case of continuous time) in the case of interacting random variables.\n",
    "- ###### Footnote 3:\n",
    "<a name=\"Footnote-3:\"></a>\n",
    "Another derivation consists in **hiding** away the non-linearity of $\\ObsMod$ by augmenting the state vector with the observations.\n",
    "We do not favor this approach pedagogically, since it makes it even less clear just what approximations are being made due to the non-linearity.\n",
    "\n",
    "<a name=\"References\"></a>\n",
    "\n",
    "### References"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "jupytext": {
   "formats": "ipynb,scripts//py:light,scripts//md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
