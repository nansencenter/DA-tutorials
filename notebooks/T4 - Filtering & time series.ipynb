{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T4 - Filtering & time series\n",
    "Before we look at the full (multivariate) Kalman filter,\n",
    "let's get more familiar with time-dependent (temporal/sequential) problems.\n",
    "$\n",
    "% START OF MACRO DEF\n",
    "% DO NOT EDIT IN INDIVIDUAL NOTEBOOKS, BUT IN macros.py\n",
    "%\n",
    "\\newcommand{\\Reals}{\\mathbb{R}}\n",
    "\\newcommand{\\Expect}[0]{\\mathbb{E}}\n",
    "\\newcommand{\\NormDist}{\\mathcal{N}}\n",
    "%\n",
    "\\newcommand{\\DynMod}[0]{\\mathscr{M}}\n",
    "\\newcommand{\\ObsMod}[0]{\\mathscr{H}}\n",
    "%\n",
    "\\newcommand{\\mat}[1]{{\\mathbf{{#1}}}}\n",
    "%\\newcommand{\\mat}[1]{{\\pmb{\\mathsf{#1}}}}\n",
    "\\newcommand{\\bvec}[1]{{\\mathbf{#1}}}\n",
    "%\n",
    "\\newcommand{\\trsign}{{\\mathsf{T}}}\n",
    "\\newcommand{\\tr}{^{\\trsign}}\n",
    "\\newcommand{\\tn}[1]{#1}\n",
    "\\newcommand{\\ceq}[0]{\\mathrel{â‰”}}\n",
    "%\n",
    "\\newcommand{\\I}[0]{\\mat{I}}\n",
    "\\newcommand{\\K}[0]{\\mat{K}}\n",
    "\\newcommand{\\bP}[0]{\\mat{P}}\n",
    "\\newcommand{\\bH}[0]{\\mat{H}}\n",
    "\\newcommand{\\bF}[0]{\\mat{F}}\n",
    "\\newcommand{\\R}[0]{\\mat{R}}\n",
    "\\newcommand{\\Q}[0]{\\mat{Q}}\n",
    "\\newcommand{\\B}[0]{\\mat{B}}\n",
    "\\newcommand{\\C}[0]{\\mat{C}}\n",
    "\\newcommand{\\Ri}[0]{\\R^{-1}}\n",
    "\\newcommand{\\Bi}[0]{\\B^{-1}}\n",
    "\\newcommand{\\X}[0]{\\mat{X}}\n",
    "\\newcommand{\\A}[0]{\\mat{A}}\n",
    "\\newcommand{\\Y}[0]{\\mat{Y}}\n",
    "\\newcommand{\\E}[0]{\\mat{E}}\n",
    "\\newcommand{\\U}[0]{\\mat{U}}\n",
    "\\newcommand{\\V}[0]{\\mat{V}}\n",
    "%\n",
    "\\newcommand{\\x}[0]{\\bvec{x}}\n",
    "\\newcommand{\\y}[0]{\\bvec{y}}\n",
    "\\newcommand{\\z}[0]{\\bvec{z}}\n",
    "\\newcommand{\\q}[0]{\\bvec{q}}\n",
    "\\newcommand{\\br}[0]{\\bvec{r}}\n",
    "\\newcommand{\\bb}[0]{\\bvec{b}}\n",
    "%\n",
    "\\newcommand{\\bx}[0]{\\bvec{\\bar{x}}}\n",
    "\\newcommand{\\by}[0]{\\bvec{\\bar{y}}}\n",
    "\\newcommand{\\barB}[0]{\\mat{\\bar{B}}}\n",
    "\\newcommand{\\barP}[0]{\\mat{\\bar{P}}}\n",
    "\\newcommand{\\barC}[0]{\\mat{\\bar{C}}}\n",
    "\\newcommand{\\barK}[0]{\\mat{\\bar{K}}}\n",
    "%\n",
    "\\newcommand{\\D}[0]{\\mat{D}}\n",
    "\\newcommand{\\Dobs}[0]{\\mat{D}_{\\text{obs}}}\n",
    "\\newcommand{\\Dmod}[0]{\\mat{D}_{\\text{obs}}}\n",
    "%\n",
    "\\newcommand{\\ones}[0]{\\bvec{1}}\n",
    "\\newcommand{\\AN}[0]{\\big( \\I_N - \\ones \\ones\\tr / N \\big)}\n",
    "%\n",
    "% END OF MACRO DEF\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import resources.workspace as ws\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Models (HMM)\n",
    "\n",
    "It is generally reasonable to assume that\n",
    "tomorrow depends on today, and only today.\n",
    "For example, if you know the state of the atmosphere today ($k$),\n",
    "then you don't need to know anything about yesterday ($k-1$)\n",
    "to make (initialize) your forecast for tomorrow ($k+1$).\n",
    "In the presence of uncertainty, this is stated formally/symbolically by the **Markov** assumption:\n",
    "$$p(\\x_{k+1} | \\x_k, ..., \\x_0) = p(\\x_{k+1} | \\x_k) \\, \\tag{HMM1}$$\n",
    "\n",
    "This Markovian/dynamic *transition* density (kernel) is assumed known.\n",
    "However, we say that the states are ***hidden*** because they are not directly observed.\n",
    "Instead, we only gain information on them through the observations.\n",
    "It is generally reasonable to assume that the measurement at time $k$\n",
    "only depends on the state at time $k$, i.e.\n",
    "$$p(\\y_k | \\x_k, ..., \\x_0, \\y_k, \\ldots, \\y_1) = p(\\y_k | \\x_k) \\,. \\tag{HMM2}$$\n",
    "which is today's observation likelihood, or *emission* pdf.\n",
    "\n",
    "These two assumptions form what we call a \"Hidden Markov Model\", and are illustrated below, for time $k=0, \\ldots, K$.\n",
    "The arrows indicate causality.\n",
    "   \n",
    "<img width=\"80%\" src=\"./resources/HMM.svg\" alt='Hidden Markov Models'/>\n",
    "\n",
    "*PS: If nature is insufficiently modelled, then these assumptions become tenuous.*\n",
    "\n",
    "*PS: You may have seen a different diagram illustrating HMM's,\n",
    "   with arrows pointing in all kinds of directions, even forming loops.\n",
    "   They are the same thing, but focus only on a single one of the time steps shown above,\n",
    "   for which they show all of the (necessarily discrete) transition probablities.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the above assumptions are very abstract,\n",
    "they still imbue the problem of estimating $\\x_{0:K}$ (shorthand for $\\x_0, \\ldots, \\x_K$) or parts of it,\n",
    "with a structure that we should be able to exploit,\n",
    "even on this most abstract of levels.\n",
    "Indeed, the above HMM assumptions produce the following factorisation\n",
    "$$ \\begin{align*}p(\\x_{0:K} | \\y_{1:K})\n",
    "%&\\propto p(\\x_{0:K}) \\, p(\\y_{1:K} | \\x_{0:K})\\\\\\\n",
    "&\\propto p(\\x_0) \\prod_{k=1}^K p(\\x_k | \\x_{k-1})  \\, p(\\y_k | \\x_k) \\tag{HMM3}\n",
    "\\,,\\end{align*}$$\n",
    "This decomposition means that $p(\\x_{0:K} | \\y_{1:K})$ is not actually a function on the space of dimension $\\dim(\\x_{0:K})$ -- imagine discretising and representing it numerically! -- but rather $2K + 1$ functions on a space of dimensions $\\dim(\\x_k)$.\n",
    "\n",
    "In addition, the decomposition has a particular sequential structure, which we can further exploit.\n",
    "Suppose we wish to forecast tomorrow's weather (or atmospheric state), $\\x_{k+1}$,\n",
    "based on all previous observations, $\\y_{1:k}$, i.e. $ p(\\x_{k+1}|\\y_{1:k})$.\n",
    "Then we need to to consider (i.e. integrate over)\n",
    "all possible $\\x_k$ producing a given value of $\\x_{k+1}$,\n",
    "weighted by the probability of that $\\x_k$ and the transition kernel:\n",
    "$$ p(\\x_{k+1}|\\y_{1:k}) = \\int  p(\\x_{k+1}| \\x_k ) \\, p(\\x_k|\\y_{1:k})\\, d \\x_k \\,, \\tag{HMM4}$$\n",
    "This **forecast** equation is also known as the Chapman-Kolmogorov equation, and is equivalent to the PDE called the Fokker-Planck equations. Either way, the forecast must be \"initialized\" by the density $p(\\x_k|\\y_{1:k})$, which is called today's **analysis**.\n",
    "It can, in turn, be written using Bayes' rule for today's observations:\n",
    "$$ p(\\x_k| \\y_{1:k}) \\propto p(\\y_k | \\x_k ) \\, p(\\x_k| \\y_{1:k-1}) \\,. \\tag{HMM5}$$\n",
    "Note that the prior is actually *yesterday's forecast*, $p(\\x_k| \\y_{1:k-1})$.\n",
    "Thus, the above two steps (forecast and conditioning/update) can be applied for sequentially increasing time, building on the previous estimates. They are known as the Bayesian **filtering** recursions. The benefit of them being recursive is that we do not need to re-do the whole computation every time, but can build our next estimate based on the previous one.\n",
    "\n",
    "**Exc (optional):** Derive eqn's HMM-3-5 from eqn's HMM-1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A straight-line example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the straight line ($x_k$) for time index $k=1, 2, \\ldots, K$, specified by\n",
    "$$\\begin{align}\n",
    "x_k = a k \\, , \\tag{1}\n",
    "\\end{align}$$\n",
    "where the slope ($a$) is unknown.\n",
    "Also suppose we have observations ($y$) of the line, but corrupted by noise ($r$):\n",
    "$$\\begin{align}\n",
    "y_k &= x_k + r_k \\, , \\tag{2}\n",
    "\\end{align}$$\n",
    "where $r_k \\sim \\mathcal{N}(0, R)$ for some $R>0$.\n",
    "The code below sets up an experiment based on eqns. (1) and (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "a = 0.4\n",
    "K = 10\n",
    "R = 1\n",
    "\n",
    "# Naming convention: xx and yy hold time series of x and y.\n",
    "xx = np.zeros(K+1) # truth states\n",
    "yy = np.zeros(K+1) # obs\n",
    "\n",
    "# Simulate synthetic truth (x) and obs(y)\n",
    "for k in 1+np.arange(K):\n",
    "    xx[k] = a*k\n",
    "    yy[k] = xx[k] + np.sqrt(R)*rnd.randn()\n",
    "\n",
    "# The obs at k==0 should not be used (since we know xx[0]==0, it is worthless).\n",
    "yy[0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ws.interact(k=ws.IntSlider(min=1, max=K))\n",
    "def plot_experiment(k):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    kk = np.arange(k+1)\n",
    "    plt.plot(kk, xx[kk], 'k' , label='true state ($x$)')\n",
    "    plt.plot(kk, yy[kk], 'k*', label='noisy obs ($y$)')\n",
    "\n",
    "    ### Uncomment this block AFTER doing the Exc 3.4 ###\n",
    "    # plt.plot(kk, kk*lin_reg(k), 'r', label='Linear regress.')\n",
    "\n",
    "    ### Uncomment this block AFTER doing the Exc 3.8 ###\n",
    "    # pw_bb, pw_xxhat = ws.weave_fa(bb, xxhat)\n",
    "    # pw_kf, pw_ka    = ws.weave_fa(np.arange(K+1))\n",
    "    # plt.plot(pw_kf[:3*k], pw_bb[:3*k]   , 'c'  , label='KF forecasts')\n",
    "    # plt.plot(pw_ka[:3*k], pw_xxhat[:3*k], 'b'  , label='KF analyses')\n",
    "    # #plt.plot(kk, kk*xxhat[k]/k         , 'g--', label='KF extrapolated')\n",
    "\n",
    "    plt.xlim([0, 1.01*K])\n",
    "    plt.ylim([-1, 1.2*a*K])\n",
    "    plt.xlabel('time index (k)')\n",
    "    plt.ylabel('$x$, $y$, and $\\hat{x}$')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation by linear regression\n",
    "The observations eqn. (2)\n",
    "yields the likelihood\n",
    "$$\\begin{align}\n",
    "p(y_k|x_k) = \\mathcal{N}(y_k \\mid x_k, R) \\, . \\tag{3}\n",
    "\\end{align}$$\n",
    "Hopefully this is intuitive; otherwise, a derivation is provided in T4.\n",
    "\n",
    "(Least-squares) linear regression minimizes the cost/objective function\n",
    "$$\\begin{align}\n",
    "J_K(a) = \\sum_{k=1}^K (y_k - a k)^2 \\, ,  \\tag{4}\n",
    "\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.2:** Use eqns. (1) and (2) and the logarithm to derive $J_K(a)$ from the likelihood $p\\, (y_1, \\ldots, y_K \\;|\\; a)$.  \n",
    "Explain (prove) that their optimum points will be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('LinReg deriv a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.3:** Show that the optimisation yields the estimator\n",
    "$$\\begin{align}\n",
    "\\hat{a} = \\frac{\\sum_{k=1}^K {k} y_{k}}{\\sum_{k=1}^K {k}^2} \\, . \\tag{6}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('LinReg deriv b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.4:** Code up the linear regression estimator (6).  \n",
    "Then, go back to the animation above and uncomment the block that plots the its estimates.\n",
    "If you did it right, then the estimated line should look reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg(k):\n",
    "    \"Liner regression estimator based on observations y_1, ..., y_k.\"\n",
    "    # PS: the observations (yy) are not among the input args\n",
    "    #     because you can just grab them from the global namespace.\n",
    "    ### INSERT ANSWER HERE ###\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('LinReg_k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we tackle the same problem, but using the Kalman filter.\n",
    "\n",
    "# Estimation by the (univariate) Kalman filter (KF)\n",
    "The KF assumes that the (\"true/nature\") state, $x_k$, evolves recursively in time (indexed by $k$) according to\n",
    "$$\\begin{align}\n",
    "x_{k} = \\DynMod_{k-1} x_{k-1} + q_{k-1} \\, , \\tag{Dyn}\n",
    "\\end{align}$$\n",
    "where $\\DynMod_{k-1}$ is called the \"dynamical\" model, and $q_k$ is a random noise (process) that accounts for \"model errors\".\n",
    "For now, $\\DynMod_{k-1}$ is just a given number (function of $k$). In later tutorials we will  generalize it to matrices, and eventually nonlinear operators (functions).\n",
    "\n",
    "####  The forecast step\n",
    "Suppose that $\\quad\\quad\\;\\;\\quad x_{k-1} \\sim \\mathcal{N}(\\hat{x}_{k-1}, P_{k-1})$,  \n",
    "and that (independently) $q_{k-1} \\sim \\mathcal{N}(0, Q_{k-1})$.\n",
    "\n",
    "By eqn. (Dyn), the mean of $x_{k}$, i.e. $b_k = \\mathbb{E}[x_k]$, is a linear function of the mean of $x_{k-1}$:\n",
    "$$\\begin{align}\n",
    "b_k &= \\DynMod_{k-1} \\hat{x}_{k-1} \\,, \\tag{9}\n",
    "\\end{align}$$\n",
    "since taking the expectation, $\\mathbb{E}$, is a [linear operation](https://en.wikipedia.org/wiki/Expected_value#Properties).  \n",
    "Meanwhile, by the [properties of variance](https://en.wikipedia.org/wiki/Variance#Propagation),\n",
    "the model gets squared in the variance of $x_{k}$,\n",
    "and the error variance is an added term\n",
    "$$\\begin{align}\n",
    "B_k &= \\DynMod_{k-1}^2 P_{k-1} + Q_{k-1} \\,. \\tag{10}\n",
    "\\end{align}$$\n",
    "\n",
    "It can also be shown that [the sum of two Gaussian random variables](https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables#Proof_using_convolutions)\n",
    "is also Gaussian.\n",
    "In summary, eqn. (Dyn) yields\n",
    "$$x_k \\sim \\mathcal{N}(b_k, B_k) \\,, \\tag{8}$$\n",
    "for some $b_k$ and $B_k$ which we can compute from the previous mean and variance, i.e. $\\hat{x}_{k-1}$ and $P_{k-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('RV sums')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.6 (a):** For the KF we want to reformulate our *example problem* of estimating the parameter $a$ as the problem of estimating $x_k$.\n",
    "\n",
    "Derive the \"forecast/dynamical model\" $\\DynMod_k$, as well as $q_k$, such that eqn. (Dyn) is equivalent to eqn (1).\n",
    "\n",
    "Then implement it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mod(k):\n",
    "    return ### INSERT ANSWER HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Sequential 2 Recursive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.6 (b):** The KF may seem like \"overkill\" for our simple example problem.\n",
    "But this \"heavy machinery\" can do a lot more, and will pay off later.\n",
    "Based on the above, *why* is it we can say that the KF can do more?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The analysis step\n",
    "\"updates\" the prior (forecast), $\\mathcal{N}(x_k \\mid \\; b_k,\\; B_k)$, given by eqns. (8), (9), (10),  \n",
    "based on the likelihood, $\\quad\\;\\;\\;\\, \\mathcal{N}(y_k \\mid \\, x_k, \\; R)$,  \n",
    "into the posterior (analysis), $\\; \\; \\, \\mathcal{N}(x_k \\mid \\; \\hat{x}_{k}, \\, P_{k})$, given by\n",
    "the update formulae derived as the Gaussian-Gaussian Bayes' rule in [Exc 2.18 of the previous tutorial](T3%20-%20Bayesian%20inference.ipynb#Exc--2.18-'Gaussian-Gaussian-Bayes':).\n",
    "\n",
    "This completes the KF cycle, which can then restart with the forecast from $k$ to $k+1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 0 # Dynamical model noise strength\n",
    "\n",
    "# Allocation\n",
    "bb    = np.zeros(K+1) # mean estimates -- prior/forecast values\n",
    "xxhat = np.zeros(K+1) # mean estimates -- post./analysis values\n",
    "BB    = np.zeros(K+1) # var  estimates -- prior/forecast values\n",
    "PP    = np.zeros(K+1) # var  estimates -- post./analysis values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.8:** Following the pattern of the code blocks below,\n",
    "implement the KF to estimate $x_k$ for a given $k$ based on the estimate of $k-1$.\n",
    "\n",
    "<mark><font size=\"-1\">\n",
    "<b>NB:</b> for this example, do not use the \"Kalman gain\" form of the analysis update.\n",
    "This problem involves the peculiar, unrealistic situation of infinities\n",
    "(related to \"improper priors\") at `k==1`, yielding platform-dependent behaviour.\n",
    "These peculiarities are of mainly of academic interest.\n",
    "</font></mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KF(k):\n",
    "    \"Cycle k of the Kalman filter\"\n",
    "    # Forecast\n",
    "    if k==1:\n",
    "        BB[k] = np.inf # The \"initial\" prior uncertainty is infinite...\n",
    "        bb[k] = 0      # ... thus the corresponding mean is inconsequential.\n",
    "    else:\n",
    "        BB[k] = ### INSERT ANSWER HERE ###\n",
    "        bb[k] = ### INSERT ANSWER HERE ###\n",
    "    # Analysis\n",
    "    PP[k]    = ### INSERT ANSWER HERE ###\n",
    "    xxhat[k] = ### INSERT ANSWER HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('KF_k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the estimation computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in 1+np.arange(K):\n",
    "    KF(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.10:** Go back to the animation above and uncomment the block that plots the KF estimates.  \n",
    "Visually: what is the relationship between the estimates provided by the KF and by linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('LinReg compare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.12 (optional):** This exercise proves (on paper) the conclusion of the previous exercise.\n",
    "\n",
    "Firstly, note that the KF forecast step (here with $Q=0$) can be inserted in the analysis step, forming a single couple of recursions:\n",
    "$$\\begin{align}\n",
    "\\hat{x}_k &= P_k \\big(y_k/R \\;+\\; \\DynMod_{k-1} \\hat{x}_{k-1} / [\\DynMod_{k-1}^2 P_{k-1}] \\big) \\tag{11} \\, , \\\\\\\n",
    "P_k &= 1/\\big(1/R \\;+\\; 1/[\\DynMod_{k-1}^2 P_{k-1}]\\big) \\tag{12} \\, .\n",
    "\\end{align}$$\n",
    "\n",
    "Use this and Exc 3.6 (a) to show that\n",
    "$$\\begin{align}\n",
    "&\\text{firstly,} &P_K &= R\\frac{K^2}{\\sum_{k=1}^K k^2} \\, , \\tag{13} \\\\\\\n",
    "&\\text{secondly,} &\\hat{x}_K &= K\\frac{\\sum_{k=1}^K k y_k}{\\sum_{k=1}^K k^2} = K \\hat{a}_K \\tag{14} \\, ,\n",
    "\\end{align}$$\n",
    "where $\\hat{a}_K$\n",
    "is given by eqn. (6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('x_KF == x_LinReg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exc 3.14:\n",
    "Set $Q=0$ in eqn (Dyn) so that $x_{k+1} = \\DynMod x_k$ *for some constant $\\DynMod>1$*.\n",
    "\n",
    "What does the sequence of $P_k$ converge to?  \n",
    "*Hint: Start from eqn (12) [eqn (13) is for the straight-line example only] and find its \"fixed point.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Asymptotic P when M>1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exc 3.15:\n",
    "Redo Exc 3.14, but assuming  \n",
    " * (a) $\\DynMod = 1$.\n",
    " * (b) $\\DynMod < 1$.\n",
    "In these cases it is not so fruitful to use the fixed point equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Asymptotic P when M=1')\n",
    "# ws.show_answer('Asymptotic P when M<1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, if $\\DynMod>1$, the KF state's uncertainty variance, $P_k$ does not converge to 0. This is because, even though you keep gaining more information, this gets balanced out by the growth in uncertainty during the forecast. On the other hand, if $\\DynMod \\leq 1$ then the error converges to zero.\n",
    "\n",
    "In general, however, $\\DynMod$, $Q$, $R$  depend on time, $k$ (often to parameterize exogenous/outside factors/forces/conditions), and there is no limit value that the state distribution (and its parameters) converges to.\n",
    "\n",
    "A particular exception is the above straight-line example. As we found above, $\\DynMod_k =\\frac{k+1}{k}$, which depends on time, and yet its limiting value can be found through eqn. (13); moreover, eqn. (13) and [the pyramidal sum](https://en.wikipedia.org/wiki/Square_pyramidal_number) can be used to show that $P_k \\rightarrow 0$, even though $\\forall k, \\; \\DynMod_k > 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.18 (optional):** Set $Q$ to 1 or more in the KF code, and re-compute its estimates. Explain why the KF estimate is now closer to the obs (always at the latest time instance) than the linear regression estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.20 (optional):** Now change $R$ (but don't re-run the simulation of the truth and obs). The KF estimates should not change (in this particular example). Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "The KF consists of two steps:\n",
    " * Forecast\n",
    " * Analysis\n",
    " \n",
    "In each step, the mean and variance must be updated.\n",
    "\n",
    "As an example, we saw that the linear regression estimate is reproduced by the KF, although it is a bit tricky to initialize the KF with infinite uncertainty. However, the KF (i.e. state estimation) is much more general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: [T5 - The Kalman filter (multivariate)](T5%20-%20Kalman%20filter%20(multivariate).ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
