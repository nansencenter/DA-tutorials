{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -qO- https://raw.githubusercontent.com/nansencenter/DA-tutorials/master/notebooks/resources/colab_bootstrap.sh | bash -s\n",
    "from resources.workspace import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "%MACRO DEFINITION\n",
    "\\newcommand{\\Reals}{\\mathbb{R}}\n",
    "\\newcommand{\\Imags}{i\\Reals}\n",
    "\\newcommand{\\Integers}{\\mathbb{Z}}\n",
    "\\newcommand{\\Naturals}{\\mathbb{N}}\n",
    "%\n",
    "\\newcommand{\\Expect}[0]{\\mathop{}\\! \\mathbb{E}}\n",
    "\\newcommand{\\NormDist}{\\mathop{}\\! \\mathcal{N}}\n",
    "%\n",
    "\\newcommand{\\mat}[1]{{\\mathbf{{#1}}}} \n",
    "%\\newcommand{\\mat}[1]{{\\pmb{\\mathsf{#1}}}}\n",
    "\\newcommand{\\bvec}[1]{{\\mathbf{#1}}}\n",
    "%\n",
    "\\newcommand{\\trsign}{{\\mathsf{T}}}\n",
    "\\newcommand{\\tr}{^{\\trsign}}\n",
    "%\n",
    "\\newcommand{\\I}[0]{\\mat{I}}\n",
    "\\newcommand{\\K}[0]{\\mat{K}}\n",
    "\\newcommand{\\bP}[0]{\\mat{P}}\n",
    "\\newcommand{\\bH}[0]{\\mat{H}}\n",
    "\\newcommand{\\bF}[0]{\\mat{F}}\n",
    "\\newcommand{\\R}[0]{\\mat{R}}\n",
    "\\newcommand{\\Q}[0]{\\mat{Q}}\n",
    "\\newcommand{\\B}[0]{\\mat{B}}\n",
    "\\newcommand{\\Ri}[0]{\\R^{-1}}\n",
    "\\newcommand{\\Bi}[0]{\\B^{-1}}\n",
    "\\newcommand{\\X}[0]{\\mat{X}}\n",
    "\\newcommand{\\A}[0]{\\mat{A}}\n",
    "\\newcommand{\\Y}[0]{\\mat{Y}}\n",
    "\\newcommand{\\E}[0]{\\mat{E}}\n",
    "\\newcommand{\\U}[0]{\\mat{U}}\n",
    "\\newcommand{\\V}[0]{\\mat{V}}\n",
    "%\n",
    "\\newcommand{\\x}[0]{\\bvec{x}}\n",
    "\\newcommand{\\y}[0]{\\bvec{y}}\n",
    "\\newcommand{\\q}[0]{\\bvec{q}}\n",
    "\\newcommand{\\br}[0]{\\bvec{r}}\n",
    "\\newcommand{\\bb}[0]{\\bvec{b}}\n",
    "%\n",
    "\\newcommand{\\cx}[0]{\\text{const}}\n",
    "\\newcommand{\\norm}[1]{\\|{#1}\\|}\n",
    "\\newcommand{\\tn}[1]{#1}\n",
    "%\n",
    "\\newcommand{\\bx}[0]{\\bvec{\\bar{x}}}\n",
    "\\newcommand{\\barP}[0]{\\mat{\\bar{P}}}\n",
    "\\newcommand{\\barK}[0]{\\mat{\\bar{K}}}\n",
    "\\newcommand{\\D}[0]{\\mat{D}}\n",
    "\\newcommand{\\Dobs}[0]{\\mat{D}_{\\text{obs}}}\n",
    "\\newcommand{\\Dmod}[0]{\\mat{D}_{\\text{mod}}}\n",
    "\\newcommand{\\ones}[0]{\\bvec{1}}\n",
    "$\n",
    "In this tutorial we shall derive:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the Kalman filter for multivariate systems.  \n",
    "\n",
    "The [forecast step](T3%20-%20Univariate%20Kalman%20filtering.ipynb#Exc-3.7:-The-forecast-step:)\n",
    "remains essentially unchanged.\n",
    "The only difference is that $\\mathscr{M}$ is now a matrix, as well as the use of the transpose ${}^T$ in the covariance equation:\n",
    "$\\begin{align}\n",
    "\\mathbf{b}_k\n",
    "&= \\mathscr{M}_{k-1} \\mathbf{\\hat{\\x}}_{k-1} \\, , \\tag{1a} \\\\\\\n",
    "\\mathbf{B}_k\n",
    "&= \\mathscr{M}_{k-1} \\bP_{k-1} \\mathscr{M}_{k-1}^T + \\Q_{k-1} \\, . \\tag{1b}\n",
    "\\end{align}$\n",
    "\n",
    "However, the analysis step [[Exc 2.18](T2%20-%20Bayesian%20inference.ipynb#Exc--2.18-'Gaussian-Bayes':)] gets a little more complicated...\n",
    "\n",
    "#### Exc 2 (The likelihood):\n",
    "<mark><font size=\"-1\">\n",
    "The analysis step is only concerned with a single time (index). We therefore drop the $k$ subscript in the following.\n",
    "</font></mark>\n",
    "\n",
    "Suppose the observation, $\\y$, is related to the true state, $\\x$, via a (possibly rectangular) matrix, $\\bH$:\n",
    "\\begin{align*}\n",
    "\\y &= \\bH \\x + \\br \\, , \\;\\; \\qquad (2)\n",
    "\\end{align*}\n",
    "where the noise follows the law $\\br \\sim \\NormDist(\\mathbf{0}, \\R)$ for some $\\R>0$ (i.e. $\\mathbf{R}$ is symmetric-positive-definite).\n",
    "\n",
    "\n",
    "Derive the expression for the likelihood, $p(\\mathbf{y}|\\mathbf{x})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_answer('Likelihood derivation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following exercise derives the analysis step\n",
    "\n",
    "#### Exc 4 (The 'precision' form of the KF):\n",
    "Similarly to [Exc 2.18](T2%20-%20Bayesian%20inference.ipynb#Exc--2.18-'Gaussian-Bayes':),\n",
    "it may be shown that the prior $p(\\x) = \\NormDist(\\x \\mid \\bb,\\B)$\n",
    "and likelihood $p(\\y|\\x) = \\NormDist(\\y \\mid \\bH \\x,\\R)$,\n",
    "yield the posterior:\n",
    "\\begin{align}\n",
    "p(\\x|\\y)\n",
    "&= \\NormDist(\\x \\mid \\hat{\\x}, \\bP) \\tag{4}\n",
    "\\, ,\n",
    "\\end{align}\n",
    "where the posterior/analysis mean (vector) and covariance (matrix) are given by:\n",
    "\\begin{align}\n",
    "\t\t\t\\bP &= (\\bH\\tr \\Ri \\bH + \\Bi)^{-1} \\, , \\tag{5} \\\\\n",
    "\t\t\t\\hat{\\x} &= \\bP\\left[\\bH\\tr \\Ri \\y + \\Bi \\bb\\right] \\tag{6}Â \\, ,\n",
    "\\end{align}\n",
    "Prove eqns (4-6).  \n",
    "Hint: as in [Exc 2.18](T2%20-%20Bayesian%20inference.ipynb#Exc--2.18-'Gaussian-Bayes':), the main part lies in \"completing the square\" in $\\x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_answer('KF precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark><font size=\"-1\">\n",
    "We have now derived (one form of) the Kalman filter. In the multivariate case,\n",
    "we know how to:\n",
    "<ul>\n",
    "  <li>Propagate our estimate of $\\x$ to the next time step using eqns (1a) and (1b). </li>\n",
    "  <li>Update our estimate of $\\x$ by assimilating the latest observation $\\y$, using eqns (5) and (6).</li>\n",
    "</ul>\n",
    "</font></mark>\n",
    "\n",
    "However, the computations can be pretty expensive..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 5:** Suppose $\\mathbf{x}$ is $M$-dimensional and has a covariance matrix $\\mathbf{B}$.\n",
    " * (a). What's the size of $\\mathbf{B}$?\n",
    " * (b). How many \"flops\" (approximately, i.e. to leading order) are required  \n",
    " to compute the \"precision form\" of the KF update equation, eqn (5) ?\n",
    " * (c). How much memory (bytes) is required to hold its covariance matrix $\\mathbf{B}$ ?\n",
    " * (d). How many mega bytes's is this if $M$ is a million?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_answer('Cov memory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of the principal reasons why basic extended KF is infeasible for DA.  \n",
    "The following derives another, often more practical, form of the KF analysis update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exc 6 (The \"Woodbury\" matrix inversion identity):\n",
    "The following is known as the Sherman-Morrison-Woodbury lemma/identity,\n",
    "$$\\begin{align}\n",
    "    \\bP = \\left( \\B^{-1} + \\V\\tr \\R^{-1} \\U \\right)^{-1}\n",
    "    =\n",
    "    \\B - \\B \\V\\tr \\left( \\R + \\U \\B \\V\\tr \\right)^{-1} \\U \\B \\, ,\n",
    "    \\tag{W}\n",
    "\\end{align}$$\n",
    "which holds for any (suitably shaped matrices)\n",
    "$\\B$, $\\R$, $\\V,\\U$ *such that the above exists*.\n",
    "\n",
    "Prove the identity. Hint: don't derive it, just prove it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_answer('Woodbury')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exc 7:\n",
    "- Show that $\\B$ and $\\R$ must be square.\n",
    "- Show that $\\U$ and $\\V$ are not necessarily square, but must have the same dimensions.\n",
    "- Show that $\\B$ and $\\R$ are not necessarily of equal size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exc 7 makes it clear that the Woodbury identity may be used to compute $\\bP$ by inverting matrices of the size of $\\R$ rather than the size of $\\B$.\n",
    "Of course, if $\\R$ is bigger than $\\B$, then the identity is useful the other way around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exc 8 (Corollary 1):\n",
    "Prove that, for any symmetric, positive-definite (SPD) matrices $\\R$ and $\\B$, and any matrix $\\bH$,\n",
    "$$\\begin{align}\n",
    " \t\\left(\\bH\\tr \\R^{-1} \\bH + \\B^{-1}\\right)^{-1}\n",
    "    &=\n",
    "    \\B - \\B \\bH\\tr \\left( \\R + \\bH \\B \\bH\\tr \\right)^{-1} \\bH \\B \\tag{C1}\n",
    "    \\, .\n",
    "\\end{align}$$\n",
    "Hint: consider the properties of [SPD](https://en.wikipedia.org/wiki/Definiteness_of_a_matrix#Properties) matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_answer('Woodbury C1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exc 10 (Corollary 2):\n",
    "Prove that, for the same matrices as for Corollary C1,\n",
    "$$\\begin{align}\n",
    "\t\\left(\\bH\\tr \\R^{-1} \\bH + \\B^{-1}\\right)^{-1}\\bH\\tr \\R^{-1}\n",
    "    &= \\B \\bH\\tr \\left( \\R + \\bH \\B \\bH\\tr \\right)^{-1}\n",
    "    \\tag{C2}\n",
    "    \\, .\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_answer('Woodbury C2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exc 12 (The \"gain\" form of the KF):\n",
    "Now, let's go back to the KF, eqns (5) and (6). Since $\\B$ and $\\R$ are covariance matrices, they are symmetric-positive. In addition, we will assume that they are full-rank, making them SPD and invertible.  \n",
    "\n",
    "Define the Kalman gain by:\n",
    " $$\\begin{align}\n",
    "    \\K &= \\B \\bH\\tr \\big(\\bH \\B \\bH\\tr + \\R\\big)^{-1} \\, . \\tag{K1}\n",
    "\\end{align}$$\n",
    " * (a) Apply (C1) to eqn (5) to obtain the Kalman gain form of analysis/posterior covariance matrix:\n",
    "$$\\begin{align}\n",
    "    \\bP &= [\\I_M - \\K \\bH]\\B \\, . \\tag{8}\n",
    "\\end{align}$$\n",
    "\n",
    "* (b) Apply (C2)  to (5) to abtain the identity\n",
    "$$\\begin{align}\n",
    "    \\K &= \\bP \\bH\\tr \\R  \\, . \\tag{K2}\n",
    "\\end{align}$$\n",
    "\n",
    "* (c) Show that $\\bP \\Bi = [\\I_M - \\K \\bH]$.\n",
    "* (d) Use (b) and (c) to obtain the Kalman gain form of analysis/posterior covariance\n",
    "$$\\begin{align}\n",
    "     \\hat{\\x} &= \\bb + \\K\\left[\\y - \\bH \\bb\\right] \\, . \\tag{9}\n",
    "\\end{align}$$\n",
    "\n",
    "Together, eqns (8) and (9) define the Kalman gain form of the KF update.\n",
    "The inversion (eqn 7) involved is of the size of $\\R$, while in eqn (5) it is of the size of $\\B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In summary: \n",
    "We have derived two forms of the multivariate KF analysis update step: the \"precision matrix\" form, and the \"Kalman gain\" form. The latter is especially practical when the number of observations is smaller than the length of the state vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: [Time series analysis](T5%20-%20Time%20series%20analysis.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
