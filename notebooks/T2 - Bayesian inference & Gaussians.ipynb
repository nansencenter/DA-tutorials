{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import resources.workspace as ws\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "% START OF MACRO DEF\n",
    "% DO NOT EDIT IN INDIVIDUAL NOTEBOOKS, BUT IN macros.py\n",
    "%\n",
    "\\newcommand{\\Reals}{\\mathbb{R}}\n",
    "\\newcommand{\\Expect}[0]{\\mathbb{E}}\n",
    "\\newcommand{\\NormDist}{\\mathcal{N}}\n",
    "%\n",
    "\\newcommand{\\DynMod}[0]{\\mathscr{M}}\n",
    "\\newcommand{\\ObsMod}[0]{\\mathscr{H}}\n",
    "%\n",
    "\\newcommand{\\mat}[1]{{\\mathbf{{#1}}}}\n",
    "%\\newcommand{\\mat}[1]{{\\pmb{\\mathsf{#1}}}}\n",
    "\\newcommand{\\bvec}[1]{{\\mathbf{#1}}}\n",
    "%\n",
    "\\newcommand{\\trsign}{{\\mathsf{T}}}\n",
    "\\newcommand{\\tr}{^{\\trsign}}\n",
    "\\newcommand{\\tn}[1]{#1}\n",
    "\\newcommand{\\ceq}[0]{\\mathrel{≔}}\n",
    "%\n",
    "\\newcommand{\\I}[0]{\\mat{I}}\n",
    "\\newcommand{\\K}[0]{\\mat{K}}\n",
    "\\newcommand{\\bP}[0]{\\mat{P}}\n",
    "\\newcommand{\\bH}[0]{\\mat{H}}\n",
    "\\newcommand{\\bF}[0]{\\mat{F}}\n",
    "\\newcommand{\\R}[0]{\\mat{R}}\n",
    "\\newcommand{\\Q}[0]{\\mat{Q}}\n",
    "\\newcommand{\\B}[0]{\\mat{B}}\n",
    "\\newcommand{\\C}[0]{\\mat{C}}\n",
    "\\newcommand{\\Ri}[0]{\\R^{-1}}\n",
    "\\newcommand{\\Bi}[0]{\\B^{-1}}\n",
    "\\newcommand{\\X}[0]{\\mat{X}}\n",
    "\\newcommand{\\A}[0]{\\mat{A}}\n",
    "\\newcommand{\\Y}[0]{\\mat{Y}}\n",
    "\\newcommand{\\E}[0]{\\mat{E}}\n",
    "\\newcommand{\\U}[0]{\\mat{U}}\n",
    "\\newcommand{\\V}[0]{\\mat{V}}\n",
    "%\n",
    "\\newcommand{\\x}[0]{\\bvec{x}}\n",
    "\\newcommand{\\y}[0]{\\bvec{y}}\n",
    "\\newcommand{\\z}[0]{\\bvec{z}}\n",
    "\\newcommand{\\q}[0]{\\bvec{q}}\n",
    "\\newcommand{\\br}[0]{\\bvec{r}}\n",
    "\\newcommand{\\bb}[0]{\\bvec{b}}\n",
    "%\n",
    "\\newcommand{\\bx}[0]{\\bvec{\\bar{x}}}\n",
    "\\newcommand{\\by}[0]{\\bvec{\\bar{y}}}\n",
    "\\newcommand{\\barB}[0]{\\mat{\\bar{B}}}\n",
    "\\newcommand{\\barP}[0]{\\mat{\\bar{P}}}\n",
    "\\newcommand{\\barC}[0]{\\mat{\\bar{C}}}\n",
    "\\newcommand{\\barK}[0]{\\mat{\\bar{K}}}\n",
    "%\n",
    "\\newcommand{\\D}[0]{\\mat{D}}\n",
    "\\newcommand{\\Dobs}[0]{\\mat{D}_{\\text{obs}}}\n",
    "\\newcommand{\\Dmod}[0]{\\mat{D}_{\\text{obs}}}\n",
    "%\n",
    "\\newcommand{\\ones}[0]{\\bvec{1}}\n",
    "\\newcommand{\\AN}[0]{\\big( \\I_N - \\ones \\ones\\tr / N \\big)}\n",
    "%\n",
    "% END OF MACRO DEF\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before discussing sequential (time-dependent) inference,\n",
    "we need to know how to estimate unknowns a single data/observations (vector),\n",
    "But before discussing \"Bayes' rule\",\n",
    "we should review the most useful of probability distributions, namely\n",
    "# The Gaussian (Normal) distribution\n",
    "\n",
    "Consider the Gaussian random variable $x \\sim \\mathcal{N}(b, B)$.\n",
    "\n",
    "Equivalently, we may write\n",
    "$\\begin{align}\n",
    "p(x) = \\mathcal{N}(x \\mid b, B)\n",
    "\\end{align}$\n",
    "for its probability density function (**pdf**), which is given by\n",
    "$$\\begin{align}\n",
    "\\mathcal{N}(x \\mid b, B) = (2 \\pi B)^{-1/2} e^{-(x-b)^2/2 B} \\, , \\tag{G1}\n",
    "\\end{align}$$\n",
    "for $x \\in (-\\infty, +\\infty)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.2:** Code it up (complete the code below)! Hints:\n",
    "* Note that `**` is the exponentiation/power operator\n",
    "* $e^x$ is available as `np.exp(x)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_G1(x, b, B):\n",
    "    \"Univariate (scalar), Gaussian pdf\"\n",
    "    ### INSERT ANSWER HERE ###\n",
    "    return pdf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('pdf_G1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid\n",
    "bounds = -20, 20\n",
    "N  = 201                    # num of grid points\n",
    "xx = np.linspace(*bounds,N) # grid\n",
    "dx = xx[1]-xx[0]            # grid spacing\n",
    "\n",
    "@ws.interact(b=bounds, B=(1, 100))\n",
    "def plot_pdf_G1(b=0, B=25):\n",
    "    plt.figure(figsize=(6, 2))\n",
    "    plt.plot(xx, pdf_G1(xx, b, B))\n",
    "    plt.xlim(*bounds)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.3:** Play around with `b` and `B` (and re-run the above code) and answer these questions by looking at the resulting figure:\n",
    " * How does the pdf curve change when `b` changes?\n",
    " * How does the pdf curve change when you increase `B`?\n",
    " * In a few words, describe the shape of the Gaussian pdf curve. Does this ring a bell for you? *Hint: it should be clear as a bell!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.4*:** Recall the definition of the expectation (with respect to $p(x)$), namely\n",
    "$$\\Expect [f(x)] \\mathrel{≔} \\int  f(x) \\, p(x) \\, d x \\,,$$\n",
    "where the integral is over the whole domain of $x$.  \n",
    "Recall $p(x) = \\mathcal{N}(x \\mid b, B)$ from eqn (G1).  \n",
    "Use pen, paper, and calculus to show that\n",
    " - (i) the first parameter, $b$, indicates its mean, i.e. that $$b = \\Expect[x] \\,.$$\n",
    "   *Hint: you can rely on the result of (iii)*\n",
    " - (ii) the second parameter, $B>0$, indicates its variance,\n",
    "   i.e. that $$B = \\mathbb{Var}(x) \\mathrel{≔} \\Expect[(x-b)^2] \\,.$$\n",
    "   *Hint: use $x^2 = x x$ to enable integration by parts.*\n",
    " - (iii) $E[1] = 1$.  \n",
    "   *Hint: Neither Bernouilli and Laplace managed this,\n",
    "   until Gauss did it by focusing on $(E[1])^2$.\n",
    "   For more help, watch [3Blue1Brown](https://www.youtube.com/watch?v=cy8r7WSuT1I&t=3m52s).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Gauss integrals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.5:** Recall $p(x) = \\mathcal{N}(x \\mid b, B)$ from eqn (G1).  \n",
    "Use pen, paper, and calculus to answer the following questions,  \n",
    "which derive some helpful mnemonics about the distribution.\n",
    "\n",
    " * (i) Find $x$ such that $p(x) = 0$.\n",
    " * (ii) Where is the location of the mode (maximum) of the distribution?  \n",
    "    I.e. find $x$ such that $\\frac{d p}{d x}(x) = 0$.  \n",
    "    *Hint: it's easier to analyse $\\log p(x)$ rather than $p(x)$ itself.*\n",
    " * (iii) Where is the inflection point? I.e. where $\\frac{d^2 p}{d x^2}(x) = 0$.\n",
    " * (iv) Some forms of \"sensitivity analysis\" (a basic form of uncertainty quantification) consist in evaluating $\\frac{d^2 p}{d x^2}(x)$ at the mode.\n",
    "Explain this by reference to the Gaussian shape.\n",
    "*Hint: calculate and interpret $\\frac{d^2 p}{d x^2}(b)$*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The multivariate (i.e. vector) case\n",
    "Here's the pdf of the *multivariate* Gaussian:\n",
    "$$\\begin{align}\n",
    "\\NormDist(x \\mid  b, B)\n",
    "&=\n",
    "|2 \\pi B|^{-1/2} \\, \\exp\\Big(-\\frac{1}{2}\\|x-b\\|^2_B\\Big) \\, , \\tag{GM}\n",
    "\\end{align}$$\n",
    "where $|.|$ represents the matrix determinant,  \n",
    "and $\\|.\\|_W$ represents the norm with weighting: $\\|x\\|^2_W = x^T W^{-1} x$.  \n",
    "In this multivariate case, $B$ is called the *covariance* (matrix).\n",
    "\n",
    "The following implements this pdf. Take a moment to digest the code. Don't worry if you don't understand all of the details. Hints:\n",
    " * `@` produces matrix multiplication (`*` in `Matlab`);\n",
    " * `*` produces array multiplication (`.*` in `Matlab`);\n",
    " * `axis=-1` makes `np.sum()` work along the last dimension of an ND-array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import det, inv\n",
    "\n",
    "def weighted_norm22(xx, W):\n",
    "    \"Computes the norm of each vector (on the last axis) of xx, weighted by W.\"\n",
    "    return np.sum( (xx @ inv(W)) * xx, axis=-1)\n",
    "\n",
    "def pdf_GM(xx, b, B):\n",
    "    \"pdf -- Gaussian, Multivariate: N(x | b, B)\"\n",
    "    c = np.sqrt(det(2*np.pi*B))\n",
    "    return 1/c * np.exp(-0.5*weighted_norm22(xx - b, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code plots the pdf as contour (iso-density) curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid-2D\n",
    "XX, YY = np.meshgrid(xx, xx)\n",
    "grid = np.dstack((XX, YY))\n",
    "\n",
    "@ws.interact(corr=(0, 1, .05), var1=(0.1**2, 10**2))\n",
    "def plot_Gaussian_contours(corr=0.7, var1=1):\n",
    "    var2 = 1\n",
    "    cov12 = np.sqrt(var1 * var2) * corr\n",
    "    Cov = 25 * np.array([[var1  , cov12],\n",
    "                        [cov12 , var2]])\n",
    "    # Eval\n",
    "    pp = pdf_GM(grid, b=0, B=Cov)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.contour(XX, YY, pp)\n",
    "    plt.axis('equal');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.7:** How do the contours look? Try to understand why. Cases:\n",
    " * (a) correlation=0.\n",
    " * (b) correlation=0.99.\n",
    " * (c) correlation=0.5. (Note that we've used `plt.axis('equal')`).\n",
    " * (d) correlation=0.5, but with non-equal variances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.8:** Play the [correlation game](http://guessthecorrelation.com/) (doesn't work right in Chrome) until you get a score (shown as gold coins) of 5 or more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.9:**\n",
    "* What's the difference between correlation and covariance?\n",
    "* What's the difference between correlation (or covariance) and dependence?  \n",
    "  *Hint: consider this [image](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient#/media/File:Correlation_examples2.svg)*\n",
    "* Does correlation imply causation?\n",
    "* Can you use correlation to in making predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.30*:** Why are we so fond of the Gaussian assumption?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Why Gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should have a feel for the Gaussian distribution and its parameters, which will help us illustrate:\n",
    "\n",
    "# Bayes' rule\n",
    "In the Bayesian approach, belief/estimates/knowledge/information about the unknown ($x$)\n",
    "is quantified by probability,\n",
    "and \"Bayes' rule\" says how to condition/update/merge/assimilate prior belief\n",
    "with data/observation ($y$).\n",
    "\n",
    "For continuous random variables, $x$ and $y$, it reads:\n",
    "\n",
    "$$\\begin{align}\n",
    "p(x|y) = \\frac{p(x) \\, p(y|x)}{p(y)} \\, , \\tag{2}\n",
    "\\end{align}$$\n",
    "\n",
    "or, in words:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{\"posterior\" (pdf of $x$ given $y$)}\n",
    "\\; = \\;\n",
    "\\frac{\\text{\"prior\" (pdf of $x$)}\n",
    "\\; \\times \\;\n",
    "\\text{\"likelihood\" (pdf of $y$ given $x$)}}\n",
    "{\\text{\"normalization\" (pdf of $y$)}} \\, .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.10:** Derive Bayes' rule from the definition of [conditional pdf's](https://en.wikipedia.org/wiki/Conditional_probability_distribution#Conditional_continuous_distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR derivation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>Exercises marked with an asterisk (*) are optional.</em>\n",
    "\n",
    "**Exc 2.11*:** Laplace called \"statistical inference\" the reasoning of \"inverse probability\" (1774). You may also have heard of \"inverse problems\" in reference to similar problems, but without a statistical framing. In view of this, why do you think we use $x$ for the unknown, and $y$ for the known/given data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('inverse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computers generally work with discrete, numerical representations of mathematical entities.\n",
    "Numerically, pdfs may be represented by their `values` on a grid, such as `xx` from above. Bayes' rule (2) then consists of *grid-point-wise* multiplication, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bayes_rule(prior_values, lklhd_values, dx):\n",
    "    \"Numerical (pointwise) implementation of Bayes' rule.\"\n",
    "    pp = prior_values * lklhd_values   # pointwise multiplication\n",
    "    posterior_values = pp/(sum(pp)*dx) # normalization\n",
    "    return posterior_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows Bayes' rule in action.  \n",
    "*Remember that the only thing it's doing is multiplying the `prior value` and `likelihood value` at each gridpoint.*  \n",
    "Move the sliders with the arrow keys to animate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the prior's parameters\n",
    "b = 0 # mean\n",
    "B = 1 # variance\n",
    "\n",
    "@ws.interact(y=(-10, 10, 1), R=(0.01, 20, 0.2))\n",
    "def animate_Bayes(y=4.0, R=1):\n",
    "    prior_vals = pdf_G1(xx, b, B)\n",
    "    lklhd_vals = pdf_G1(y, xx, R)\n",
    "\n",
    "    postr_vals = Bayes_rule(prior_vals, lklhd_vals, xx[1]-xx[0])\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(xx, prior_vals, label='prior $\\mathcal{N}(x | b, B)$')\n",
    "    plt.plot(xx, lklhd_vals, label='likelihood $\\mathcal{N}(y | x, R)$')\n",
    "    plt.plot(xx, postr_vals, label='posterior - pointwise')\n",
    "\n",
    "    try:\n",
    "        label = 'posterior - parametric\\n $\\mathcal{N}(x|\\hat{x}, P)$'\n",
    "        xhat, P = Bayes_rule_G1(b, B, y, R)\n",
    "        postr_vals2 = pdf_G1(xx, xhat, P)\n",
    "        plt.plot(xx, postr_vals2, '--', label=label)\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    plt.ylim(ymax=0.6)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_example('BR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.12:** This exercise serves to make you acquainted with how Bayes' rule blends information.  \n",
    "Move the sliders to see what happens, and answer the following:\n",
    " * What happens to the posterior when $R \\rightarrow \\infty$ ?\n",
    " * What happens to the posterior when $R \\rightarrow 0$ ?\n",
    " * Move around $y$. What is the posterior's location (mean/mode) when $R = B$ ?\n",
    " * Can you say something universally valid (for any $y$ and $R$) about the height of the posterior pdf?\n",
    " * Does the posterior scale (width) depend on $y$?  \n",
    "   *Optional*: What does this mean [information-wise](https://en.wikipedia.org/wiki/Differential_entropy#Differential_entropies_for_various_distributions)?\n",
    " * Consider the shape (ignoring location & scale) of the posterior. Does it depend on $R$ or $y$?\n",
    " * Can you see a shortcut to computing this posterior rather than having to do the pointwise multiplication?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Posterior behaviour')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.14:** Show that the normalization in `Bayes_rule()` amounts to (approximately) the same as dividing by $p(y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, since $p(y)$ is thusly implicitly known,\n",
    "we often don't bother to write it down, simplifying Bayes' rule (2) to\n",
    "$$\\begin{align}\n",
    "p(x|y) \\propto p(x) \\, p(y|x) \\, .  \\tag{3}\n",
    "\\end{align}$$\n",
    "In fact, do we even need to care about $p(y)$ at all? All we really need to know is how much more likely some value of $x$ (or an interval around it) is compared to any other $x$.\n",
    "The normalisation is only necessary because of the *convention* that all densities integrate to $1$.\n",
    "However, for large models, we usually can only afford to evaluate $p(y|x)$ at a few points (of $x$), so that the integral for $p(y)$ can only be roughly approximated. In such settings, estimation of the normalisation factor becomes an important question too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.15:** The following implements a \"uniform\" (or \"flat\" or \"box\") distribution [pdf](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)#Moments).\n",
    "In the above animations, replace `pdf_G1` with your new `pdf_U1` (both for the prior and likelihood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_U1(x, b, B):\n",
    "    \"Univariate (scalar), Uniform pdf.\"\n",
    "    lower = b - np.sqrt(3*B)\n",
    "    upper = b + np.sqrt(3*B)\n",
    "    height = 1/(upper - lower)\n",
    "    pdfx = height * np.ones_like(x)\n",
    "    pdfx[x<lower] = 0\n",
    "    pdfx[x>upper] = 0\n",
    "    return pdfx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)\n",
    " - Why (in the figure) are the walls of the pdf (ever so slightly) inclined?\n",
    " - What happens when you move the prior and likelihood too far apart? Is the fault of the implementation, the math, or the problem statement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR U1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b): Re-do Exc 2.12, now with `pdf_U1`.  \n",
    "(c): Now test a Gaussian prior with a uniform likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gaussian-Gaussian Bayes\n",
    "\n",
    "The above animation shows Bayes' rule in 1 dimension. Previously, we saw how a Gaussian looks in 2 dimensions. Can you imagine how Bayes' rule looks in 2 dimensions? In higher dimensions, these things get difficult to imagine, let alone visualize.\n",
    "\n",
    "Similarly, the size of the calculations required for Bayes' rule poses a difficulty. Indeed, the following exercise shows that (pointwise) multiplication for all grid points becomes preposterous in high dimensions.\n",
    "\n",
    "**Exc 2.16:**\n",
    " * (a) How many point-multiplications are needed on a grid with $N$ points in $M$ dimensions? (Imagine an $M$-dimensional cube where each side has a grid with $N$ points on it)\n",
    " * (b) Suppose we model 15 physical quantities, on each grid point, on a discretized surface model of Earth. Assume the resolution is $1^\\circ$ for latitude (110km), $1^\\circ$ for longitude. How many variables are there in total? This is the dimensionality ($M$) of the problem.\n",
    " * (c) Suppose each variable is has a pdf represented with a grid using only $N=20$ points. How many multiplications are necessary to calculate Bayes rule (jointly) for all variables on our Earth model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Dimensionality a')\n",
    "# ws.show_answer('Dimensionality b')\n",
    "# ws.show_answer('Dimensionality c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In response to this computational difficulty, we try to be smart and do something more analytical (\"pen-and-paper\"): we only compute the parameters (mean and (co)variance) of the posterior pdf.\n",
    "\n",
    "This is doable and quite simple in the Gaussian-Gaussian case:  \n",
    "With a prior $p(x) = \\mathcal{N}(x \\mid b,B)$ and  \n",
    "a likelihood $p(y|x) = \\mathcal{N}(y \\mid x,R)$,\n",
    "the posterior is\n",
    "$$\\begin{align}\n",
    "p(x|y)\n",
    "&= \\mathcal{N}(x \\mid \\hat{x},P) \\tag{4} \\, ,\n",
    "\\end{align}$$\n",
    "where, in the univariate (1-dimensional) case:\n",
    "$$\\begin{align}\n",
    "    P &= 1/(1/B + 1/R) \\, , \\tag{5} \\\\\\\n",
    "  \\hat{x} &= P(b/B + y/R) \\, .  \\tag{6}\n",
    "\\end{align}$$\n",
    "\n",
    "The multivariate case is discussed in a later tutorial; for now, try to tackle exc 2.18.\n",
    "\n",
    "#### Exc  2.18 'Gaussian Bayes':\n",
    "Derive the above expressions for $P$ and $\\hat{x}$\n",
    "from Bayes' rule (3) and the expression for a Gaussian pdf (G1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR Gauss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.20:** Algebra exercise: Show that eqn. (5) can be written as\n",
    "$$P = K R \\,,    \\tag{8}$$\n",
    "where\n",
    "$$K = B/(B+R) \\,,    \\tag{9}$$\n",
    "is called the \"Kalman gain\".  \n",
    "Then shown that eqns (5) and (6) can be written as\n",
    "$$\\begin{align}\n",
    "    P &= (1-K)B \\, ,  \\tag{10} \\\\\\\n",
    "  \\hat{x} &= b + K (y-b) \\tag{11} \\, ,\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR Kalman1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.22*:** Consider the formula for $K$ and its role in the previous couple of equations. Why do you think $K$ is called a \"gain\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('KG intuition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.24:** Implement a Gaussian-Gaussian Bayes' rule (eqns 5 and 6, or eqns 9-11) by completing the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bayes_rule_G1(b, B, y, R):\n",
    "    ### INSERT ANSWER HERE ###\n",
    "    return xhat, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR Gauss code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 2.26:** Go back to the above animation code cell. Restore `pdf_G1` (both for prior and likelihood). Run/execute.\n",
    "- What is the relationship between the two posterior curves?  \n",
    "  *Hint: This is the main secret of the \"Kalman filter\".*\n",
    "- Now use `_U1` instead of `_G1` to compute `prior_vals` or `lklhd_vals` or both.  \n",
    "  Does `Bayes_rule_G1()` provide a good approximation to `Bayes_rule()`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: [Univariate (scalar) Kalman filtering](T3%20-%20Univariate%20Kalman%20filtering.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
