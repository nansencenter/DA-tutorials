{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6de1e4a8",
   "metadata": {},
   "source": [
    "# T5 - The Kalman filter (KF) -- multivariate\n",
    "Dealing with vectors and matrices is a lot like plain numbers. But some things get more complicated.\n",
    "$\n",
    "% Loading TeX (MathJax)... Please wait\n",
    "%\n",
    "\\newcommand{\\Reals}{\\mathbb{R}}\n",
    "\\newcommand{\\Expect}[0]{\\mathbb{E}}\n",
    "\\newcommand{\\NormDist}{\\mathcal{N}}\n",
    "%\n",
    "\\newcommand{\\DynMod}[0]{\\mathscr{M}}\n",
    "\\newcommand{\\ObsMod}[0]{\\mathscr{H}}\n",
    "%\n",
    "\\newcommand{\\mat}[1]{{\\mathbf{{#1}}}}\n",
    "%\\newcommand{\\mat}[1]{{\\pmb{\\mathsf{#1}}}}\n",
    "\\newcommand{\\bvec}[1]{{\\mathbf{#1}}}\n",
    "%\n",
    "\\newcommand{\\trsign}{{\\mathsf{T}}}\n",
    "\\newcommand{\\tr}{^{\\trsign}}\n",
    "\\newcommand{\\tn}[1]{#1}\n",
    "\\newcommand{\\ceq}[0]{\\mathrel{â‰”}}\n",
    "\\newcommand{\\xDim}[0]{D_x}\n",
    "%\n",
    "\\newcommand{\\I}[0]{\\mat{I}}\n",
    "\\newcommand{\\K}[0]{\\mat{K}}\n",
    "\\newcommand{\\bP}[0]{\\mat{P}}\n",
    "\\newcommand{\\bH}[0]{\\mat{H}}\n",
    "\\newcommand{\\bF}[0]{\\mat{F}}\n",
    "\\newcommand{\\R}[0]{\\mat{R}}\n",
    "\\newcommand{\\Q}[0]{\\mat{Q}}\n",
    "\\newcommand{\\B}[0]{\\mat{B}}\n",
    "\\newcommand{\\C}[0]{\\mat{C}}\n",
    "\\newcommand{\\Ri}[0]{\\R^{-1}}\n",
    "\\newcommand{\\Bi}[0]{\\B^{-1}}\n",
    "\\newcommand{\\X}[0]{\\mat{X}}\n",
    "\\newcommand{\\A}[0]{\\mat{A}}\n",
    "\\newcommand{\\Y}[0]{\\mat{Y}}\n",
    "\\newcommand{\\E}[0]{\\mat{E}}\n",
    "\\newcommand{\\U}[0]{\\mat{U}}\n",
    "\\newcommand{\\V}[0]{\\mat{V}}\n",
    "%\n",
    "\\newcommand{\\x}[0]{\\bvec{x}}\n",
    "\\newcommand{\\y}[0]{\\bvec{y}}\n",
    "\\newcommand{\\z}[0]{\\bvec{z}}\n",
    "\\newcommand{\\q}[0]{\\bvec{q}}\n",
    "\\newcommand{\\br}[0]{\\bvec{r}}\n",
    "\\newcommand{\\bb}[0]{\\bvec{b}}\n",
    "%\n",
    "\\newcommand{\\bx}[0]{\\bvec{\\bar{x}}}\n",
    "\\newcommand{\\by}[0]{\\bvec{\\bar{y}}}\n",
    "\\newcommand{\\barB}[0]{\\mat{\\bar{B}}}\n",
    "\\newcommand{\\barP}[0]{\\mat{\\bar{P}}}\n",
    "\\newcommand{\\barC}[0]{\\mat{\\bar{C}}}\n",
    "\\newcommand{\\barK}[0]{\\mat{\\bar{K}}}\n",
    "%\n",
    "\\newcommand{\\D}[0]{\\mat{D}}\n",
    "\\newcommand{\\Dobs}[0]{\\mat{D}_{\\text{obs}}}\n",
    "\\newcommand{\\Dmod}[0]{\\mat{D}_{\\text{obs}}}\n",
    "%\n",
    "\\newcommand{\\ones}[0]{\\bvec{1}}\n",
    "\\newcommand{\\AN}[0]{\\big( \\I_N - \\ones \\ones\\tr / N \\big)}\n",
    "%\n",
    "% END OF MACRO DEF\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca4eab",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import resources.workspace as ws\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928ce4c1",
   "metadata": {},
   "source": [
    "## Another time series problem, now multivariate\n",
    "\n",
    "Recall the AR(1) process from the previous tutorial: $x_{k+1} = \\DynMod x_k + q_k$.\n",
    "- It could result from discretizing [exponential decay](https://en.wikipedia.org/wiki/Exponential_decay):\n",
    "  $\\frac{d x}{d t} = - \\beta x \\,,$ for some $\\beta \\geq 0$, and\n",
    "  adding some white noise, $\\frac{d q}{d t}$.\n",
    "- Discretisation\n",
    "  - using explicit-Euler produces $\\DynMod = (1 - \\beta\\, \\Delta t)$,\n",
    "  - using implicit-Euler produces $\\DynMod = 1/(1 + \\beta\\, \\Delta t)$.\n",
    "  - such that $x_{k+1}$ is equals the analytic solution requires $\\DynMod = e^{- \\beta\\, \\Delta t}$.\n",
    "  - *PS: note that the 1-st order Taylor expansion of each scheme is the same.*\n",
    "- Recall that $\\{x_k\\}$ became a (noisy) constant (horizontal) line when $\\DynMod = 1$,\n",
    "  which makes sense since then $\\beta = 0$.\n",
    "- Similarly, a straight (sloping) line would result from\n",
    "  $\\frac{d^2 x}{d t^2} = 0 \\, .$\n",
    "\n",
    "To make matters more interesting we're now going to consider the $\\xDim$-th order model:\n",
    "  $\\displaystyle \\frac{d^{\\xDim} x}{d t^\\xDim} = 0 \\, .$\n",
    "- This can be rewritten as a 1-st order *vector* (i.e. coupled system of) ODE:\n",
    "  $\\frac{d x_i}{d t} = x_{i+1} \\, ,$ and $x_{{\\xDim}+1} = 0$   \n",
    "  where the subscript $i$ is now instead the *index* of the state vector element.\n",
    "- Again we include noise, $\\frac{d q_i}{d t}$,\n",
    "  and damping (exponential decay), $- \\beta x_i$, to each component.\n",
    "- In total, $ \\frac{d x_i}{d t} = x_{i+1} - \\beta x_i + \\frac{d q_i}{d t} \\, .$\n",
    "- Discretizing with time step `dt=1` produces\n",
    "  $ x_{k+1, i} = x_{k, i+1} + 0.9 x_{k, i} + q_{k, i}\\, ,$  \n",
    "  i.e. $\\beta = 0.1$ or $\\beta = -\\log(0.9)$ depending on which scheme was used.\n",
    "\n",
    "Thus, $\\x_{k+1} = \\DynMod \\x_k + \\q_k$, with $\\DynMod$ the matrix specified below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c8a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xDim = 4 # state (x) length, also model order\n",
    "M = 0.9*np.eye(xDim) + np.diag(np.ones(xDim-1), 1)\n",
    "print(\"M\", M, sep=\"=\\n\")\n",
    "\n",
    "K = 100\n",
    "Q = 0.01**2 * np.diag(1+np.arange(xDim))\n",
    "\n",
    "# Initial conditions\n",
    "xa0 = np.zeros(xDim)\n",
    "Pa0 = 0.1**2 * np.diag(np.arange(xDim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee55cb8f",
   "metadata": {},
   "source": [
    "Note that will generate a $\\xDim$-dimensional time series.\n",
    "But we will only observe the 1st (0th in Python) element/component of the state vector.\n",
    "We say that the other components are **hidden**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf7b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = np.zeros((1, xDim))\n",
    "H[0, 0] = 1.0\n",
    "print(f\"{H=!s}\")\n",
    "\n",
    "R = .1**2 * np.identity(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e504e8c1",
   "metadata": {},
   "source": [
    "Simulate synthetic truth (x) and observations (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad83676",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd.seed(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d9cd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "truths, obsrvs = [], []\n",
    "\n",
    "x = xa0 + np.sqrt(Pa0)@rnd.randn(xDim)\n",
    "for k in range(K):\n",
    "    x = M @ x + np.sqrt(Q)@rnd.randn(xDim)\n",
    "    y = H @ x + np.sqrt(R)@rnd.randn(1)\n",
    "    truths.append(x)\n",
    "    obsrvs.append(y)\n",
    "truths = np.array(truths)\n",
    "obsrvs = np.array(obsrvs)\n",
    "\n",
    "for i, x in enumerate(truths.T):\n",
    "    scaling = (i+1)**4  # for illustration purposes\n",
    "    plt.plot(scaling*x, label=fr\"${scaling}\\,x_{i}$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef152945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import inv\n",
    "estimates = []\n",
    "variances = []\n",
    "\n",
    "xa = xa0\n",
    "Pa = Pa0\n",
    "for k in range(K):\n",
    "    # Forecast step\n",
    "    xf = M@xa\n",
    "    Pf = M@Pa@M.T + Q\n",
    "    # Analysis update step\n",
    "    Pa = inv( inv(Pf) + H.T@inv(R)@H )\n",
    "    xa = Pa @ ( inv(Pf)@xf + H.T@inv(R)@obsrvs[k] )\n",
    "    # Assign\n",
    "    estimates.append((xf, xa))\n",
    "    variances.append((Pf, Pa))\n",
    "    \n",
    "estimates = np.array(estimates)\n",
    "variances = np.array(variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09f0d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(10, 6), nrows=xDim, sharex=True)\n",
    "for i, (truth, estim, ax) in enumerate(zip(truths.T, estimates.T, axs)):\n",
    "    std = np.sqrt(variances.T[i, i])\n",
    "    kk = 1 + np.arange(K)\n",
    "    kk2 = kk.repeat(2)\n",
    "    ax.plot(kk, truth, c='k')\n",
    "    ax.plot(kk2, estim.flatten())\n",
    "    ax.fill_between(kk2, *((estim - std).flatten(),\n",
    "                           (estim + std).flatten()), alpha=.2)\n",
    "    ax.set_ylabel(f\"$x_{i}$\")\n",
    "    ax.set_xlim([0, K])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5d7f4b",
   "metadata": {},
   "source": [
    "## The KF forecast step\n",
    "\n",
    "The forecast step remains essentially unchanged from the [univariate case](T3%20-%20Bayesian%20inference.ipynb).\n",
    "The only difference is that $\\DynMod$ is now a matrix, as well as the use of the transpose ${}^T$ in the covariance equation:\n",
    "$\\begin{align}\n",
    "\\bb_k\n",
    "&= \\DynMod_{k-1} \\hat{\\x}_{k-1} \\, , \\tag{1a} \\\\\\\n",
    "\\B_k\n",
    "&= \\DynMod_{k-1} \\bP_{k-1} \\DynMod_{k-1}^T + \\Q_{k-1} \\, . \\tag{1b}\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31aa600",
   "metadata": {},
   "source": [
    "<mark><font size=\"-1\">\n",
    "We have now derived the Kalman filter, also in the multivariate case. We know how to:\n",
    "</font></mark>\n",
    "- Propagate our estimate of $\\x$ to the next time step using eqns (1a) and (1b).\n",
    "- Update our estimate of $\\x$ by assimilating the latest observation $\\y$, using eqns (5) and (6)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa63694c",
   "metadata": {},
   "source": [
    "## The KF analysis step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e358f65",
   "metadata": {},
   "source": [
    "The following exercise derives the analysis step.\n",
    "\n",
    "#### Exc -- The 'precision' form of the KF\n",
    "Similarly to [Exc 2.18](T3%20-%20Bayesian%20inference.ipynb#Exc----GG-Bayes),\n",
    "it may be shown that the prior $p(\\x) = \\NormDist(\\x \\mid \\bb,\\B)$\n",
    "and likelihood $p(\\y|\\x) = \\NormDist(\\y \\mid \\bH \\x,\\R)$,\n",
    "yield the posterior:\n",
    "\\begin{align}\n",
    "p(\\x|\\y)\n",
    "&= \\NormDist(\\x \\mid \\hat{\\x}, \\bP) \\tag{4}\n",
    "\\, ,\n",
    "\\end{align}\n",
    "where the posterior/analysis mean (vector) and covariance (matrix) are given by:\n",
    "\\begin{align}\n",
    "\t\t\t\\bP &= (\\bH\\tr \\Ri \\bH + \\Bi)^{-1} \\, , \\tag{5} \\\\\n",
    "\t\t\t\\hat{\\x} &= \\bP\\left[\\bH\\tr \\Ri \\y + \\Bi \\bb\\right] \\tag{6}Â \\, ,\n",
    "\\end{align}\n",
    "Prove eqns (4-6).  \n",
    "*Hint: like the last time, the main part lies in \"completing the square\" in $\\x$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad586834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('KF precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07755d38",
   "metadata": {},
   "source": [
    "## Implementation & illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ff3ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KF(K, xa, Pa, M, H, Q, R, obsrvs):\n",
    "    \"\"\"Kalman filter. PS: (xa, Pa) should be input with *initial* values.\"\"\"\n",
    "    ############################\n",
    "    # TEMPORARY IMPLEMENTATION #\n",
    "    ############################\n",
    "    estimates = np.zeros((K, 2))\n",
    "    variances = np.zeros((K, 2))\n",
    "    for k in range(K):\n",
    "        # Forecast step\n",
    "        xf = xa\n",
    "        Pf = Pa\n",
    "        # Analysis update step\n",
    "        Pa = R / H**2\n",
    "        xa = obsrvs[k] / H\n",
    "        # Assign\n",
    "        estimates[k] = xf, xa\n",
    "        variances[k] = Pf, Pa\n",
    "    return estimates, variances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a9766",
   "metadata": {},
   "source": [
    "## Woodbury and the Kalman gain\n",
    "The KF formulae, as specified above, can be pretty expensive...\n",
    "\n",
    "#### Exc (optional) -- flops and MBs\n",
    "Suppose $\\x$ is $\\xDim$-dimensional and has a covariance matrix $\\B$.\n",
    " * (a). What's the size of $\\B$?\n",
    " * (b). How many \"flops\" (approximately, i.e. to leading order) are required  \n",
    " to compute the \"precision form\" of the KF update equation, eqn (5) ?\n",
    " * (c). How much memory (bytes) is required to hold its covariance matrix $\\B$ ?\n",
    " * (d). How many megabytes (MB) is this if $\\xDim$ is a million?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882cf716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('nD-covars are big)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4e4c0c",
   "metadata": {},
   "source": [
    "This is one of the principal reasons why basic extended KF is infeasible for DA.  \n",
    "The following derives another, often more practical, form of the KF analysis update."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d30b423",
   "metadata": {},
   "source": [
    "#### Exc -- The \"Woodbury\" matrix inversion identity\n",
    "The following is known as the Sherman-Morrison-Woodbury lemma/identity,\n",
    "$$\\begin{align}\n",
    "    \\bP = \\left( \\B^{-1} + \\V\\tr \\R^{-1} \\U \\right)^{-1}\n",
    "    =\n",
    "    \\B - \\B \\V\\tr \\left( \\R + \\U \\B \\V\\tr \\right)^{-1} \\U \\B \\, ,\n",
    "    \\tag{W}\n",
    "\\end{align}$$\n",
    "which holds for any (suitably shaped matrices)\n",
    "$\\B$, $\\R$, $\\V,\\U$ *such that the above exists*.\n",
    "\n",
    "Prove the identity. Hint: don't derive it, just prove it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1927009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Woodbury')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ad5f28",
   "metadata": {},
   "source": [
    "#### Exc (optional) -- Matrix shape compatibility\n",
    "- Show that $\\B$ and $\\R$ must be square.\n",
    "- Show that $\\U$ and $\\V$ are not necessarily square, but must have the same dimensions.\n",
    "- Show that $\\B$ and $\\R$ are not necessarily of equal size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f68f996",
   "metadata": {},
   "source": [
    "Exc 7 makes it clear that the Woodbury identity may be used to compute $\\bP$ by inverting matrices of the size of $\\R$ rather than the size of $\\B$.\n",
    "Of course, if $\\R$ is bigger than $\\B$, then the identity is useful the other way around."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac7959e",
   "metadata": {},
   "source": [
    "#### Exc (optional) -- Corollary 1\n",
    "Prove that, for any symmetric, positive-definite (SPD) matrices $\\R$ and $\\B$, and any matrix $\\bH$,\n",
    "$$\\begin{align}\n",
    " \t\\left(\\bH\\tr \\R^{-1} \\bH + \\B^{-1}\\right)^{-1}\n",
    "    &=\n",
    "    \\B - \\B \\bH\\tr \\left( \\R + \\bH \\B \\bH\\tr \\right)^{-1} \\bH \\B \\tag{C1}\n",
    "    \\, .\n",
    "\\end{align}$$\n",
    "*Hint: consider the properties of [SPD](https://en.wikipedia.org/wiki/Definiteness_of_a_matrix#Properties) matrices.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccaad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Woodbury C1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dc4629",
   "metadata": {},
   "source": [
    "#### Exc (optional) -- Corollary 2\n",
    "Prove that, for the same matrices as for Corollary C1,\n",
    "$$\\begin{align}\n",
    "\t\\left(\\bH\\tr \\R^{-1} \\bH + \\B^{-1}\\right)^{-1}\\bH\\tr \\R^{-1}\n",
    "    &= \\B \\bH\\tr \\left( \\R + \\bH \\B \\bH\\tr \\right)^{-1}\n",
    "    \\tag{C2}\n",
    "    \\, .\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de7fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Woodbury C2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc6c6de",
   "metadata": {},
   "source": [
    "#### Exc -- The \"Gain\" form of the KF\n",
    "Now, let's go back to the KF, eqns (5) and (6). Since $\\B$ and $\\R$ are covariance matrices, they are symmetric-positive. In addition, we will assume that they are full-rank, making them SPD and invertible.  \n",
    "\n",
    "Define the Kalman gain by:\n",
    " $$\\begin{align}\n",
    "    \\K &= \\B \\bH\\tr \\big(\\bH \\B \\bH\\tr + \\R\\big)^{-1} \\, . \\tag{K1}\n",
    "\\end{align}$$\n",
    " * (a) Apply (C1) to eqn (5) to obtain the Kalman gain form of analysis/posterior covariance matrix:\n",
    "$$\\begin{align}\n",
    "    \\bP &= [\\I_{\\xDim} - \\K \\bH]\\B \\, . \\tag{8}\n",
    "\\end{align}$$\n",
    "\n",
    "* (b) Apply (C2)  to (5) to obtain the identity\n",
    "$$\\begin{align}\n",
    "    \\K &= \\bP \\bH\\tr \\R^{-1}  \\, . \\tag{K2}\n",
    "\\end{align}$$\n",
    "\n",
    "* (c) Show that $\\bP \\Bi = [\\I_{\\xDim} - \\K \\bH]$.\n",
    "* (d) Use (b) and (c) to obtain the Kalman gain form of analysis/posterior covariance\n",
    "$$\\begin{align}\n",
    "     \\hat{\\x} &= \\bb + \\K\\left[\\y - \\bH \\bb\\right] \\, . \\tag{9}\n",
    "\\end{align}$$\n",
    "Together, eqns (8) and (9) define the Kalman gain form of the KF update.\n",
    "Note that the inversion (eqn 7) involved is of the size of $\\R$, while in eqn (5) it is of the size of $\\B$.\n",
    "\n",
    "#### Exc -- KF with gain\n",
    "Implement the Kalman gain form in place of the precision form of the KF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd649c0d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We have derived two forms of the multivariate KF analysis update step: the \"precision matrix\" form, and the \"Kalman gain\" form. The latter is especially practical when the number of observations is smaller than the length of the state vector. Still, the best is yet to come: the ability to handle very large and chaotic systems\n",
    "(which are more fun than stochastically driven signals such as above)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4b1a71",
   "metadata": {},
   "source": [
    "### Next: [T6 - Spatial statistics (\"geostatistics\") & Kriging](T6%20-%20Geostatistics%20%26%20Kriging.ipynb)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
