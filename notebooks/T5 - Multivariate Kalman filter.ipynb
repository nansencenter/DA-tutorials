{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ba6cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote = \"https://raw.githubusercontent.com/nansencenter/DA-tutorials\"\n",
    "!wget -qO- {remote}/master/notebooks/resources/colab_bootstrap.sh | bash -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca4eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources import show_answer, interact, cInterval, import_from_nb, get_jointplotter\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "from scipy.linalg import inv\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102a1dc1",
   "metadata": {},
   "source": [
    "# T5 - The Kalman filter (KF) -- multivariate\n",
    "\n",
    "We have already seen the Kalman filter (KF) in the scalar/univariate/1D case. Now, we will derive it for the multivariate (vector) case.\n",
    "Vectors are denoted using bold typeface, matrices are bold and uppercase.\n",
    "$\n",
    "\\newcommand{\\NormDist}{\\mathscr{N}}\n",
    "\\newcommand{\\DynMod}[0]{\\mathscr{M}}\n",
    "\\newcommand{\\ObsMod}[0]{\\mathscr{H}}\n",
    "\\newcommand{\\mat}[1]{{\\mathbf{{#1}}}}\n",
    "\\newcommand{\\vect}[1]{{\\mathbf{#1}}}\n",
    "\\newcommand{\\trsign}{{\\mathsf{T}}}\n",
    "\\newcommand{\\tr}{^{\\trsign}}\n",
    "\\newcommand{\\xDim}[0]{D}\n",
    "\\newcommand{\\ta}[0]{\\text{a}}\n",
    "\\newcommand{\\tf}[0]{\\text{f}}\n",
    "\\newcommand{\\I}[0]{\\mat{I}}\n",
    "\\newcommand{\\K}[0]{\\mat{K}}\n",
    "\\newcommand{\\bP}[0]{\\mat{P}}\n",
    "\\newcommand{\\bH}[0]{\\mat{H}}\n",
    "\\newcommand{\\R}[0]{\\mat{R}}\n",
    "\\newcommand{\\Q}[0]{\\mat{Q}}\n",
    "\\newcommand{\\B}[0]{\\mat{B}}\n",
    "\\newcommand{\\Ri}[0]{\\R^{-1}}\n",
    "\\newcommand{\\U}[0]{\\mat{U}}\n",
    "\\newcommand{\\V}[0]{\\mat{V}}\n",
    "\\newcommand{\\x}[0]{\\vect{x}}\n",
    "\\newcommand{\\y}[0]{\\vect{y}}\n",
    "\\newcommand{\\q}[0]{\\vect{q}}\n",
    "$\n",
    "\n",
    "## Prelude: Multivariate Bayes\n",
    "\n",
    "In the following, we will see Bayes' rule in the 2D (i.e. multivariate) case. Recall from T3:\n",
    "$$\n",
    "p(\\x|\\y) \\propto p(\\x) \\, p(\\y|\\x) \\,.  \\tag{BR}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfbb97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes_rule, = import_from_nb(\"T3\", [\"Bayes_rule\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45d37f8",
   "metadata": {},
   "source": [
    "The prior is a Gaussian distribution, whose density we also recall:\n",
    "$$\n",
    "\\NormDist(\\x \\mid  \\mathbf{\\mu}, \\mathbf{\\Sigma})\n",
    "= |2 \\pi \\mathbf{\\Sigma}|^{-1/2} \\, \\exp\\Big(-\\frac{1}{2}\\|\\x-\\mathbf{\\mu}\\|^2_\\mathbf{\\Sigma} \\Big) \\,. \\tag{GM}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be21f654",
   "metadata": {},
   "outputs": [],
   "source": [
    "(grid1d, dx, pdf_GM, grid2d, bounds) = import_from_nb(\"T2\", (\"grid1d\", \"dx\", \"pdf_GM\", \"grid2d\", \"bounds\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236927d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "H_kinds = {\n",
    "    \"x\":       lambda x: x,\n",
    "    \"x^2\":     lambda x: x**2,\n",
    "    \"x_1\":     lambda x: x[:1],\n",
    "    \"mean(x)\": lambda x: x[:1] + x[1:],\n",
    "    \"diff(x)\": lambda x: x[:1] - x[1:],\n",
    "    \"prod(x)\": lambda x: x[:1] * x[1:],\n",
    "    # OR np.mean/prod with `keepdims=True`\n",
    "}\n",
    "\n",
    "v = dict(orientation=\"vertical\"),\n",
    "@interact(top=[['corr_Pf', 'corr_R']], bottom=[['y1', 'R1']], right=['H_kind', ['y2', 'R2']],\n",
    "             corr_R =(-.999, .999, .01), y1=bounds,     R1=(0.01, 36, 0.2),\n",
    "             corr_Pf=(-.999, .999, .01), y2=bounds + v, R2=(0.01, 36, 0.2) + v, H_kind=list(H_kinds))\n",
    "def Bayes2(  corr_R =.6,                 y1=1,          R1=4**2,                H_kind=\"x\",\n",
    "             corr_Pf=.6,                 y2=-12,        R2=1):\n",
    "    # Prior\n",
    "    xf = np.zeros(2)\n",
    "    Pf = 25 * np.array([[1, corr_Pf],\n",
    "                       [corr_Pf, 1]])\n",
    "    # Likelihood\n",
    "    H = H_kinds[H_kind]\n",
    "    cov_R = np.sqrt(R1*R2)*corr_R\n",
    "    R = np.array([[R1, cov_R],\n",
    "                  [cov_R, R2]])\n",
    "    y = np.array([y1, y2])\n",
    "\n",
    "    # Restrict dimensionality to that of output of H\n",
    "    if len(H(xf)) == 1:\n",
    "        i = slice(None, 1)\n",
    "    else:\n",
    "        i = slice(None)\n",
    "\n",
    "    # Compute BR\n",
    "    x = grid2d\n",
    "    prior = pdf_GM(x, xf, Pf)\n",
    "    lklhd = pdf_GM(y[i], H(x.T)[i].T, R[i, i])\n",
    "    postr = Bayes_rule(prior, lklhd, dx**2)\n",
    "\n",
    "    ax, jplot = get_jointplotter(grid1d)\n",
    "    contours = [jplot(prior, 'blue'),\n",
    "                jplot(lklhd, 'green'),\n",
    "                jplot(postr, 'red', linewidths=2)]\n",
    "    ax.legend(contours, ['prior', 'lklhd', 'postr'], loc=\"upper left\")\n",
    "    ax.set_title(r\"Using $\\mathscr{H}(x) = \" + H_kind + \"$\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11341cdf",
   "metadata": {},
   "source": [
    "Note that the likelihood is again defined as in eqn. (Lklhd):\n",
    "$$ p(\\y|\\x) = \\NormDist(\\y| \\ObsMod(\\x), \\R) \\,. \\tag{Lklhd} $$\n",
    "\n",
    "Examples of $\\ObsMod(\\x)$ for multivariate $\\x$ (and possibly $\\y$) include:\n",
    "\n",
    "- $\\ObsMod(\\x) = \\x$ if directly observing everything.\n",
    "- $\\ObsMod(\\x) = (x_i,  i \\in \\text{subset})$ if only partially (sparsely?) directly observing.\n",
    "- $\\ObsMod(\\x) = \\sum_k x_{ijk}$ if \"integrating\" over the vertical \"column\" of the atmosphere at $(\\text{lat, long}) = (i,j)$.\n",
    "- $\\ObsMod(\\x) = (x_{i+1} - x_i, \\forall i)$ if interested in the slope/roughness of the state field.\n",
    "- $\\ObsMod(\\x) = g(\\|\\x\\|^2)$ for some $g$ that only depends on some distance or magnitude.\n",
    "\n",
    "#### Exc -- Multivariate observation models\n",
    "\n",
    "- (a) Does the posterior (pdf) generally lie \"between\" the prior and likelihood?\n",
    "- (b) Try the different observation models in the dropdown menu.  \n",
    "  *Note that if $\\ObsMod$ only yield a single (scalar/1D) output, then `y2`, `R2` and `corr_R` become inactive.*\n",
    "  - Explain the impact on the likelihood (and thereby posterior).  \n",
    "  - Consider to what extent it is reasonable to say that $\\ObsMod$ gets \"inverted\".\n",
    "  - For those of the above models that are linear,\n",
    "    find the (possibly rectangular) matrix $\\bH$ such that $\\ObsMod(\\x) = \\bH \\x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44208fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('Multivariate Observations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9adf6a",
   "metadata": {},
   "source": [
    "While conceptually and technically simple, the sheer **amount** of computations done by `Bayes_rule` quickly becomes a difficulty in higher dimensions. This is highlighted in the following exercise.\n",
    "\n",
    "<a name=\"Exc-(optional)----Curse-of-dimensionality\"></a>\n",
    "\n",
    "#### Exc (optional) -- Curse of dimensionality\n",
    "\n",
    "- (a) How many point-multiplications are needed on a grid with $N$ points in $\\xDim$ dimensions? Imagine an $\\xDim$-dimensional cube where each side has a grid with $N$ points on it.\n",
    "  *PS: Of course, if the likelihood contains an actual model $\\ObsMod(x)$ as well, its evaluations (computations) could be significantly more costly than the point-multiplications of Bayes' rule itself.*\n",
    "- (b) Suppose we model 5 physical quantities [for example: velocity (u, v, w), pressure, and humidity fields] at each grid point/node for a discretized atmosphere of Earth. Assume the resolution is $1^\\circ$ for latitude (110km), $1^\\circ$ for longitude, and that we only use $3$ vertical layers. How many variables, $\\xDim$, are there in total? This is the ***dimensionality*** of the unknown.\n",
    "- (c) Suppose each variable is has a pdf represented with a grid using only $N=20$ points. How many multiplications are necessary to calculate Bayes rule (jointly) for all variables on our Earth model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680abc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('nD-space is big', 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928ce4c1",
   "metadata": {},
   "source": [
    "## Another time series problem, now multivariate\n",
    "\n",
    "Recall the AR(1) process from the previous tutorial: $x_{k+1} = \\DynMod x_k + q_k$.\n",
    "\n",
    "- It could result from discretizing [exponential decay](https://en.wikipedia.org/wiki/Exponential_decay):\n",
    "  $\\frac{d x}{d t} = - \\beta x \\,,$ for some $\\beta \\geq 0$, and\n",
    "  adding some white noise, $\\frac{d q}{d t}$.\n",
    "- Discretization alternatives (*should share the same 1-st order Taylor expansion!*):\n",
    "  - using explicit-Euler produces $\\DynMod = (1 - \\beta\\, \\Delta t)$,\n",
    "  - using implicit-Euler produces $\\DynMod = 1/(1 + \\beta\\, \\Delta t)$.  \n",
    "  - setting $x_{k+1}$ equal to the analytic solution produces $\\DynMod = e^{- \\beta\\, \\Delta t}$.\n",
    "- Recall that $\\{x_k\\}$ became a (noisy) constant (horizontal) line when $\\DynMod = 1$,\n",
    "  which makes sense since then $\\beta = 0$.  \n",
    "  Similarly, a straight (sloping) line would result from\n",
    "  $\\frac{d^2 x}{d t^2} = 0 \\,.$\n",
    "\n",
    "The above properties motivate the following $\\xDim$-th order generalisation:\n",
    "$\\displaystyle \\frac{d^{\\xDim} x}{d t^\\xDim} = 0 \\,$,\n",
    "which can be rewritten as a 1st-order *vector* (i.e. coupled system of) ODEs:\n",
    "$\\frac{d x_i}{d t} = x_{i+1}$\n",
    "for dimensions $i = 1, \\ldots, D{-}1$, and $x_{{\\xDim}+1} = 0$.\n",
    "Again we add noise, $\\frac{d q_i}{d t}$, to each component,\n",
    "but also friction/damping/decay, $- \\beta x_i$, so that, all in all,\n",
    "$$ \\frac{d x_i}{d t} = x_{i+1} - \\beta x_i + \\frac{d q_i}{d t} \\, .$$\n",
    "\n",
    "Discretizing with time step $\\Delta t=1$ produces\n",
    "$ x_{k+1, i} = x_{k, i+1} + 0.9 x_{k, i} + q_{k, i}\\,,$\n",
    "i.e. $\\beta = 0.1$ or $\\beta = -\\log(0.9)$ depending on which scheme was used.\n",
    "Thus, $\\x_{k+1} = \\DynMod \\x_k + \\q_k$, where $\\DynMod$ is the matrix specified below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c8a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xDim = 4 # state (x) length, also model order\n",
    "M = 0.9*np.eye(xDim) + np.diag(np.ones(xDim-1), 1)\n",
    "print(\"M =\", M, sep=\"\\n\")\n",
    "\n",
    "nTime = 100\n",
    "Q = 0.01**2 * np.diag(1+np.arange(xDim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee55cb8f",
   "metadata": {},
   "source": [
    "#### Observing system\n",
    "\n",
    "The above will generate a $\\xDim$-dimensional time series.\n",
    "However, we will only observe the 1st (i.e., `0`th in Python) element/component of the state vector.\n",
    "The other components are considered **hidden**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf7b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = np.zeros((1, xDim))\n",
    "H[0, 0] = 1.0\n",
    "print(\"H =\", H)\n",
    "\n",
    "R = 30**2 * np.identity(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e504e8c1",
   "metadata": {},
   "source": [
    "#### Simulation\n",
    "\n",
    "The following simulates a synthetic truth (x) time series and observations (y).\n",
    "In particular, note the use of `@` for matrix/vector algebra, instead of `*` as in the [scalar case of the previous tutorial](T4%20-%20Time%20series%20filtering.ipynb#Example-problem:-AR(1))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad83676",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd.seed(4)\n",
    "\n",
    "# Initial condition\n",
    "xa = np.zeros(xDim)\n",
    "Pa = 0.1**2 * np.diag(np.arange(xDim))\n",
    "x = xa + np.sqrt(Pa) @ rnd.randn(xDim)\n",
    "\n",
    "truths = np.zeros((nTime, xDim))\n",
    "obsrvs = np.zeros((nTime, len(H)))\n",
    "for k in range(nTime):\n",
    "    x = M @ x + np.sqrt(Q) @ rnd.randn(xDim)\n",
    "    y = H @ x + np.sqrt(R) @ rnd.randn(1)\n",
    "    truths[k] = x\n",
    "    obsrvs[k] = y\n",
    "\n",
    "for i, x in enumerate(truths.T):\n",
    "    magnification = (i+1)**4  # for illustration purposes\n",
    "    plt.plot(magnification*x, label=fr\"${magnification}\\,x_{i}$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5d7f4b",
   "metadata": {},
   "source": [
    "## The KF forecast step\n",
    "\n",
    "The forecast step (and its derivation) remains essentially unchanged from the [univariate case](T4%20-%20Time%20series%20filtering.ipynb#The-(univariate)-Kalman-filter-(KF)).\n",
    "The only difference is that $\\DynMod$ is now a *matrix*, and we use the transpose ${}^T$ in the covariance equation:\n",
    "$\\begin{align}\n",
    "\\x^\\tf_k\n",
    "&= \\DynMod_{k-1} \\x^\\ta_{k-1} \\,, \\tag{1a} \\\\\\\n",
    "\\bP^\\tf_k\n",
    "&= \\DynMod_{k-1} \\bP^\\ta_{k-1} \\DynMod_{k-1}^T + \\Q_{k-1} \\,. \\tag{1b}\n",
    "\\end{align}$\n",
    "\n",
    "## The KF analysis step\n",
    "\n",
    "It can be shown that the prior $p(\\x) = \\NormDist(\\x \\mid \\x^\\tf,\\bP^\\tf)$\n",
    "and likelihood $p(\\y|\\x) = \\NormDist(\\y \\mid \\ObsMod \\x,\\R)$\n",
    "yield the posterior:\n",
    "$$\n",
    "p(\\x|\\y)\n",
    "= \\NormDist(\\x \\mid \\x^\\ta, \\bP^\\ta) \\tag{4}\n",
    "\\,,\n",
    "$$\n",
    "where the posterior/analysis mean (vector) and covariance (matrix) are given by:\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\bP^\\ta &= \\big(\\ObsMod\\tr \\Ri \\ObsMod + (\\bP^\\tf)^{-1}\\big)^{-1} \\,, \\tag{5} \\\\\n",
    "  \\x^\\ta &= \\bP^\\ta\\left[\\ObsMod\\tr \\Ri \\y + (\\bP^\\tf)^{-1} \\x^\\tf\\right] \\tag{6} \\,,\n",
    "\\end{align}\n",
    "$$\n",
    "*PS: all of the objects in the analysis equations could also be subscripted by the time index ($k$), but that seems unnecessary (since it is the same one for all of the objects involved).*\n",
    "\n",
    "**Exc (optional) -- The 'precision' form of the KF:** Prove eqns. (4-6).  \n",
    "*Hint: As in the [univariate case](T3%20-%20Bayesian%20inference.ipynb#Exc----BR-LG1), the main part lies in \"completing the square\" in $\\x$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad586834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('KF precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07755d38",
   "metadata": {},
   "source": [
    "<a name=\"Implementation-and-illustration\"></a>\n",
    "\n",
    "## Implementation and illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de578c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "estims = np.zeros((nTime, 2, xDim))\n",
    "covars = np.zeros((nTime, 2, xDim, xDim))\n",
    "for k in range(nTime):\n",
    "    # Forecast step\n",
    "    xf = M@xa\n",
    "    Pf = M@Pa@M.T + Q\n",
    "    # Analysis update step\n",
    "    y = obsrvs[k]\n",
    "    Pa = inv( inv(Pf) + H.T@inv(R)@H )\n",
    "    xa = Pa @ ( inv(Pf)@xf + H.T@inv(R)@y )\n",
    "    # Assign\n",
    "    estims[k] = xf, xa\n",
    "    covars[k] = Pf, Pa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649aaeec",
   "metadata": {},
   "source": [
    "Using `inv` is generally bad practice, since it is not numerically stable.\n",
    "You typically want to use `scipy.linalg.solve` instead, or a more fine-grained matrix decomposition routine.\n",
    "However, that is not possible here, since we have no \"right hand side\" to solve for in the formula for `Pa`.\n",
    "We'll address this point later.\n",
    "\n",
    "<mark><font size=\"-1\">\n",
    "*Caution!: Because of our haphazard use of global variables, re-running the KF (without re-running the truth-generating cell) will take as initial condition the endpoint of the previous run.*\n",
    "</font></mark>\n",
    "\n",
    "Use the following to plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29108e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(10, 6), nrows=xDim, sharex=True)\n",
    "for i, (ax, truth, estim) in enumerate(zip(axs, truths.T, estims.T)):\n",
    "    kk = 1 + np.arange(nTime)\n",
    "    kk2 = kk.repeat(2)\n",
    "    ax.plot(kk, truth, c='k')\n",
    "    ax.plot(kk2, estim.T.flatten())\n",
    "    ax.fill_between(kk2, *cInterval(estim.T, covars[..., i, i]), alpha=.2)\n",
    "    if i == 0 and H[0, 0] == 1 and np.sum(np.abs(H)) == 1:\n",
    "        ax.plot(kk, obsrvs, '.')\n",
    "    ax.set_ylabel(f\"$x_{i}$\")\n",
    "    ax.set_xlim([0, nTime])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89176b90",
   "metadata": {},
   "source": [
    "Note that the other, *unobserved* components also get updated. As you can see from eqn. (5), the KF will update such *hidden* components as long as $\\bP^\\tf$ is not diagonal (i.e., as long as there are correlations between the state components). Let us inspect this correlation matrix. Run the cell below, and note:\n",
    "\n",
    "- It converges in time to a fixed value, as we might expect from [T4](T4%20-%20Time%20series%20filtering.ipynb#Exc----Temporal-convergence).\n",
    "- There are no negative correlations in this case, which is perhaps a bit boring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f762bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(k=(1, nTime))\n",
    "def plot_correlation_matrix(k=1, analysis=True):\n",
    "    Pf, Pa = covars[k-1]\n",
    "    covmat = Pa if analysis else Pf\n",
    "    stds = np.sqrt(np.diag(covmat))\n",
    "    corrmat = covmat / np.outer(stds, stds)\n",
    "    plt.matshow(corrmat, cmap='coolwarm', vmin=-1, vmax=+1)\n",
    "    plt.grid(False)\n",
    "    plt.colorbar(shrink=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a9766",
   "metadata": {},
   "source": [
    "## Woodbury and the Kalman gain\n",
    "\n",
    "The KF formulae, as specified above, can be pretty expensive...\n",
    "\n",
    "#### Exc -- flops and MBs\n",
    "\n",
    "Suppose the length of $\\x$ is $\\xDim$ and denote its covariance matrix by $\\bP$.\n",
    "\n",
    "- (a) What's the size of $\\bP$?\n",
    "- (b) To leading order, how many \"flops\" (elementary additions and multiplications) are required  \n",
    "   to compute the \"precision form\" of the KF update equation, eqn. (5) ?  \n",
    "   *Hint: Assume the computationally demanding part is the [Cholesky decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition#Computation).*\n",
    "- (c) How much memory (bytes) is required to hold its covariance matrix $\\bP$ ?\n",
    "- (d) How many megabytes (MB) is that if $\\xDim$ is a million,\n",
    "   as in our [$1^\\circ$ (110km) resolution Earth atmosphere model](#Exc-(optional)----Curse-of-dimensionality).\n",
    "- (e) How many times more MB or flops are needed if you double the resolution (in all 3 dimensions) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882cf716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('nD-covars are big')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4e4c0c",
   "metadata": {},
   "source": [
    "This is one of the main reasons why the basic extended KF is infeasible for data assimilation (DA). In the following, we derive the \"gain\" form of the KF analysis update, which should help at least a little bit.\n",
    "\n",
    "#### Exc -- The \"Woodbury\" matrix inversion identity\n",
    "\n",
    "The following is known as the Sherman-Morrison-Woodbury lemma/identity,\n",
    "$$\n",
    "  \\bP = \\left( \\B^{-1} + \\V\\tr \\R^{-1} \\U \\right)^{-1} =\n",
    "  \\B - \\B \\V\\tr \\left( \\R + \\U \\B \\V\\tr \\right)^{-1} \\U \\B \\,,\n",
    "  \\tag{W}\n",
    "$$\n",
    "which holds for any (suitably shaped matrices)\n",
    "$\\B$, $\\R$, $\\V,\\U$ *such that the above exists*.\n",
    "\n",
    "Prove the identity. *Hint: don't derive it, just prove it!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1927009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('Woodbury general')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595d85ce",
   "metadata": {},
   "source": [
    "#### Exc (optional) -- Matrix shape compatibility\n",
    "\n",
    "With reference to eqn. (W),\n",
    "\n",
    "- Show that $\\B$ and $\\R$ must be square.\n",
    "- Show that $\\U$ and $\\V$ are not necessarily square, but must have the same dimensions.  \n",
    "  *Hint: in so doing, you will also show that $\\B$ and $\\R$ are not necessarily of equal size.*\n",
    "\n",
    "The above exercise makes it clear that the Woodbury identity may be used to compute $\\bP$ by inverting matrices of the size of $\\R$ rather than the size of $\\B$.\n",
    "Of course, if $\\R$ is bigger than $\\B$, then the identity is useful the other way around.\n",
    "\n",
    "#### Exc (optional) -- Corollary 1\n",
    "\n",
    "Prove that, for any symmetric, positive-definite\n",
    "([SPD](https://en.wikipedia.org/wiki/Definiteness_of_a_matrix#Properties))\n",
    "matrices $\\R$ and $\\B$, and any matrix $\\ObsMod$,\n",
    "$$\n",
    "  \\left(\\ObsMod\\tr \\R^{-1} \\ObsMod + \\B^{-1}\\right)^{-1} =\n",
    "  \\B - \\B \\ObsMod\\tr \\left( \\R + \\ObsMod \\B \\ObsMod\\tr \\right)^{-1} \\ObsMod \\B \\tag{C1} \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccaad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('inv(SPD + SPD)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dc4629",
   "metadata": {},
   "source": [
    "#### Exc (optional) -- Corollary 2\n",
    "\n",
    "Prove that, for the same matrices as for Corollary C1,\n",
    "$$\n",
    "  \\left(\\ObsMod\\tr \\R^{-1} \\ObsMod + \\B^{-1}\\right)^{-1}\\ObsMod\\tr \\R^{-1} =\n",
    "  \\B \\ObsMod\\tr \\left( \\R + \\ObsMod \\B \\ObsMod\\tr \\right)^{-1} \\tag{C2} \\, .\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de7fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('Woodbury C2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc6c6de",
   "metadata": {},
   "source": [
    "#### Exc -- The \"Gain\" form of the KF\n",
    "\n",
    "Now, let's return to the KF, eqns. (5) and (6). Since $\\bP^\\tf$ and $\\R$ are covariance matrices, they are symmetric and positive. In addition, we will assume that they are full-rank, i.e. definite (i.e. SPD) and invertible.\n",
    "\n",
    "Define the Kalman gain as:\n",
    "$$ \\K = \\bP^\\tf \\ObsMod\\tr \\big(\\ObsMod \\bP^\\tf \\ObsMod\\tr + \\R\\big)^{-1} \\,. \\tag{K1} $$\n",
    "\n",
    "- (a) Apply (C1) to eqn. (5) to obtain the Kalman gain form of analysis/posterior covariance matrix:\n",
    "  $$ \\bP^\\ta = [\\I_{\\xDim} - \\K \\ObsMod]\\bP^\\tf \\,. \\tag{8} $$\n",
    "- (b) Apply (C2)  to (5) to obtain the identity\n",
    "  $$ \\K = \\bP^\\ta \\ObsMod\\tr \\R^{-1}  \\,. \\tag{K2} $$\n",
    "- (c) Show that $\\bP^\\ta (\\bP^\\tf)^{-1} = [\\I_{\\xDim} - \\K \\ObsMod]$.\n",
    "- (d) Use (b) and (c) to obtain the Kalman gain form of analysis/posterior covariance\n",
    "  $$ \\x^\\ta = \\x^\\tf + \\K\\left[\\y - \\ObsMod \\x^\\tf\\right] \\, . \\tag{9} $$\n",
    "\n",
    "Together, eqns. (8) and (9) define the Kalman gain form of the KF update.\n",
    "Note that the inversion (eqn. 7) involved is of the size of $\\R$, while in eqn. (5) it is of the size of $\\bP^\\tf$.\n",
    "\n",
    "#### Exc -- KF implemented with gain\n",
    "\n",
    "- Implement the Kalman gain form (eqns. K1, 8, 9) of the KF in place of the precision form (eqns. 5, 6)\n",
    "  that is [implemented above](#Implementation-and-illustration).\n",
    "\n",
    "  - *Hint: $\\I_{\\xDim}$ can be obtained from `np.eye` or `np.identity`.\n",
    "    But you do not actually even need it.*\n",
    "  - *Hint: To avoid scrolling back and forth, copy the text with the Kalman gain formulae and paste them above.*\n",
    "\n",
    "- Re-run all cells, and verify that you get the same result as before\n",
    "  (inspecting the plot of the example problem, or printouts of the same numbers).\n",
    "- Replace the use of `inv` by `scipy.linalg.solve`.\n",
    "  Don't hesitate to google their documentations.\n",
    "\n",
    "## Summary\n",
    "\n",
    "We have derived two forms of the multivariate KF analysis update step: the\n",
    "\"precision matrix\" form and the \"Kalman gain\" form. The latter is especially\n",
    "practical when the number of observations is smaller than the length of the\n",
    "state vector. Still, the best is yet to come: the ability to handle very large and chaotic systems\n",
    "(which are more interesting than stochastically driven signals such as above).\n",
    "But first (optionally), we will try to gain some understanding of covariance-based estimation\n",
    "in the case of spatial fields.\n",
    "\n",
    "### Next: [T6 - Chaos & Lorenz](T6%20-%20Chaos%20%26%20Lorenz%20[optional].ipynb)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,scripts//py:light,scripts//md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
