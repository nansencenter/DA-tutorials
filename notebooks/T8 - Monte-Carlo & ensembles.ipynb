{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3703145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote = \"https://raw.githubusercontent.com/nansencenter/DA-tutorials\"\n",
    "!wget -qO- {remote}/master/notebooks/resources/colab_bootstrap.sh | bash -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea12014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources import show_answer, interact, import_from_nb\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import scipy.stats as ss\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "plt.ion();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad1ac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "(pdf_G1, grid1d) = import_from_nb(\"T2\", (\"pdf_G1\", \"grid1d\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e48e86",
   "metadata": {},
   "source": [
    "# T8 - The ensemble (Monte-Carlo) approach\n",
    "is an approximate method for doing Bayesian inference.\n",
    "Instead of computing the full (gridvalues, or parameters, of the) posterior distributions,\n",
    "we instead try to generate ensembles from them.\n",
    "An ensemble is an *iid* sample. I.e. a set of \"members\" (\"particles\", \"realizations\", or \"sample points\") that have been drawn (\"sampled\") independently from the same distribution. With the EnKF, these assumptions are generally tenuous, but pragmatic.\n",
    "$\n",
    "% ######################################## Loading TeX (MathJax)... Please wait ########################################\n",
    "\\newcommand{\\Reals}{\\mathbb{R}} \\newcommand{\\Expect}[0]{\\mathbb{E}} \\newcommand{\\NormDist}{\\mathcal{N}} \\newcommand{\\DynMod}[0]{\\mathscr{M}} \\newcommand{\\ObsMod}[0]{\\mathscr{H}} \\newcommand{\\mat}[1]{{\\mathbf{{#1}}}} \\newcommand{\\bvec}[1]{{\\mathbf{#1}}} \\newcommand{\\trsign}{{\\mathsf{T}}} \\newcommand{\\tr}{^{\\trsign}} \\newcommand{\\ceq}[0]{\\mathrel{≔}} \\newcommand{\\xDim}[0]{D} \\newcommand{\\supa}[0]{^\\text{a}} \\newcommand{\\supf}[0]{^\\text{f}} \\newcommand{\\I}[0]{\\mat{I}} \\newcommand{\\K}[0]{\\mat{K}} \\newcommand{\\bP}[0]{\\mat{P}} \\newcommand{\\bH}[0]{\\mat{H}} \\newcommand{\\bF}[0]{\\mat{F}} \\newcommand{\\R}[0]{\\mat{R}} \\newcommand{\\Q}[0]{\\mat{Q}} \\newcommand{\\B}[0]{\\mat{B}} \\newcommand{\\C}[0]{\\mat{C}} \\newcommand{\\Ri}[0]{\\R^{-1}} \\newcommand{\\Bi}[0]{\\B^{-1}} \\newcommand{\\X}[0]{\\mat{X}} \\newcommand{\\A}[0]{\\mat{A}} \\newcommand{\\Y}[0]{\\mat{Y}} \\newcommand{\\E}[0]{\\mat{E}} \\newcommand{\\U}[0]{\\mat{U}} \\newcommand{\\V}[0]{\\mat{V}} \\newcommand{\\x}[0]{\\bvec{x}} \\newcommand{\\y}[0]{\\bvec{y}} \\newcommand{\\z}[0]{\\bvec{z}} \\newcommand{\\q}[0]{\\bvec{q}} \\newcommand{\\br}[0]{\\bvec{r}} \\newcommand{\\bb}[0]{\\bvec{b}} \\newcommand{\\bx}[0]{\\bvec{\\bar{x}}} \\newcommand{\\by}[0]{\\bvec{\\bar{y}}} \\newcommand{\\barB}[0]{\\mat{\\bar{B}}} \\newcommand{\\barP}[0]{\\mat{\\bar{P}}} \\newcommand{\\barC}[0]{\\mat{\\bar{C}}} \\newcommand{\\barK}[0]{\\mat{\\bar{K}}} \\newcommand{\\D}[0]{\\mat{D}} \\newcommand{\\Dobs}[0]{\\mat{D}_{\\text{obs}}} \\newcommand{\\Dmod}[0]{\\mat{D}_{\\text{obs}}} \\newcommand{\\ones}[0]{\\bvec{1}} \\newcommand{\\AN}[0]{\\big( \\I_N - \\ones \\ones\\tr / N \\big)}\n",
    "$\n",
    "\n",
    "Ensembles can be used to characterize uncertainty: either by using it to compute (estimate) *statistics* thereof, such as the mean, median, variance, covariance, skewness, confidence intervals, etc (any function of the ensemble can be seen as a \"statistic\"), or by using it to reconstruct the distribution/density from which it is sampled. The latter is illustrated by the plot below. Take a moment to digest its code, and then answer the following exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb570908",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0\n",
    "sigma2 = 25\n",
    "N = 80\n",
    "\n",
    "@interact(              seed=(1, 10), nbins=(2, 60), bw=(0.1, 1))\n",
    "def pdf_reconstructions(seed=5,       nbins=10,      bw=.3):\n",
    "    rnd.seed(seed)\n",
    "    E = mu + np.sqrt(sigma2)*rnd.randn(N)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(grid1d, pdf_G1(grid1d, mu, sigma2), lw=5,                      label=\"True\")\n",
    "    ax.plot(E, np.zeros(N), '|k', ms=100, mew=.4,                          label=\"_raw ens\")\n",
    "    ax.hist(E, nbins, density=1, alpha=.7, color=\"C5\",                     label=\"Histogram\")\n",
    "    ax.plot(grid1d, pdf_G1(grid1d, np.mean(E), np.var(E)), lw=5,           label=\"Parametric\")\n",
    "    ax.plot(grid1d, gaussian_kde(E.ravel(), bw**2).evaluate(grid1d), lw=5, label=\"KDE\")\n",
    "    ax.set_ylim(top=(3*sigma2)**-.5)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972fc3c6",
   "metadata": {},
   "source": [
    "**Exc -- A matter of taste?:**\n",
    "- Which approximation to the true pdf looks better?\n",
    "- Which approximation starts with more information?  \n",
    "  What is the downside of making such assumptions?\n",
    "- What value of `bw` causes the \"KDE\" method to most closely\n",
    "  reproduce/recover the \"Parametric\" method?\n",
    "  What about the \"Histogram\" method?  \n",
    "  *PS: we might say that the KDE method \"bridges\" the other two.*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba33d56",
   "metadata": {},
   "source": [
    "Being able to sample a multivariate Gaussian distribution is a building block of the EnKF.\n",
    "That is the objective of the following exercise.\n",
    "\n",
    "**Exc -- Multivariate Gaussian sampling:**\n",
    "Suppose $\\z$ is a standard Gaussian,\n",
    "i.e. $p(\\z) = \\mathcal{N}(\\z \\mid \\bvec{0},\\I_{\\xDim})$,\n",
    "where $\\I_{\\xDim}$ is the $\\xDim$-dimensional identity matrix.  \n",
    "Let $\\x = \\mat{L}\\z + \\mu$.\n",
    "\n",
    " * (a -- optional). Refer to the exercise on [change of variables](T2%20-%20Gaussian%20distribution.ipynb#Exc-(optional)----Probability-and-Change-of-variables) to show that $p(\\x) = \\mathcal{N}(\\x \\mid \\mu, \\mat{C})$, where $\\mat{C} = \\mat{L}^{}\\mat{L}^T$.\n",
    " * (b). The code below samples $N = 100$ realizations of $\\x$\n",
    "   and collects them in an ${\\xDim}$-by-$N$ \"ensemble matrix\" $\\E$.\n",
    "   But `for` loops are slow in plain Python (and Matlab).\n",
    "   Replace it with something akin to `E = mu + L@Z`.\n",
    "   *Hint: this code snippet fails because it's trying to add a vector to a matrix.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bec352",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.array([1, 100, 5])\n",
    "xDim = len(mu)\n",
    "L = np.diag(1+np.arange(xDim))\n",
    "C = L @ L.T\n",
    "Z = rnd.randn(xDim, N)\n",
    "\n",
    "# Using a loop (\"slow\")\n",
    "E = np.zeros((xDim, N))\n",
    "for n in range(N):\n",
    "    E[:, n] = mu + L@Z[:, n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d254ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('Gaussian sampling', 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f46e7f6",
   "metadata": {},
   "source": [
    "The following prints some numbers that can be used to ascertain if you got it right.\n",
    "Note that the estimates will never be exact:\n",
    "they contain some amount of random error, a.k.a. ***sampling error***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ffb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.printoptions(precision=1):\n",
    "    print(\"Estimated mean =\", np.mean(E, axis=1))\n",
    "    print(\"Estimated cov =\", np.cov(E), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809f7a8a",
   "metadata": {},
   "source": [
    "**Exc -- Moment estimation code:** Above, we used numpy's (`np`) functions to compute the sample-estimated mean and covariance matrix,\n",
    "$\\bx$ and $\\barC$,\n",
    "from the ensemble matrix $\\E$.\n",
    "Now, instead, implement these estimators yourself:\n",
    "$$\\begin{align}\\bx &\\ceq \\frac{1}{N}   \\sum_{n=1}^N \\x_n \\,, \\\\\n",
    "   \\barC &\\ceq \\frac{1}{N-1} \\sum_{n=1}^N (\\x_n - \\bx) (\\x_n - \\bx)^T \\,. \\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7227350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't use numpy's mean, cov, but rather a `for` loop.\n",
    "def estimate_mean_and_cov(E):\n",
    "    xDim, N = E.shape\n",
    "\n",
    "    ### FIX THIS ###\n",
    "    x_bar = np.zeros(xDim)\n",
    "    C_bar = np.zeros((xDim, xDim))\n",
    "\n",
    "    return x_bar, C_bar\n",
    "\n",
    "x_bar, C_bar = estimate_mean_and_cov(E)\n",
    "with np.printoptions(precision=1):\n",
    "    print(\"Mean =\", x_bar)\n",
    "    print(\"Covar =\", C_bar, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b3983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('ensemble moments, loop')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd7960a",
   "metadata": {},
   "source": [
    "**Exc -- An obsession?:** Why do we normalize by $(N-1)$ for the covariance computation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994e9487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('Why (N-1)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471bc6ef",
   "metadata": {},
   "source": [
    "It can be shown that the above estimators are ***consistent and unbiased***.\n",
    "Thus, if we let $N \\rightarrow \\infty$, their sampling error will vanish (\"almost surely\"),\n",
    "and we therefor say that our estimators are *consistent*.\n",
    "Meanwhile, if we repeat the estimation experiment many times (but use a fixed, finite $N$),\n",
    "then the average of sampling errors will also vanish, since our estimators are also *unbiased*.\n",
    "Under relatively mild assumptions, the [absence of bias implies concistency](https://en.wikipedia.org/wiki/Consistent_estimator#Bias_versus_consistency)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279989f1",
   "metadata": {},
   "source": [
    "The following computes a large number ($K$) of $\\barC$ and $1/\\barC$, estimated with a given ensemble size ($N$).\n",
    "Note that the true variance is $C = 1$.\n",
    "The histograms of the estimates is plotted, along with vertical lines displaying the mean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57e0d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10000\n",
    "@interact(N=(2, 30), bottom=True)\n",
    "def var_and_precision_estimates(N=4):\n",
    "    E = rnd.randn(K, N)\n",
    "    estims = np.var(E, ddof=1, axis=-1)\n",
    "    bins = np.linspace(0, 6, 40)\n",
    "    plt.figure()\n",
    "    plt.hist(estims,   bins, alpha=.6, density=1)\n",
    "    plt.hist(1/estims, bins, alpha=.6, density=1)\n",
    "    plt.axvline(np.mean(estims),   color=\"C0\", label=\"C\")\n",
    "    plt.axvline(np.mean(1/estims), color=\"C1\", label=\"1/C\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a66ff5",
   "metadata": {},
   "source": [
    "**Exc -- There's bias, and then there's bias:**\n",
    "- Note that $1/\\barC$ does not appear to be an unbiased estimate of $1/C = 1$.  \n",
    "  Explain this by referring to a well-known property of the expectation, $\\Expect$.  \n",
    "  In view of this, consider the role and utility of \"unbiasedness\" in estimation.\n",
    "- What, roughly, is the dependence of the mean values (vertical lines) on the ensemble size?  \n",
    "  What do they tend to as $N$ goes to $0$?  \n",
    "  What about $+\\infty$ ?\n",
    "- Optional: What are the theoretical distributions of $\\barC$ and $1/\\barC$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb10b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('variance estimate statistics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b387e8",
   "metadata": {},
   "source": [
    "**Exc (optional) -- Error notions:**\n",
    " * (a). What's the difference between error and residual?\n",
    " * (b). What's the difference between error and bias?\n",
    " * (c). Show that `\"mean-square-error\" (RMSE^2) = Bias^2 + Var`.  \n",
    "   *Hint: Let $e = \\hat{\\theta} - \\theta$ be the random \"error\" referred to above.\n",
    "   Express each term using the expectation $\\Expect$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ee124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35f757e",
   "metadata": {},
   "source": [
    "**Exc -- Vectorization:** Like Matlab, Python (numpy) is quicker if you \"vectorize\" loops.\n",
    "This is eminently possible with computations of ensemble moments.  \n",
    "Let $\\X \\ceq\n",
    "\\begin{bmatrix}\n",
    "\t\t\\x_1 -\\bx, & \\ldots & \\x_N -\\bx\n",
    "\t\\end{bmatrix} \\,.$\n",
    " * (a). Show that $\\X = \\E \\AN$, where $\\ones$ is the column vector of length $N$ with all elements equal to $1$.  \n",
    "   *Hint: consider column $n$ of $\\X$.*  \n",
    "   *PS: it can be shown that $\\ones \\ones\\tr / N$ and its complement is a \"projection matrix\".*\n",
    " * (b). Show that $\\barC = \\X \\X^T /(N-1)$.\n",
    " * (c). Code up this, latest, formula for $\\barC$ and insert it in `estimate_mean_and_cov(E)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e2f553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('ensemble moments vectorized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584a9ac3",
   "metadata": {},
   "source": [
    "**Exc -- Moment estimation code, part 2:** The cross-covariance between two random vectors, $\\bx$ and $\\by$, is given by\n",
    "$$\\begin{align}\n",
    "\\barC_{\\x,\\y}\n",
    "&\\ceq \\frac{1}{N-1} \\sum_{n=1}^N\n",
    "(\\x_n - \\bx) (\\y_n - \\by)^T \\\\\\\n",
    "&= \\X \\Y^T /(N-1)\n",
    "\\end{align}$$\n",
    "where $\\Y$ is, similar to $\\X$, the matrix whose columns are $\\y_n - \\by$ for $n=1,\\ldots,N$.  \n",
    "Note that this is simply the covariance formula, but for two different variables.  \n",
    "I.e. if $\\Y = \\X$, then $\\barC_{\\x,\\y} = \\barC_{\\x}$ (which we have denoted $\\barC$ in the above).\n",
    "\n",
    "Implement the cross-covariance estimator in the code-cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9eb4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_cross_cov(Ex, Ey):\n",
    "    Cxy = np.zeros((len(Ex), len(Ey)))  ### INSERT ANSWER ###\n",
    "    return Cxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45524f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('estimate cross')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75527f06",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Parametric assumptions (e.g. assuming Gaussianity) can be useful in approximating distributions.\n",
    "Sample covariance estimates can be expressed and computed in a vectorized form.\n",
    "\n",
    "### Next: [T9 - Writing your own EnKF](T9%20-%20Writing%20your%20own%20EnKF.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "jupytext": {
   "formats": "ipynb,scripts//py"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
