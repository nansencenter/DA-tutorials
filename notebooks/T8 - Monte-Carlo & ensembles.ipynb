{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6070b681",
   "metadata": {},
   "source": [
    "# T8 - The ensemble (Monte-Carlo) approach\n",
    "is an approximate method for doing Bayesian inference. Instead of computing the full (gridvalues, or parameters, of the) posterior distributions, we instead try to generate ensembles from them.\n",
    "\n",
    "An ensemble is an *iid* sample. I.e. a set of \"members\" (\"particles\", \"realizations\", or \"sample points\") that have been drawn (\"sampled\") independently from the same distribution. With the EnKF, these assumptions are generally tenuous, but pragmatic.\n",
    "$\n",
    "% ######################################## Loading TeX (MathJax)... Please wait ########################################\n",
    "\\newcommand{\\Reals}{\\mathbb{R}} \\newcommand{\\Expect}[0]{\\mathbb{E}} \\newcommand{\\NormDist}{\\mathcal{N}} \\newcommand{\\DynMod}[0]{\\mathscr{M}} \\newcommand{\\ObsMod}[0]{\\mathscr{H}} \\newcommand{\\mat}[1]{{\\mathbf{{#1}}}} \\newcommand{\\bvec}[1]{{\\mathbf{#1}}} \\newcommand{\\trsign}{{\\mathsf{T}}} \\newcommand{\\tr}{^{\\trsign}} \\newcommand{\\ceq}[0]{\\mathrel{≔}} \\newcommand{\\xDim}[0]{D} \\newcommand{\\supa}[0]{^\\text{a}} \\newcommand{\\supf}[0]{^\\text{f}} \\newcommand{\\I}[0]{\\mat{I}} \\newcommand{\\K}[0]{\\mat{K}} \\newcommand{\\bP}[0]{\\mat{P}} \\newcommand{\\bH}[0]{\\mat{H}} \\newcommand{\\bF}[0]{\\mat{F}} \\newcommand{\\R}[0]{\\mat{R}} \\newcommand{\\Q}[0]{\\mat{Q}} \\newcommand{\\B}[0]{\\mat{B}} \\newcommand{\\C}[0]{\\mat{C}} \\newcommand{\\Ri}[0]{\\R^{-1}} \\newcommand{\\Bi}[0]{\\B^{-1}} \\newcommand{\\X}[0]{\\mat{X}} \\newcommand{\\A}[0]{\\mat{A}} \\newcommand{\\Y}[0]{\\mat{Y}} \\newcommand{\\E}[0]{\\mat{E}} \\newcommand{\\U}[0]{\\mat{U}} \\newcommand{\\V}[0]{\\mat{V}} \\newcommand{\\x}[0]{\\bvec{x}} \\newcommand{\\y}[0]{\\bvec{y}} \\newcommand{\\z}[0]{\\bvec{z}} \\newcommand{\\q}[0]{\\bvec{q}} \\newcommand{\\br}[0]{\\bvec{r}} \\newcommand{\\bb}[0]{\\bvec{b}} \\newcommand{\\bx}[0]{\\bvec{\\bar{x}}} \\newcommand{\\by}[0]{\\bvec{\\bar{y}}} \\newcommand{\\barB}[0]{\\mat{\\bar{B}}} \\newcommand{\\barP}[0]{\\mat{\\bar{P}}} \\newcommand{\\barC}[0]{\\mat{\\bar{C}}} \\newcommand{\\barK}[0]{\\mat{\\bar{K}}} \\newcommand{\\D}[0]{\\mat{D}} \\newcommand{\\Dobs}[0]{\\mat{D}_{\\text{obs}}} \\newcommand{\\Dmod}[0]{\\mat{D}_{\\text{obs}}} \\newcommand{\\ones}[0]{\\bvec{1}} \\newcommand{\\AN}[0]{\\big( \\I_N - \\ones \\ones\\tr / N \\big)}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3703145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote = \"https://raw.githubusercontent.com/nansencenter/DA-tutorials\"\n",
    "!wget -qO- {remote}/master/notebooks/resources/colab_bootstrap.sh | bash -s\n",
    "import resources.workspace as ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea12014",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import scipy.stats as ss\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a8b39",
   "metadata": {},
   "source": [
    "Ensembles can be used to characterize uncertainty: either by reconstructing (estimating) the distribution from which it is assumed drawn, or by computing various *statistics* such as the mean, median, variance, covariance, skewness, confidence intervals, etc (any function of the ensemble can be seen as a \"statistic\"). This is illustrated by the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c65831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "b   = 0\n",
    "B   = 25\n",
    "B12 = np.sqrt(B)\n",
    "\n",
    "def true_pdf(x):\n",
    "    return ss.norm.pdf(x, b, np.sqrt(B))\n",
    "\n",
    "# Plot true pdf\n",
    "xx = 3*np.linspace(-B12, B12, 201)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xx, true_pdf(xx), label=\"True\");\n",
    "\n",
    "# Sample and plot ensemble\n",
    "xDim = 1   # length of state vector\n",
    "N = 100 # ensemble size\n",
    "E = b + B12*rnd.randn(N, xDim)\n",
    "ax.plot(E, np.zeros(N), '|k', alpha=0.3, ms=100)\n",
    "\n",
    "# Plot histogram\n",
    "nbins = max(10, N//30)\n",
    "heights, bins, _ = ax.hist(E, density=1, bins=nbins, label=\"Histogram estimate\")\n",
    "\n",
    "# Plot parametric estimate\n",
    "x_bar = np.mean(E)\n",
    "B_bar = np.var(E)\n",
    "ax.plot(xx, ss.norm.pdf(xx, x_bar, np.sqrt(B_bar)), label=\"Parametric estimate\")\n",
    "\n",
    "ax.legend();\n",
    "\n",
    "# Uncomment AFTER Exc \"KDE\":\n",
    "# dx = bins[1]-bins[0]\n",
    "# c = 0.5/np.sqrt(2*np.pi*B)\n",
    "# for height, x in zip(heights, bins):\n",
    "#     ax.add_patch(mpl.patches.Rectangle((x, 0), dx, c*height/true_pdf(x+dx/2), alpha=0.3))\n",
    "# Also set\n",
    "#  * N = 10**4\n",
    "#  * nbins = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972fc3c6",
   "metadata": {},
   "source": [
    "The plot demonstrates that the true distribution can be represented by a sample thereof (since we can almost reconstruct the Gaussian distribution by estimating the moments from the sample). However, there are other ways to reconstruct (estimate) a distribution from a sample. For example: a histogram.\n",
    "\n",
    "**Exc -- A matter of taste?:** Which approximation to the true pdf looks better: Histogram or the parametric?\n",
    "Does one approximation actually start with more information? The EnKF takes advantage of this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7220d32a",
   "metadata": {},
   "source": [
    "#### Exc (optional) -- KDE\n",
    "Use the method of `gaussian_kde` from `scipy.stats` to make a \"continuous histogram\" and plot it above.\n",
    "`gaussian_kde`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb61ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer(\"KDE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39245e84",
   "metadata": {},
   "source": [
    "**Exc (optional) -- Rank histograms:** Suppose the histogram bars get normalized (divided) by the value of the pdf at their location.  \n",
    "How do you expect the resulting histogram to look?  \n",
    "Test your answer by uncommenting the block in the above code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba33d56",
   "metadata": {},
   "source": [
    "Being able to sample a Gaussian distribution is a building block of the EnKF.\n",
    "In the previous example, we generated samples from a Gaussian distribution using the `randn` function.\n",
    "However, that was just for a scalar (univariate) case, i.e. with `xDim=1`. We need to be able to sample a multivariate Gaussian distribution. That is the objective of the following exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577dc590",
   "metadata": {},
   "source": [
    "**Exc -- Multivariate Gaussian sampling:**\n",
    "Suppose $\\z$ is a standard Gaussian,\n",
    "i.e. $p(\\z) = \\mathcal{N}(\\z \\mid \\bvec{0},\\I_{\\xDim})$,\n",
    "where $\\I_{\\xDim}$ is the $\\xDim$-dimensional identity matrix.  \n",
    "Let $\\x = \\mat{L}\\z + \\bb$,\n",
    "yielding $p(\\x) = \\mathcal{N}(\\x \\mid \\bb, \\mat{L}^{}\\mat{L}^T)$.\n",
    "\n",
    " * (a). $\\z$ can be sampled using `rnd.randn(xDim, 1)`. How is `randn` defined?\n",
    " * (b). Consider the above definition of $\\x$ and the code below.\n",
    " Complete it so as to generate a random realization of $\\x$.  \n",
    " Hint: matrix-vector multiplication can be done using the symbol `@`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b362346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Gaussian sampling', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ffb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "xDim = 3\n",
    "b = 10*np.ones(xDim)\n",
    "B = np.diag(1+np.arange(xDim))\n",
    "L = np.linalg.cholesky(B) # B12\n",
    "print(\"True mean and cov:\")\n",
    "print(b)\n",
    "print(B)\n",
    "\n",
    "### INSERT ANSWER (b) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61931fe7",
   "metadata": {},
   "source": [
    " * (c). In the code cell below, sample $N = 100$ realizations of $\\x$\n",
    " and collect them in an ${\\xDim}$-by-$N$ \"ensemble matrix\" $\\E$.  \n",
    "   - Try to avoid `for` loops (the main thing to figure out is: how to add a (mean) vector to a matrix).\n",
    "   - Run the cell and inspect the computed mean and covariance to see if they're close to the true values, printed in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48f1aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "N  = 100 # ensemble size\n",
    "\n",
    "E = ### INSERT ANSWER (c) ###\n",
    "\n",
    "# Use the code below to assess whether you got it right\n",
    "x_bar = np.mean(E, axis=1)\n",
    "B_bar = np.cov(E)\n",
    "\n",
    "with np.printoptions(precision=1):\n",
    "    print(\"Estimated mean:\")\n",
    "    print(x_bar)\n",
    "    print(\"Estimated covariance:\")\n",
    "    print(B_bar)\n",
    "plt.matshow(B_bar, cmap=\"Blues\"); plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f340a240",
   "metadata": {},
   "source": [
    "**Exc (optional) -- Sampling error:** How erroneous are the ensemble estimates on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2816182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Average sampling error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809f7a8a",
   "metadata": {},
   "source": [
    "**Exc -- Moment estimation code:** Above, we used numpy's (`np`) functions to compute the sample-estimated mean and covariance matrix,\n",
    "$\\bx$ and $\\barB$,\n",
    "from the ensemble matrix $\\E$.\n",
    "Now, instead, implement these estimators yourself:\n",
    "$$\\begin{align}\\bx &\\ceq \\frac{1}{N}   \\sum_{n=1}^N \\x_n \\, , \\\\\n",
    "   \\barB &\\ceq \\frac{1}{N-1} \\sum_{n=1}^N (\\x_n - \\bx) (\\x_n - \\bx)^T \\, . \\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7227350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't use numpy's mean, cov, but rather a `for` loop.\n",
    "def estimate_mean_and_cov(E):\n",
    "    xDim, N = E.shape\n",
    "\n",
    "    ### INSERT ANSWER ###\n",
    "\n",
    "    return x_bar, B_bar\n",
    "\n",
    "x_bar, B_bar = estimate_mean_and_cov(E)\n",
    "with np.printoptions(precision=1):\n",
    "    print(x_bar)\n",
    "    print(B_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b3983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('ensemble moments')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd7960a",
   "metadata": {},
   "source": [
    "**Exc -- An obsession?:** Why do we normalize by $(N-1)$ for the covariance computation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994e9487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Why (N-1)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279989f1",
   "metadata": {},
   "source": [
    "The following computes many $\\barB$ and $1/\\barB$ estimated with a given ensemble size.\n",
    "Note that the values of the true variance being used is 1, as is its inverse.\n",
    "The histograms of the estimates is plotted, along with vertical lines displaying their mean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57e0d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ws.interact(N=(2, 30))\n",
    "def var_and_precision_estimates(N=4):\n",
    "    E = rnd.randn(10000, N)\n",
    "    estims = np.var(E, ddof=1, axis=-1)\n",
    "    bins = np.linspace(0, 6, 40)\n",
    "    plt.figure()\n",
    "    plt.hist(estims, bins, alpha=.6, );\n",
    "    plt.hist(1/estims, bins, alpha=.6);\n",
    "    plt.axvline(np.mean(estims), color=\"C0\")\n",
    "    plt.axvline(np.mean(1/estims), color=\"C1\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a66ff5",
   "metadata": {},
   "source": [
    "**Exc -- There's bias, and then there's bias:**\n",
    "- Is $1/\\barB$ an unbiased estimate of the true precision (i.e. the reciprocal of the variance, i.e. $1$)?\n",
    "- What, roughly, is the dependence of the mean values (vertical lines) on the ensemble size?\n",
    "  What do they tend to as $N$ goes to $0$? What about $+\\infty$ ?\n",
    "- What are the theoretical distributions of $\\barB$ and $1/\\barB$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb10b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('variance estimate statistics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35f757e",
   "metadata": {},
   "source": [
    "**Exc -- Vectorization:** Like Matlab, Python (numpy) is quicker if you \"vectorize\" loops.\n",
    "This is eminently possible with computations of ensemble moments.  \n",
    "Let $\\X \\ceq\n",
    "\\begin{bmatrix}\n",
    "\t\t\\x_1 -\\bx, & \\ldots & \\x_n -\\bx, & \\ldots & \\x_N -\\bx\n",
    "\t\\end{bmatrix} \\, .$\n",
    " * (a). Show that $\\X = \\E \\AN$, where $\\ones$ is the column vector of length $N$ with all elements equal to $1$.\n",
    " Hint: consider column $n$ of $\\X$.\n",
    " * (b). Show that $\\barB = \\X \\X^T /(N-1)$.\n",
    " * (c). Code up this, latest, formula for $\\barB$ and insert it in `estimate_mean_and_cov(E)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e2f553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('ensemble moments vectorized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584a9ac3",
   "metadata": {},
   "source": [
    "**Exc -- Moment estimation code, part 2:** The cross-covariance between two random vectors, $\\bx$ and $\\by$, is given by\n",
    "$$\\begin{align}\n",
    "\\barC_{\\x,\\y}\n",
    "&\\ceq \\frac{1}{N-1} \\sum_{n=1}^N\n",
    "(\\x_n - \\bx) (\\y_n - \\by)^T \\\\\\\n",
    "&= \\X \\Y^T /(N-1)\n",
    "\\end{align}$$\n",
    "where $\\Y$ is, similar to $\\X$, the matrix whose columns are $\\y_n - \\by$ for $n=1,\\ldots,N$.  \n",
    "Note that this is simply the covariance formula, but for two different variables.  \n",
    "I.e. if $\\Y = \\X$, then $\\barC_{\\x,\\y} = \\barC_{\\x}$ (which we have denoted $\\barB$ in the above).\n",
    "\n",
    "Implement the cross-covariance estimator in the code-cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9eb4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_cross_cov(Ex, Ey):\n",
    "    ### INSERT ANSWER ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45524f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('estimate cross')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa0e2c4",
   "metadata": {},
   "source": [
    "**Exc (optional) -- Error notions:**\n",
    " * (a). What's the difference between error residual?\n",
    " * (b). What's the difference between error and bias?\n",
    " * (c). Show `MSE = RMSE^2 = Bias^2 + Var`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec72d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75527f06",
   "metadata": {},
   "source": [
    "### Next: [T9 - Writing your own EnKF](T9%20-%20Writing%20your%20own%20EnKF.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "jupytext": {
   "formats": "ipynb,scripts//py"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
