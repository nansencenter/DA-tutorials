{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import resources.workspace as ws\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "% START OF MACRO DEF\n",
    "% DO NOT EDIT IN INDIVIDUAL NOTEBOOKS, BUT IN macros.py\n",
    "%\n",
    "\\newcommand{\\Reals}{\\mathbb{R}}\n",
    "\\newcommand{\\Expect}[0]{\\mathbb{E}}\n",
    "\\newcommand{\\NormDist}{\\mathcal{N}}\n",
    "%\n",
    "\\newcommand{\\DynMod}[0]{\\mathscr{M}}\n",
    "\\newcommand{\\ObsMod}[0]{\\mathscr{H}}\n",
    "%\n",
    "\\newcommand{\\mat}[1]{{\\mathbf{{#1}}}}\n",
    "%\\newcommand{\\mat}[1]{{\\pmb{\\mathsf{#1}}}}\n",
    "\\newcommand{\\bvec}[1]{{\\mathbf{#1}}}\n",
    "%\n",
    "\\newcommand{\\trsign}{{\\mathsf{T}}}\n",
    "\\newcommand{\\tr}{^{\\trsign}}\n",
    "\\newcommand{\\tn}[1]{#1}\n",
    "\\newcommand{\\ceq}[0]{\\mathrel{â‰”}}\n",
    "%\n",
    "\\newcommand{\\I}[0]{\\mat{I}}\n",
    "\\newcommand{\\K}[0]{\\mat{K}}\n",
    "\\newcommand{\\bP}[0]{\\mat{P}}\n",
    "\\newcommand{\\bH}[0]{\\mat{H}}\n",
    "\\newcommand{\\bF}[0]{\\mat{F}}\n",
    "\\newcommand{\\R}[0]{\\mat{R}}\n",
    "\\newcommand{\\Q}[0]{\\mat{Q}}\n",
    "\\newcommand{\\B}[0]{\\mat{B}}\n",
    "\\newcommand{\\C}[0]{\\mat{C}}\n",
    "\\newcommand{\\Ri}[0]{\\R^{-1}}\n",
    "\\newcommand{\\Bi}[0]{\\B^{-1}}\n",
    "\\newcommand{\\X}[0]{\\mat{X}}\n",
    "\\newcommand{\\A}[0]{\\mat{A}}\n",
    "\\newcommand{\\Y}[0]{\\mat{Y}}\n",
    "\\newcommand{\\E}[0]{\\mat{E}}\n",
    "\\newcommand{\\U}[0]{\\mat{U}}\n",
    "\\newcommand{\\V}[0]{\\mat{V}}\n",
    "%\n",
    "\\newcommand{\\x}[0]{\\bvec{x}}\n",
    "\\newcommand{\\y}[0]{\\bvec{y}}\n",
    "\\newcommand{\\z}[0]{\\bvec{z}}\n",
    "\\newcommand{\\q}[0]{\\bvec{q}}\n",
    "\\newcommand{\\br}[0]{\\bvec{r}}\n",
    "\\newcommand{\\bb}[0]{\\bvec{b}}\n",
    "%\n",
    "\\newcommand{\\bx}[0]{\\bvec{\\bar{x}}}\n",
    "\\newcommand{\\by}[0]{\\bvec{\\bar{y}}}\n",
    "\\newcommand{\\barB}[0]{\\mat{\\bar{B}}}\n",
    "\\newcommand{\\barP}[0]{\\mat{\\bar{P}}}\n",
    "\\newcommand{\\barC}[0]{\\mat{\\bar{C}}}\n",
    "\\newcommand{\\barK}[0]{\\mat{\\bar{K}}}\n",
    "%\n",
    "\\newcommand{\\D}[0]{\\mat{D}}\n",
    "\\newcommand{\\Dobs}[0]{\\mat{D}_{\\text{obs}}}\n",
    "\\newcommand{\\Dmod}[0]{\\mat{D}_{\\text{obs}}}\n",
    "%\n",
    "\\newcommand{\\ones}[0]{\\bvec{1}}\n",
    "\\newcommand{\\AN}[0]{\\big( \\I_N - \\ones \\ones\\tr / N \\big)}\n",
    "%\n",
    "% END OF MACRO DEF\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we look at the full Kalman filter and how it sequentially/recursively works in time, let us consider simple linear regression for a time-dependent problem.\n",
    "\n",
    "# A straight-line example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the straight line ($x_k$) for time index $k=1, 2, \\ldots, K$, specified by\n",
    "$$\\begin{align}\n",
    "x_k = a k \\, , \\tag{1}\n",
    "\\end{align}$$\n",
    "where the slope ($a$) is unknown.\n",
    "Also suppose we have observations ($y$) of the line, but corrupted by noise ($r$):\n",
    "$$\\begin{align}\n",
    "y_k &= x_k + r_k \\, , \\tag{2}\n",
    "\\end{align}$$\n",
    "where $r_k \\sim \\mathcal{N}(0, R)$ for some $R>0$.\n",
    "The code below sets up an experiment based on eqns. (1) and (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "a = 0.4\n",
    "K = 10\n",
    "R = 1\n",
    "\n",
    "# Naming convention: xx and yy hold time series of x and y.\n",
    "xx = np.zeros(K+1) # truth states\n",
    "yy = np.zeros(K+1) # obs\n",
    "\n",
    "# Simulate synthetic truth (x) and obs(y)\n",
    "for k in 1+np.arange(K):\n",
    "    xx[k] = a*k\n",
    "    yy[k] = xx[k] + np.sqrt(R)*rnd.randn()\n",
    "\n",
    "# The obs at k==0 should not be used (since we know xx[0]==0, it is worthless).\n",
    "yy[0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ws.interact(k=ws.IntSlider(min=1, max=K))\n",
    "def plot_experiment(k):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    kk = np.arange(k+1)\n",
    "    plt.plot(kk, xx[kk], 'k' , label='true state ($x$)')\n",
    "    plt.plot(kk, yy[kk], 'k*', label='noisy obs ($y$)')\n",
    "\n",
    "    ### Uncomment this block AFTER doing the Exc 3.4 ###\n",
    "    # plt.plot(kk, kk*lin_reg(k), 'r', label='Linear regress.')\n",
    "\n",
    "    ### Uncomment this block AFTER doing the Exc 3.8 ###\n",
    "    # pw_bb, pw_xxhat = ws.weave_fa(bb, xxhat)\n",
    "    # pw_kf, pw_ka    = ws.weave_fa(np.arange(K+1))\n",
    "    # plt.plot(pw_kf[:3*k], pw_bb[:3*k]   , 'c'  , label='KF forecasts')\n",
    "    # plt.plot(pw_ka[:3*k], pw_xxhat[:3*k], 'b'  , label='KF analyses')\n",
    "    # #plt.plot(kk, kk*xxhat[k]/k         , 'g--', label='KF extrapolated')\n",
    "\n",
    "    plt.xlim([0, 1.01*K])\n",
    "    plt.ylim([-1, 1.2*a*K])\n",
    "    plt.xlabel('time index (k)')\n",
    "    plt.ylabel('$x$, $y$, and $\\hat{x}$')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation by linear regression\n",
    "The observations eqn. (2)\n",
    "yields the likelihood\n",
    "$$\\begin{align}\n",
    "p(y_k|x_k) = \\mathcal{N}(y_k \\mid x_k, R) \\, . \\tag{3}\n",
    "\\end{align}$$\n",
    "Hopefully this is intuitive; otherwise, a derivation is provided in T4.\n",
    "\n",
    "(Least-squares) linear regression minimizes the cost/objective function\n",
    "$$\\begin{align}\n",
    "J_K(a) = \\sum_{k=1}^K (y_k - a k)^2 \\, ,  \\tag{4}\n",
    "\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.2:** Use eqns. (1) and (2) and the logarithm to derive $J_K(a)$ from the likelihood $p\\, (y_1, \\ldots, y_K \\;|\\; a)$.  \n",
    "Explain (prove) that their optimum points will be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('LinReg deriv a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.3:** Show that the optimisation yields the estimator\n",
    "$$\\begin{align}\n",
    "\\hat{a} = \\frac{\\sum_{k=1}^K {k} y_{k}}{\\sum_{k=1}^K {k}^2} \\, . \\tag{6}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('LinReg deriv b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.4:** Code up the linear regression estimator (6).  \n",
    "Then, go back to the animation above and uncomment the block that plots the its estimates.\n",
    "If you did it right, then the estimated line should look reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg(k):\n",
    "    \"Liner regression estimator based on observations y_1, ..., y_k.\"\n",
    "    # PS: the observations (yy) are not among the input args\n",
    "    #     because you can just grab them from the global namespace.\n",
    "    ### INSERT ANSWER HERE ###\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('LinReg_k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we tackle the same problem, but using the Kalman filter.\n",
    "\n",
    "# Estimation by the (univariate) Kalman filter (KF)\n",
    "The KF assumes that the (\"true/nature\") state, $x_k$, evolves recursively in time (indexed by $k$) according to\n",
    "$$\\begin{align}\n",
    "x_{k} = \\DynMod_{k-1} x_{k-1} + q_{k-1} \\, , \\tag{Dyn}\n",
    "\\end{align}$$\n",
    "where $\\DynMod_{k-1}$ is called the \"dynamical\" model, and $q_k$ is a random noise (process) that accounts for \"model errors\".\n",
    "For now, $\\DynMod_{k-1}$ is just a given number (function of $k$). In later tutorials we will  generalize it to matrices, and eventually nonlinear operators (functions).\n",
    "\n",
    "####  The forecast step\n",
    "Suppose that $\\quad\\quad\\;\\;\\quad x_{k-1} \\sim \\mathcal{N}(\\hat{x}_{k-1}, P_{k-1})$,  \n",
    "and that (independently) $q_{k-1} \\sim \\mathcal{N}(0, Q_{k-1})$.\n",
    "\n",
    "By eqn. (Dyn), the mean of $x_{k}$, i.e. $b_k = \\mathbb{E}[x_k]$, is a linear function of the mean of $x_{k-1}$:\n",
    "$$\\begin{align}\n",
    "b_k &= \\DynMod_{k-1} \\hat{x}_{k-1} \\,, \\tag{9}\n",
    "\\end{align}$$\n",
    "since taking the expectation, $\\mathbb{E}$, is a [linear operation](https://en.wikipedia.org/wiki/Expected_value#Properties).  \n",
    "Meanwhile, by the [properties of variance](https://en.wikipedia.org/wiki/Variance#Propagation),\n",
    "the model gets squared in the variance of $x_{k}$,\n",
    "and the error variance is an added term\n",
    "$$\\begin{align}\n",
    "B_k &= \\DynMod_{k-1}^2 P_{k-1} + Q_{k-1} \\,. \\tag{10}\n",
    "\\end{align}$$\n",
    "\n",
    "It can also be shown that [the sum of two Gaussian random variables](https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables#Proof_using_convolutions)\n",
    "is also Gaussian.\n",
    "In summary, eqn. (Dyn) yields\n",
    "$$x_k \\sim \\mathcal{N}(b_k, B_k) \\,, \\tag{8}$$\n",
    "for some $b_k$ and $B_k$ which we can compute from the previous mean and variance, i.e. $\\hat{x}_{k-1}$ and $P_{k-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('RV sums')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.6 (a):** For the KF we want to reformulate our *example problem* of estimating the parameter $a$ as the problem of estimating $x_k$.\n",
    "\n",
    "Derive the \"forecast/dynamical model\" $\\DynMod_k$, as well as $q_k$, such that eqn. (Dyn) is equivalent to eqn (1).\n",
    "\n",
    "Then implement it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mod(k):\n",
    "    return ### INSERT ANSWER HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Sequential 2 Recursive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.6 (b):** The KF may seem like \"overkill\" for our simple example problem.\n",
    "But this \"heavy machinery\" can do a lot more, and will pay off later.\n",
    "Based on the above, *why* is it we can say that the KF can do more?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The analysis step\n",
    "\"updates\" the prior (forecast), $\\mathcal{N}(x_k \\mid \\; b_k,\\; B_k)$, given by eqns. (8), (9), (10),  \n",
    "based on the likelihood, $\\quad\\;\\;\\;\\, \\mathcal{N}(y_k \\mid \\, x_k, \\; R)$,  \n",
    "into the posterior (analysis), $\\; \\; \\, \\mathcal{N}(x_k \\mid \\; \\hat{x}_{k}, \\, P_{k})$, given by\n",
    "the update formulae derived as the Gaussian-Gaussian Bayes' rule in [the previous tutorial](T2%20-%20Bayesian%20inference%20%26%20Gaussians.ipynb#Gaussian-Gaussian-Bayes).\n",
    "\n",
    "This completes the KF cycle, which can then restart with the forecast from $k$ to $k+1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 0 # Dynamical model noise strength\n",
    "\n",
    "# Allocation\n",
    "bb    = np.zeros(K+1) # mean estimates -- prior/forecast values\n",
    "xxhat = np.zeros(K+1) # mean estimates -- post./analysis values\n",
    "BB    = np.zeros(K+1) # var  estimates -- prior/forecast values\n",
    "PP    = np.zeros(K+1) # var  estimates -- post./analysis values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.8:** Following the pattern of the code blocks below,\n",
    "implement the KF to estimate $x_k$ for a given $k$ based on the estimate of $k-1$.\n",
    "\n",
    "<mark><font size=\"-1\">\n",
    "<b>NB:</b> for this example, do not use the \"Kalman gain\" form of the analysis update.\n",
    "This problem involves the peculiar, unrealistic situation of infinities\n",
    "(related to \"improper priors\") at `k==1`, yielding platform-dependent behaviour.\n",
    "These peculiarities are of mainly of academic interest.\n",
    "</font></mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KF(k):\n",
    "    \"Cycle k of the Kalman filter\"\n",
    "    # Forecast\n",
    "    if k==1:\n",
    "        BB[k] = np.inf # The \"initial\" prior uncertainty is infinite...\n",
    "        bb[k] = 0      # ... thus the corresponding mean is inconsequential.\n",
    "    else:\n",
    "        BB[k] = ### INSERT ANSWER HERE ###\n",
    "        bb[k] = ### INSERT ANSWER HERE ###\n",
    "    # Analysis\n",
    "    PP[k]    = ### INSERT ANSWER HERE ###\n",
    "    xxhat[k] = ### INSERT ANSWER HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('KF_k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the estimation computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in 1+np.arange(K):\n",
    "    KF(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.10:** Go back to the animation above and uncomment the block that plots the KF estimates.  \n",
    "Visually: what is the relationship between the estimates provided by the KF and by linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('LinReg compare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark><font size=\"-1\">\n",
    "Exercises marked with an asterisk (*) are <em>optional.</em>\n",
    "</font></mark>\n",
    "\n",
    "**Exc 3.12*:** This exercise proves (on paper) the conclusion of the previous exercise.\n",
    "\n",
    "Firstly, note that the KF forecast step (here with $Q=0$) can be inserted in the analysis step, forming a single couple of recursions:\n",
    "$$\\begin{align}\n",
    "\\hat{x}_k &= P_k \\big(y_k/R \\;+\\; \\DynMod_{k-1} \\hat{x}_{k-1} / [\\DynMod_{k-1}^2 P_{k-1}] \\big) \\tag{11} \\, , \\\\\\\n",
    "P_k &= 1/\\big(1/R \\;+\\; 1/[\\DynMod_{k-1}^2 P_{k-1}]\\big) \\tag{12} \\, .\n",
    "\\end{align}$$\n",
    "\n",
    "Use this and Exc 3.6 (a) to show that\n",
    "$$\\begin{align}\n",
    "&\\text{firstly,} &P_K &= R\\frac{K^2}{\\sum_{k=1}^K k^2} \\, , \\tag{13} \\\\\\\n",
    "&\\text{secondly,} &\\hat{x}_K &= K\\frac{\\sum_{k=1}^K k y_k}{\\sum_{k=1}^K k^2} = K \\hat{a}_K \\tag{14} \\, ,\n",
    "\\end{align}$$\n",
    "where $\\hat{a}_K$\n",
    "is given by eqn. (6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('x_KF == x_LinReg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exc 3.14:\n",
    "Set $Q=0$ in eqn (Dyn) so that $x_{k+1} = \\DynMod x_k$ *for some constant $\\DynMod>1$*.\n",
    "\n",
    "What does the sequence of $P_k$ converge to?  \n",
    "*Hint: Start from eqn (12) [eqn (13) is for the straight-line example only] and find its \"fixed point.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Asymptotic P when M>1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exc 3.15:\n",
    "Redo Exc 3.14, but assuming  \n",
    " * (a) $\\DynMod = 1$.\n",
    " * (b) $\\DynMod < 1$.\n",
    "In these cases it is not so fruitful to use the fixed point equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Asymptotic P when M=1')\n",
    "# ws.show_answer('Asymptotic P when M<1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, if $\\DynMod>1$, the KF state's uncertainty variance, $P_k$ does not converge to 0. This is because, even though you keep gaining more information, this gets balanced out by the growth in uncertainty during the forecast. On the other hand, if $\\DynMod \\leq 1$ then the error converges to zero.\n",
    "\n",
    "In general, however, $\\DynMod$, $Q$, $R$  depend on time, $k$ (often to parameterize exogenous/outside factors/forces/conditions), and there is no limit value that the state distribution (and its parameters) converges to.\n",
    "\n",
    "A particular exception is the above straight-line example. As we found above, $\\DynMod_k =\\frac{k+1}{k}$, which depends on time, and yet its limiting value can be found through eqn. (13); moreover, eqn. (13) and [the pyramidal sum](https://en.wikipedia.org/wiki/Square_pyramidal_number) can be used to show that $P_k \\rightarrow 0$, even though $\\forall k, \\; \\DynMod_k > 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.18*:** Set $Q$ to 1 or more in the KF code, and re-compute its estimates. Explain why the KF estimate is now closer to the obs (always at the latest time instance) than the linear regression estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exc 3.20*:** Now change $R$ (but don't re-run the simulation of the truth and obs). The KF estimates should not change (in this particular example). Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "The KF consists of two steps:\n",
    " * Forecast\n",
    " * Analysis\n",
    " \n",
    "In each step, the mean and variance must be updated.\n",
    "\n",
    "As an example, we saw that the linear regression estimate is reproduced by the KF, although it is a bit tricky to initialize the KF with infinite uncertainty. However, the KF (i.e. state estimation) is much more general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next: [Multivariate Kalman](T4%20-%20Multivariate%20Kalman.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
