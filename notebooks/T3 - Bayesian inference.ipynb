{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbcdbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import resources.workspace as ws\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion();\n",
    "\n",
    "def pdf_G1(x, b, B):\n",
    "    \"Univariate (scalar), Gaussian pdf\"\n",
    "    return sp.stats.norm.pdf(x, loc=b, scale=np.sqrt(B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67741537",
   "metadata": {},
   "source": [
    "$\n",
    "% START OF MACRO DEF\n",
    "% DO NOT EDIT IN INDIVIDUAL NOTEBOOKS, BUT IN macros.py\n",
    "%\n",
    "\\newcommand{\\Reals}{\\mathbb{R}}\n",
    "\\newcommand{\\Expect}[0]{\\mathbb{E}}\n",
    "\\newcommand{\\NormDist}{\\mathcal{N}}\n",
    "%\n",
    "\\newcommand{\\DynMod}[0]{\\mathscr{M}}\n",
    "\\newcommand{\\ObsMod}[0]{\\mathscr{H}}\n",
    "%\n",
    "\\newcommand{\\mat}[1]{{\\mathbf{{#1}}}}\n",
    "%\\newcommand{\\mat}[1]{{\\pmb{\\mathsf{#1}}}}\n",
    "\\newcommand{\\bvec}[1]{{\\mathbf{#1}}}\n",
    "%\n",
    "\\newcommand{\\trsign}{{\\mathsf{T}}}\n",
    "\\newcommand{\\tr}{^{\\trsign}}\n",
    "\\newcommand{\\tn}[1]{#1}\n",
    "\\newcommand{\\ceq}[0]{\\mathrel{â‰”}}\n",
    "%\n",
    "\\newcommand{\\I}[0]{\\mat{I}}\n",
    "\\newcommand{\\K}[0]{\\mat{K}}\n",
    "\\newcommand{\\bP}[0]{\\mat{P}}\n",
    "\\newcommand{\\bH}[0]{\\mat{H}}\n",
    "\\newcommand{\\bF}[0]{\\mat{F}}\n",
    "\\newcommand{\\R}[0]{\\mat{R}}\n",
    "\\newcommand{\\Q}[0]{\\mat{Q}}\n",
    "\\newcommand{\\B}[0]{\\mat{B}}\n",
    "\\newcommand{\\C}[0]{\\mat{C}}\n",
    "\\newcommand{\\Ri}[0]{\\R^{-1}}\n",
    "\\newcommand{\\Bi}[0]{\\B^{-1}}\n",
    "\\newcommand{\\X}[0]{\\mat{X}}\n",
    "\\newcommand{\\A}[0]{\\mat{A}}\n",
    "\\newcommand{\\Y}[0]{\\mat{Y}}\n",
    "\\newcommand{\\E}[0]{\\mat{E}}\n",
    "\\newcommand{\\U}[0]{\\mat{U}}\n",
    "\\newcommand{\\V}[0]{\\mat{V}}\n",
    "%\n",
    "\\newcommand{\\x}[0]{\\bvec{x}}\n",
    "\\newcommand{\\y}[0]{\\bvec{y}}\n",
    "\\newcommand{\\z}[0]{\\bvec{z}}\n",
    "\\newcommand{\\q}[0]{\\bvec{q}}\n",
    "\\newcommand{\\br}[0]{\\bvec{r}}\n",
    "\\newcommand{\\bb}[0]{\\bvec{b}}\n",
    "%\n",
    "\\newcommand{\\bx}[0]{\\bvec{\\bar{x}}}\n",
    "\\newcommand{\\by}[0]{\\bvec{\\bar{y}}}\n",
    "\\newcommand{\\barB}[0]{\\mat{\\bar{B}}}\n",
    "\\newcommand{\\barP}[0]{\\mat{\\bar{P}}}\n",
    "\\newcommand{\\barC}[0]{\\mat{\\bar{C}}}\n",
    "\\newcommand{\\barK}[0]{\\mat{\\bar{K}}}\n",
    "%\n",
    "\\newcommand{\\D}[0]{\\mat{D}}\n",
    "\\newcommand{\\Dobs}[0]{\\mat{D}_{\\text{obs}}}\n",
    "\\newcommand{\\Dmod}[0]{\\mat{D}_{\\text{obs}}}\n",
    "%\n",
    "\\newcommand{\\ones}[0]{\\bvec{1}}\n",
    "\\newcommand{\\AN}[0]{\\big( \\I_N - \\ones \\ones\\tr / N \\big)}\n",
    "%\n",
    "% END OF MACRO DEF\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8add4a2",
   "metadata": {},
   "source": [
    "The previous tutorial studied the Gaussian distribution, with pdf:\n",
    "$$\\begin{align}\n",
    "\\mathcal{N}(x \\mid b, B) = (2 \\pi B)^{-1/2} e^{-(x-b)^2/2 B} \\, . \\tag{G1}\n",
    "\\end{align}$$\n",
    "This will help us illustrate:\n",
    "\n",
    "# Bayes' rule\n",
    "In the Bayesian approach, belief/estimates/knowledge/information about the unknown ($x$)\n",
    "is quantified by probability.\n",
    "And \"Bayes' rule\" says how to condition/merge/assimilate/update this belief based on data/observation ($y$).\n",
    "For continuous random variables, $x$ and $y$, it reads:\n",
    "\n",
    "$$\\begin{align}\n",
    "p(x|y) &= \\frac{p(x) \\, p(y|x)}{p(y)} \\, , \\tag{BR} \\\\\n",
    "\\text{i.e.} \\qquad \\text{\"posterior\" (pdf of $x$ given $y$)}\n",
    "\\; &= \\;\n",
    "\\frac{\\text{\"prior\" (pdf of $x$)}\n",
    "\\; \\times \\;\n",
    "\\text{\"likelihood\" (pdf of $y$ given $x$)}}\n",
    "{\\text{\"normalization\" (pdf of $y$)}} \\, .\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e527c45",
   "metadata": {},
   "source": [
    "**Exc 2.10:** Derive Bayes' rule from the definition of [conditional pdf's](https://en.wikipedia.org/wiki/Conditional_probability_distribution#Conditional_continuous_distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb63318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR derivation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc12b697",
   "metadata": {},
   "source": [
    "<em>Exercises marked with an asterisk (*) are optional.</em>\n",
    "\n",
    "**Exc 2.11*:** Laplace called \"statistical inference\" the reasoning of \"inverse probability\" (1774). You may also have heard of \"inverse problems\" in reference to similar problems, but without a statistical framing. In view of this, why do you think we use $x$ for the unknown, and $y$ for the known/given data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c45b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('inverse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe582dc",
   "metadata": {},
   "source": [
    "Bayes' rule (eqn. BR) simply consists of (point-wise) multiplication.\n",
    "Thus, upon discretisation, Bayes' rule becomes the multiplication of two arrays of values,\n",
    "followed by a normalisation (explained later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a7b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bayes_rule(prior_values, lklhd_values, dx):\n",
    "    mult = prior_values * lklhd_values      # pointwise multiplication\n",
    "    posterior_values = mult/(sum(mult)*dx)  # normalization\n",
    "    return posterior_values\n",
    "\n",
    "grid = np.linspace(-15, 15, 201)\n",
    "dx = grid[1]  - grid[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b6c68d",
   "metadata": {},
   "source": [
    "The code below shows Bayes' rule in action.  \n",
    "*Remember that the only thing it's doing is multiplying the `prior value` and `likelihood value` at each gridpoint.*  \n",
    "Move the sliders with the arrow keys to animate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad62ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ws.interact(y=(-10, 10, 1),\n",
    "             R=(0.01, 20, 0.2))\n",
    "def animate_Bayes(y=4.0, R=1):\n",
    "    # Compute\n",
    "    prior_vals = pdf_G1(grid, 0, 1)\n",
    "    lklhd_vals = pdf_G1(y, grid, R)\n",
    "    postr_vals = Bayes_rule(prior_vals, lklhd_vals, dx)\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(grid, prior_vals, label='prior $\\mathcal{N}(x|b,B)$')\n",
    "    plt.plot(grid, lklhd_vals, label='likelihood $\\mathcal{N}(y|x,R)$')\n",
    "    plt.plot(grid, postr_vals, label='posterior - pointwise')\n",
    "    try:\n",
    "        # See exercise below\n",
    "        label = 'posterior - parametric\\n $\\mathcal{N}(x|\\hat{x},P)$'\n",
    "        xhat, P = Bayes_rule_G1(b, B, y, R)\n",
    "        postr_vals2 = pdf_G1(grid, xhat, P)\n",
    "        plt.plot(grid, postr_vals2, '--', label=label)\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    plt.ylim(0, 0.6)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6519933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_example('BR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36d610c",
   "metadata": {},
   "source": [
    "**Exc 2.12:** This exercise serves to make you acquainted with how Bayes' rule blends information.  \n",
    "Move the sliders to see what happens, and answer the following:\n",
    " * What happens to the posterior when $R \\rightarrow \\infty$ ?\n",
    " * What happens to the posterior when $R \\rightarrow 0$ ?\n",
    " * Move around $y$. What is the posterior's location (mean/mode) when $R = B$ ?\n",
    " * Can you say something universally valid (for any $y$ and $R$) about the height of the posterior pdf?\n",
    " * Does the posterior scale (width) depend on $y$?  \n",
    "   *Optional*: What does this mean [information-wise](https://en.wikipedia.org/wiki/Differential_entropy#Differential_entropies_for_various_distributions)?\n",
    " * Consider the shape (ignoring location & scale) of the posterior. Does it depend on $R$ or $y$?\n",
    " * Can you see a shortcut to computing this posterior rather than having to do the pointwise multiplication?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ab05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Posterior behaviour')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d4b52",
   "metadata": {},
   "source": [
    "**Exc 2.14:** Show that the normalization in `Bayes_rule()` amounts to (approximately) the same as dividing by $p(y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbec0251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d71e15",
   "metadata": {},
   "source": [
    "In fact, since $p(y)$ is thusly implicitly known,\n",
    "we often don't bother to write it down, simplifying Bayes' rule (eqn. BR) to\n",
    "$$\\begin{align}\n",
    "p(x|y) \\propto p(x) \\, p(y|x) \\, .  \\tag{BR2}\n",
    "\\end{align}$$\n",
    "Actually, do we even need to care about $p(y)$ at all? All we really need to know is how much more likely some value of $x$ (or an interval around it) is compared to any other $x$.\n",
    "The normalisation is only necessary because of the *convention* that all densities integrate to $1$.\n",
    "However, for large models, we usually can only afford to evaluate $p(y|x)$ at a few points (of $x$), so that the integral for $p(y)$ can only be roughly approximated. In such settings, estimation of the normalisation factor becomes an important question too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79bb9d3",
   "metadata": {},
   "source": [
    "**Exc 2.15:** The following implements a [uniform](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)#Moments)\n",
    "(or \"flat\" or \"box\") pdf.\n",
    "In the above animations, replace `pdf_G1` with your new `pdf_U1` (both for the prior and likelihood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b222a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_U1(x, b, B):\n",
    "    \"Univariate (scalar), Uniform pdf.\"\n",
    "    lower = b - np.sqrt(3*B)\n",
    "    upper = b + np.sqrt(3*B)\n",
    "    height = 1/(upper - lower)\n",
    "    pdfx = height * np.ones_like(x)\n",
    "    pdfx[x<lower] = 0\n",
    "    pdfx[x>upper] = 0\n",
    "    return pdfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b229e4d",
   "metadata": {},
   "source": [
    "(a)\n",
    " - Why (in the figure) are the walls of the pdf (ever so slightly) inclined?\n",
    " - What happens when you move the prior and likelihood too far apart? Is the fault of the implementation, the math, or the problem statement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9010b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR U1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c1d809",
   "metadata": {},
   "source": [
    "(b): Re-do Exc 2.12, now with `pdf_U1`.  \n",
    "(c): Now test a Gaussian prior with a uniform likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d41651",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gaussian-Gaussian Bayes\n",
    "\n",
    "The above animation shows Bayes' rule in 1 dimension. Previously, we saw how a Gaussian looks in 2 dimensions. Can you imagine how Bayes' rule looks in 2 dimensions? In higher dimensions, these things get difficult to imagine, let alone visualize.\n",
    "\n",
    "Similarly, the size of the calculations required for Bayes' rule poses a difficulty. Indeed, the following exercise shows that (pointwise) multiplication for all grid points becomes preposterous in high dimensions.\n",
    "\n",
    "**Exc 2.16:**\n",
    " * (a) How many point-multiplications are needed on a grid with $N$ points in $M$ dimensions? (Imagine an $M$-dimensional cube where each side has a grid with $N$ points on it)\n",
    " * (b) Suppose we model 15 physical quantities, on each grid point, on a discretized surface model of Earth. Assume the resolution is $1^\\circ$ for latitude (110km), $1^\\circ$ for longitude. How many variables are there in total? This is the dimensionality ($M$) of the problem.\n",
    " * (c) Suppose each variable is has a pdf represented with a grid using only $N=20$ points. How many multiplications are necessary to calculate Bayes rule (jointly) for all variables on our Earth model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bdf66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Dimensionality a')\n",
    "# ws.show_answer('Dimensionality b')\n",
    "# ws.show_answer('Dimensionality c')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b0735f",
   "metadata": {},
   "source": [
    "In response to this computational difficulty, we try to be smart and do something more analytical (\"pen-and-paper\"): we only compute the parameters (mean and (co)variance) of the posterior pdf.\n",
    "\n",
    "This is doable and quite simple in the Gaussian-Gaussian case:  \n",
    "With a prior $p(x) = \\mathcal{N}(x \\mid b,B)$ and  \n",
    "a likelihood $p(y|x) = \\mathcal{N}(y \\mid x,R)$,\n",
    "the posterior is\n",
    "$$\\begin{align}\n",
    "p(x|y)\n",
    "&= \\mathcal{N}(x \\mid \\hat{x},P) \\tag{4} \\, ,\n",
    "\\end{align}$$\n",
    "where, in the univariate (1-dimensional) case:\n",
    "$$\\begin{align}\n",
    "    P &= 1/(1/B + 1/R) \\, , \\tag{5} \\\\\\\n",
    "  \\hat{x} &= P(b/B + y/R) \\, .  \\tag{6}\n",
    "\\end{align}$$\n",
    "\n",
    "The multivariate case is discussed in a later tutorial; for now, try to tackle exc 2.18.\n",
    "\n",
    "#### Exc  2.18 'Gaussian Bayes':\n",
    "Derive the above expressions for $P$ and $\\hat{x}$\n",
    "from Bayes' rule (BR2) and the expression for a Gaussian pdf (G1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f4c49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR Gauss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6717a439",
   "metadata": {},
   "source": [
    "**Exc 2.20:** Algebra exercise: Show that eqn. (5) can be written as\n",
    "$$P = K R \\,,    \\tag{8}$$\n",
    "where\n",
    "$$K = B/(B+R) \\,,    \\tag{9}$$\n",
    "is called the \"Kalman gain\".  \n",
    "Then shown that eqns (5) and (6) can be written as\n",
    "$$\\begin{align}\n",
    "    P &= (1-K)B \\, ,  \\tag{10} \\\\\\\n",
    "  \\hat{x} &= b + K (y-b) \\tag{11} \\, ,\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce20dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR Kalman1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34803dbd",
   "metadata": {},
   "source": [
    "**Exc 2.22*:** Consider the formula for $K$ and its role in the previous couple of equations. Why do you think $K$ is called a \"gain\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('KG intuition')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b894e9ac",
   "metadata": {},
   "source": [
    "**Exc 2.24:** Implement a Gaussian-Gaussian Bayes' rule (eqns 5 and 6, or eqns 9-11) by completing the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af85dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bayes_rule_G1(b, B, y, R):\n",
    "    ### INSERT ANSWER HERE ###\n",
    "    return xhat, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc66c164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR Gauss code')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9903c366",
   "metadata": {},
   "source": [
    "**Exc 2.26:** Go back to the above animation code cell. Restore `pdf_G1` (both for prior and likelihood). Run/execute.\n",
    "- What is the relationship between the two posterior curves?  \n",
    "  *Hint: This is the main secret of the \"Kalman filter\".*\n",
    "- Now use `_U1` instead of `_G1` to compute `prior_vals` or `lklhd_vals` or both.  \n",
    "  Does `Bayes_rule_G1()` provide a good approximation to `Bayes_rule()`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e02e31",
   "metadata": {},
   "source": [
    "### Next: [Univariate (scalar) Kalman filtering](T3%20-%20Univariate%20Kalman%20filtering.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
