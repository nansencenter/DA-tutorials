{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33a6a71d",
   "metadata": {},
   "source": [
    "# T3 - Bayesian inference\n",
    "Now that we have reviewed some probability, we can look at statistical inference.\n",
    "$\n",
    "% START OF MACRO DEF\n",
    "% DO NOT EDIT IN INDIVIDUAL NOTEBOOKS, BUT IN macros.py\n",
    "%\n",
    "\\newcommand{\\Reals}{\\mathbb{R}}\n",
    "\\newcommand{\\Expect}[0]{\\mathbb{E}}\n",
    "\\newcommand{\\NormDist}{\\mathcal{N}}\n",
    "%\n",
    "\\newcommand{\\DynMod}[0]{\\mathscr{M}}\n",
    "\\newcommand{\\ObsMod}[0]{\\mathscr{H}}\n",
    "%\n",
    "\\newcommand{\\mat}[1]{{\\mathbf{{#1}}}}\n",
    "%\\newcommand{\\mat}[1]{{\\pmb{\\mathsf{#1}}}}\n",
    "\\newcommand{\\bvec}[1]{{\\mathbf{#1}}}\n",
    "%\n",
    "\\newcommand{\\trsign}{{\\mathsf{T}}}\n",
    "\\newcommand{\\tr}{^{\\trsign}}\n",
    "\\newcommand{\\tn}[1]{#1}\n",
    "\\newcommand{\\ceq}[0]{\\mathrel{â‰”}}\n",
    "%\n",
    "\\newcommand{\\I}[0]{\\mat{I}}\n",
    "\\newcommand{\\K}[0]{\\mat{K}}\n",
    "\\newcommand{\\bP}[0]{\\mat{P}}\n",
    "\\newcommand{\\bH}[0]{\\mat{H}}\n",
    "\\newcommand{\\bF}[0]{\\mat{F}}\n",
    "\\newcommand{\\R}[0]{\\mat{R}}\n",
    "\\newcommand{\\Q}[0]{\\mat{Q}}\n",
    "\\newcommand{\\B}[0]{\\mat{B}}\n",
    "\\newcommand{\\C}[0]{\\mat{C}}\n",
    "\\newcommand{\\Ri}[0]{\\R^{-1}}\n",
    "\\newcommand{\\Bi}[0]{\\B^{-1}}\n",
    "\\newcommand{\\X}[0]{\\mat{X}}\n",
    "\\newcommand{\\A}[0]{\\mat{A}}\n",
    "\\newcommand{\\Y}[0]{\\mat{Y}}\n",
    "\\newcommand{\\E}[0]{\\mat{E}}\n",
    "\\newcommand{\\U}[0]{\\mat{U}}\n",
    "\\newcommand{\\V}[0]{\\mat{V}}\n",
    "%\n",
    "\\newcommand{\\x}[0]{\\bvec{x}}\n",
    "\\newcommand{\\y}[0]{\\bvec{y}}\n",
    "\\newcommand{\\z}[0]{\\bvec{z}}\n",
    "\\newcommand{\\q}[0]{\\bvec{q}}\n",
    "\\newcommand{\\br}[0]{\\bvec{r}}\n",
    "\\newcommand{\\bb}[0]{\\bvec{b}}\n",
    "%\n",
    "\\newcommand{\\bx}[0]{\\bvec{\\bar{x}}}\n",
    "\\newcommand{\\by}[0]{\\bvec{\\bar{y}}}\n",
    "\\newcommand{\\barB}[0]{\\mat{\\bar{B}}}\n",
    "\\newcommand{\\barP}[0]{\\mat{\\bar{P}}}\n",
    "\\newcommand{\\barC}[0]{\\mat{\\bar{C}}}\n",
    "\\newcommand{\\barK}[0]{\\mat{\\bar{K}}}\n",
    "%\n",
    "\\newcommand{\\D}[0]{\\mat{D}}\n",
    "\\newcommand{\\Dobs}[0]{\\mat{D}_{\\text{obs}}}\n",
    "\\newcommand{\\Dmod}[0]{\\mat{D}_{\\text{obs}}}\n",
    "%\n",
    "\\newcommand{\\ones}[0]{\\bvec{1}}\n",
    "\\newcommand{\\AN}[0]{\\big( \\I_N - \\ones \\ones\\tr / N \\big)}\n",
    "%\n",
    "% END OF MACRO DEF\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b268717",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import resources.workspace as ws\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67741537",
   "metadata": {},
   "source": [
    "The [previous tutorial](T2%20-%20Gaussian%20distribution.ipynb) studied the Gaussian probability density function (pdf), defined by:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathcal{N}(x \\mid b, B) = (2 \\pi B)^{-1/2} e^{-(x-b)^2/2 B} \\, , \\tag{G1}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d1dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_G1(x, b, B):\n",
    "    return sp.stats.norm.pdf(x, loc=b, scale=np.sqrt(B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4594d7c9",
   "metadata": {},
   "source": [
    "The following implements the the [uniform](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous))\n",
    "(or \"flat\" or \"box\") pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7633c2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_U1(x, b, B):\n",
    "    lower = b - np.sqrt(3*B)\n",
    "    upper = b + np.sqrt(3*B)\n",
    "    # pdfx = scipy.stats.uniform(loc=lower, scale=(upper-lower)).pdf(x)\n",
    "    height = 1/(upper - lower)\n",
    "    pdfx = height * np.ones_like(x)\n",
    "    pdfx[x<lower] = 0\n",
    "    pdfx[x>upper] = 0\n",
    "    return pdfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8add4a2",
   "metadata": {},
   "source": [
    "These distributions will help illustrate:\n",
    "\n",
    "# Bayes' rule\n",
    "In the Bayesian approach, knowledge and uncertainty about the unknown ($x$)\n",
    "is quantified through probability.\n",
    "And **Bayes' rule** is how we do inference: it says how to condition/merge/assimilate/update this belief based on data/observation ($y$).\n",
    "For *continuous* \"random variables\", $x$ and $y$, it reads:\n",
    "\n",
    "$$\\begin{align}\n",
    "p(x|y) &= \\frac{p(x) \\, p(y|x)}{p(y)} \\, , \\tag{BR} \\\\[1em]\n",
    "\\text{i.e.} \\qquad \\text{\"posterior\" (pdf of $x$ given $y$)}\n",
    "\\; &= \\;\n",
    "\\frac{\\text{\"prior\" (pdf of $x$)}\n",
    "\\; \\times \\;\n",
    "\\text{\"likelihood\" (pdf of $y$ given $x$)}}\n",
    "{\\text{\"normalization\" (pdf of $y$)}} \\, ,\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95555a5",
   "metadata": {},
   "source": [
    "Note that, in contrast to (the frequent aim of) classical statistics, Bayes' rule in itself makes no attempt at producing only a single estimate (but the topic is briefly discussed [further below](#Exc-2.28-(optional):)). It merely states how quantitative belief (weighted possibilities) should be updated in view of new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e527c45",
   "metadata": {},
   "source": [
    "**Exc 2.10:** Derive Bayes' rule from the definition of [conditional pdf's](https://en.wikipedia.org/wiki/Conditional_probability_distribution#Conditional_continuous_distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb63318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR derivation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc12b697",
   "metadata": {},
   "source": [
    "**Exc 2.11 (optional):** Laplace called \"statistical inference\" the reasoning of \"inverse probability\" (1774). You may also have heard of \"inverse problems\" in reference to similar problems, but without a statistical framing. In view of this, why do you think we use $x$ for the unknown, and $y$ for the known/given data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c45b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('inverse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe582dc",
   "metadata": {},
   "source": [
    "Bayes' rule, eqn. (BR), involves functions (the densities), but applies for any/all values of $x$ (and $y$).\n",
    "Thus, upon discretisation, eqn. (BR) becomes the multiplication of two arrays of values,\n",
    "followed by a normalisation (explained [below](#Exc-2.14:)). It is hard to overstate how simple this principle is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a7b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bayes_rule(prior_values, lklhd_values, dx):\n",
    "    prod = prior_values * lklhd_values         # pointwise multiplication\n",
    "    posterior_values = prod/(np.sum(prod)*dx)  # normalization\n",
    "    return posterior_values\n",
    "\n",
    "bounds = -15, 15\n",
    "grid1d = np.linspace(*bounds, 201)\n",
    "dx = grid1d[1]  - grid1d[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b6c68d",
   "metadata": {},
   "source": [
    "The code below shows Bayes' rule in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad62ea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@ws.interact(y=(*bounds, 1),\n",
    "             R=(0.01, 20, 0.2),\n",
    "             top=['y', 'R'])\n",
    "def Bayes1(y=4.0, R=1.0,\n",
    "           prior_is_G=True,\n",
    "           lklhd_is_G=True):\n",
    "    b = 0\n",
    "    B = 1\n",
    "    x = grid1d\n",
    "\n",
    "    prior_vals = pdf_G1(x, b, B) if prior_is_G else pdf_U1(x, b, B)\n",
    "    lklhd_vals = pdf_G1(y, x, R) if lklhd_is_G else pdf_U1(y, x, R)\n",
    "    postr_vals = Bayes_rule(prior_vals, lklhd_vals, dx)\n",
    "\n",
    "    def plot(x, y, c, lbl):\n",
    "        plt.fill_between(x, y, color=c, alpha=.3, label=lbl)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plot(x, prior_vals, 'blue'  , f'Prior, {b=:.4g}, {B=:.4g}')\n",
    "    plot(x, lklhd_vals, 'green' , f'Lklhd, {y=}, {R=:.4g}')\n",
    "    plot(x, postr_vals, 'red'   , f'Postr, pointwise')\n",
    "\n",
    "    try:\n",
    "        # See exercise below\n",
    "        xhat, P = Bayes_rule_G1(b, B, y, R)\n",
    "        label = f'Postr, parametric\\n{xhat=:.4g},{P=:.4g}'\n",
    "        postr_vals_G1 = pdf_G1(x, xhat, P)\n",
    "        plt.plot(x, postr_vals_G1, 'purple', label=label)\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    plt.ylim(0, 0.6)\n",
    "    plt.legend(loc=\"upper right\", prop={'family': 'monospace'})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36d610c",
   "metadata": {},
   "source": [
    "**Exc 2.12:** This exercise serves to make you acquainted with how Bayes' rule blends information.  \n",
    " Move the sliders (use arrow keys?) to animate it, and answer the following (with the boolean checkmarks both on and off).\n",
    " * What happens to the posterior when $R \\rightarrow \\infty$ ?\n",
    " * What happens to the posterior when $R \\rightarrow 0$ ?\n",
    " * Move $y$ around. What is the posterior's location (mean/mode) when $R = B$ ?\n",
    " * Can you say something universally valid (for any $y$ and $R$) about the height of the posterior pdf?\n",
    " * Does the posterior scale (width) depend on $y$?  \n",
    "   *Optional*: What does this mean [information-wise](https://en.wikipedia.org/wiki/Differential_entropy#Differential_entropies_for_various_distributions)?\n",
    " * Consider the shape (ignoring location & scale) of the posterior. Does it depend on $R$ or $y$?\n",
    " * Can you see a shortcut to computing this posterior rather than having to do the pointwise multiplication?\n",
    " * For the case of two uniform distributions: What happens when you move the prior and likelihood too far apart? Is the fault of the implementation, the math, or the problem statement?\n",
    " * Play around with the grid resolution (see the cell above). What is in your opinion a \"sufficient\" grid resolution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ab05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Posterior behaviour')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d4b52",
   "metadata": {},
   "source": [
    "#### Exc 2.14 (optional):\n",
    "Show that the normalization in `Bayes_rule()` amounts to (approximately) the same as dividing by $p(y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbec0251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d71e15",
   "metadata": {},
   "source": [
    "In fact, since $p(y)$ is thusly implicitly known,\n",
    "we often don't bother to write it down, simplifying Bayes' rule (eqn. BR) to\n",
    "$$\\begin{align}\n",
    "p(x|y) \\propto p(x) \\, p(y|x) \\, .  \\tag{BR2}\n",
    "\\end{align}$$\n",
    "Actually, do we even need to care about $p(y)$ at all? All we really need to know is how much more likely some value of $x$ (or an interval around it) is compared to any other $x$.\n",
    "The normalisation is only necessary because of the *convention* that all densities integrate to $1$.\n",
    "However, for large models, we usually can only afford to evaluate $p(y|x)$ at a few points (of $x$), so that the integral for $p(y)$ can only be roughly approximated. In such settings, estimation of the normalisation factor becomes an important question too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a76780b",
   "metadata": {},
   "source": [
    "#### Exc 2.15 'Nonlinear regression':\n",
    "- (a) Suppose the \"observation model\" consists in squaring, i.e.\n",
    "      $y = x^2/4 + \\varepsilon$, i.e. $p(y|x) = \\NormDist(y|x^2/4, R)$, where $R$ is the variance of $\\varepsilon$. Code this into the animation code.\n",
    "- (b) Try $y = |x|$. Compare with (a).\n",
    "- (c) Try $y = 2 x$. Can you reproduce a posterior obtained with $y = x$ ?\n",
    "\n",
    "Restore $y = x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d41651",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gaussian-Gaussian Bayes\n",
    "\n",
    "The above animation shows Bayes' rule in 1 dimension. Previously, we saw how a Gaussian looks in 2 dimensions. Can you imagine how Bayes' rule looks in 2 dimensions (we'll see in [T4](T4%20-%20Multivariate%20Kalman.ipynb))? In higher dimensions ($M \\gg 1$), these things get difficult to imagine, let alone visualize. Similarly, the size of the problem becomes a computational difficulty.\n",
    "\n",
    "**Exc 2.16 (optional):**\n",
    " * (a) How many point-multiplications are needed on a grid with $N$ points in $M$ dimensions? Imagine an $M$-dimensional cube where each side has a grid with $N$ points on it.\n",
    " * *PS: Of course, if the likelihood contains an actual model $\\mathcal{H}(x)$ as well, its evaluations (computations) could be significantly more costly than the point-multiplications of Bayes' rule itself.*\n",
    " * (b) Suppose we model 15 physical quantities (fields), at each grid node, on a discretized surface model of Earth. Assume the resolution is $1^\\circ$ for latitude (110km), $1^\\circ$ for longitude. How many variables, $M$, are there in total? This is the ***dimensionality*** of the unknown.\n",
    " * (c) Suppose each variable is has a pdf represented with a grid using only $N=20$ points. How many multiplications are necessary to calculate Bayes rule (jointly) for all variables on our Earth model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bdf66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Dimensionality', 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf29f7d",
   "metadata": {},
   "source": [
    "In response to this computational difficulty, we try to be smart and do something more analytical (\"pen-and-paper\"): we only compute the parameters (mean and (co)variance) of the posterior pdf.\n",
    "\n",
    "This is doable and quite simple in the Gaussian-Gaussian case:  \n",
    "- With a prior $p(x) = \\mathcal{N}(x \\mid b,B)$ and  \n",
    "- a likelihood $p(y|x) = \\mathcal{N}(y \\mid x,R)$,  \n",
    "- the posterior is\n",
    "$\n",
    "p(x|y)\n",
    "= \\mathcal{N}(x \\mid \\hat{x},P) \\,,\n",
    "$\n",
    "where, in the 1-dimensional/univariate/scalar (multivariate is discussed in [T4](T4%20-%20Multivariate%20Kalman.ipynb)) case:\n",
    "\n",
    "$$\\begin{align}\n",
    "    P &= 1/(1/B + 1/R) \\, , \\tag{5} \\\\\\\n",
    "  \\hat{x} &= P(b/B + y/R) \\, .  \\tag{6}\n",
    "\\end{align}$$\n",
    "\n",
    "#### Exc  2.18 'Gaussian-Gaussian Bayes':\n",
    "Consider the following identity, where $P$ and $\\hat{x}$ are given by eqns. (5) and (6).\n",
    "$$\\frac{(x-b)^2}{B} + \\frac{(x-y)^2}{R} \\quad=\\quad \\frac{(x - \\hat{x})^2}{P} + \\frac{(y - b)^2}{B + R} \\,, \\tag{S2}$$\n",
    "Notice that the left hand side (LHS) is the sum of two squares with $x$,\n",
    "but the RHS only contains one square with $x$.\n",
    "- (a) Derive the first term of the RHS, i.e. eqns. (5) and (6).\n",
    "- (b) *Optional*: Derive the full RHS (i.e. also the second term).\n",
    "- (c) Derive $p(x|y) = \\mathcal{N}(x \\mid \\hat{x},P)$ from eqns. (5) and (6)\n",
    "  using part (a), Bayes' rule (BR2), and the Gaussian pdf (G1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90eb909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answers.show_answer('BR Gauss, a.k.a. completing the square', 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3527be59",
   "metadata": {},
   "source": [
    "**Exc 2.17:**\n",
    "The statement $x = \\mu \\pm \\sigma$ is *sometimes* used\n",
    "as a shorthand for $p(x) = \\mathcal{N}(x \\mid \\mu, \\sigma^2)$. Suppose\n",
    "- you think the temperature $x = 20Â°C \\pm 2Â°C$,\n",
    "- a thermometer yields the observation $y = 18Â°C \\pm 2Â°C$.\n",
    "\n",
    "Show that your posterior is $p(x|y) = \\mathcal{N}(x \\mid 19, 2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab02ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('GG BR example')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0925f3",
   "metadata": {},
   "source": [
    "The following implements a Gaussian-Gaussian Bayes' rule (eqns 5 and 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd52539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bayes_rule_G1(b, B, y, R):\n",
    "    P = 1/(1/B+1/R)\n",
    "    xhat = P*(b/B+y/R)\n",
    "    return xhat, P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b342a",
   "metadata": {},
   "source": [
    "**Re-run**/execute the interactive animation code cell up above.\n",
    "*Note that the inputs and outputs for `Bayes_rule_G1()` are not discretised density values (as for `Bayes_rule()`), but simply 2 numbers: the mean and the variance.*\n",
    "\n",
    "#### Exc 2.18:\n",
    "- (a) Under what conditions does `Bayes_rule_G1()` provide a good approximation to `Bayes_rule()`?\n",
    "- (b) *Optional*. Try using one or more of the other [distributions readily available in `scipy`](https://stackoverflow.com/questions/37559470/) in the above animation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6717a439",
   "metadata": {},
   "source": [
    "**Exc 2.20:** Algebra exercise: Show that eqn. (5) can be written as\n",
    "$$P = K R \\,,    \\tag{8}$$\n",
    "where\n",
    "$$K = B/(B+R) \\,,    \\tag{9}$$\n",
    "is called the \"Kalman gain\".  \n",
    "Then shown that eqns (5) and (6) can be written as\n",
    "$$\\begin{align}\n",
    "    P &= (1-K)B \\, ,  \\tag{10} \\\\\\\n",
    "  \\hat{x} &= b + K (y-b) \\tag{11} \\, ,\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce20dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR Kalman1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34803dbd",
   "metadata": {},
   "source": [
    "**Exc 2.22 (optional):**\n",
    "- (a) Show that $0 < K < 1$ since $0 < B, R$.\n",
    "- (b) Show that $P < B, R$.\n",
    "- (c) Show that $\\hat{x} \\in (b, y)$.\n",
    "- (d) Why do you think $K$ is called a \"gain\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('KG intuition')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbec053",
   "metadata": {},
   "source": [
    "**Exc 2.24:** Re-define `Bayes_rule_G1` so to as to use eqns. 9-11. Remember to re-run the cell. Verify that you get the same plots as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc66c164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR Kalman1 code')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369cec84",
   "metadata": {},
   "source": [
    "#### Exc 2.28 (optional):\n",
    "*If you must* pick a single point value for your estimate (for example, an action to be taken), you can **decide** on it by optimising (with respect to the estimate) the expected value of some utility/loss function [[ref](https://en.wikipedia.org/wiki/Bayes_estimator)]. For example, if the density of $X$ is symmetric,\n",
    "   and $\\text{Loss}$ is convex and symmetric,\n",
    "   then $\\Expect[\\text{Loss}(X - \\theta)]$ is minimized\n",
    "   by the mean, $\\Expect[X]$, which also coincides with the median.\n",
    "   <!-- See Corollary 7.19 of Lehmann, Casella -->\n",
    "For the expected *squared* loss, $\\Expect[(X - \\theta)^2]$,\n",
    "the minimum is the mean for *any distribution*.\n",
    "Show the latter result.  \n",
    "*Hint: insert $0 = \\,?\\, - \\,?$.*\n",
    "\n",
    "In summary, the intuitive idea of **considering the mean of $p(x)$ as the point estimate** has good theoretical foundations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666b3b6a",
   "metadata": {},
   "source": [
    "## Multivariate illlustration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee2b481",
   "metadata": {},
   "source": [
    "Unlike previous tutorial, which implemented the Gaussian pdf,\n",
    "we here take it from `scipy.stats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26fe828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "def pdf_GM(points, mu, Sigma):\n",
    "    diff = points - mu  # enable broadcasting of *mean*\n",
    "    dims = len(Sigma)\n",
    "    return multivariate_normal(np.zeros(dims), Sigma).pdf(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c03cb0c",
   "metadata": {},
   "source": [
    "Notice that we're re-using the very same `Bayes_rule` as in the 1D case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5b9d46",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid2d = np.dstack(np.meshgrid(grid1d, grid1d))\n",
    "\n",
    "@ws.interact(corr_R=(-0.999, 0.999, .01),\n",
    "             corr_B=(-0.999, 0.999, .01),\n",
    "             y1=bounds,\n",
    "             y2=bounds,\n",
    "             R1=(0.01, 36, 0.2),\n",
    "             R2=(0.01, 36, 0.2),\n",
    "             top=[['corr_B', 'corr_R'], 'y1_only'],\n",
    "             bottom=[['y1', 'R1']],\n",
    "             vertical=['y2', 'R2'],\n",
    "             right=[['y2', 'R2']],\n",
    "             )\n",
    "def Bayes2(corr_B=.6, corr_R=.6, y1=3, y2=-12, R1=4**2, R2=1, y1_only=False):\n",
    "    x = grid2d\n",
    "    # Prior\n",
    "    mu = np.zeros(2)\n",
    "    B = 25 * np.array([[1, corr_B],\n",
    "                       [corr_B, 1]])\n",
    "    # Likelihood\n",
    "    cov_R = np.sqrt(R1*R2)*corr_R\n",
    "    R = np.array([[R1, cov_R],\n",
    "                  [cov_R, R2]])\n",
    "    y = np.array([y1, y2])\n",
    "    Hx = x\n",
    "    #Hx = x**2/4\n",
    "    #Hx = x**3/36\n",
    "\n",
    "    #Hx = x[..., :1] * x[..., 1:2]\n",
    "    #y1_only = True\n",
    "\n",
    "    if y1_only:\n",
    "        y = y[:1]\n",
    "        R = R[:1, :1]\n",
    "        Hx = Hx[..., :1]\n",
    "\n",
    "    # Compute\n",
    "    lklhd = pdf_GM(y, Hx, R)\n",
    "    prior = pdf_GM(x, mu, B)\n",
    "    postr = Bayes_rule(prior, lklhd, dx**2)\n",
    "\n",
    "    ax, plot = ws.get_jointplotter(grid1d)\n",
    "    contours = [plot(prior, 'blue'),\n",
    "                plot(lklhd, 'green'),\n",
    "                plot(postr, 'red', linewidths=2)]\n",
    "    ax.legend(contours, ['prior', 'lklhd', 'postr'], loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fea4a2",
   "metadata": {},
   "source": [
    "#### Exc\n",
    "- Does the posterior (pdf) lie \"between\" the prior and likelihood?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eda57e4",
   "metadata": {},
   "source": [
    "#### Exc: Observation models\n",
    "- Implement $\\mathcal{H}(\\x) = \\x_1$.\n",
    "- Implement $\\mathcal{H}(\\x) = \\frac{1}{M} \\sum_{i=1}^M \\x_i$.\n",
    "- Implement $\\mathcal{H}(\\x) = \\x_2 - \\x_1$.\n",
    "- Implement $\\mathcal{H}(\\x) = (\\x_1^2, \\x_2^2)$.\n",
    "- Implement $\\mathcal{H}(\\x) = \\x_1 \\x_2$.\n",
    "- For those of the above models that are linear,\n",
    "  find the matrix $\\bH$ such that $\\mathcal{H}(\\x) = \\bH \\x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea1e53b",
   "metadata": {},
   "source": [
    "It will not surprise you to learn that the shape of the posterior is again Gaussian,\n",
    "essentially for the same reason as in 1D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e02e31",
   "metadata": {},
   "source": [
    "### Next: [T4 - Filtering & time series](T4%20-%20Filtering%20%26%20time%20series.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
