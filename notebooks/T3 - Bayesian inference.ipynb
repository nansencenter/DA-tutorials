{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a39d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote = \"https://raw.githubusercontent.com/nansencenter/DA-tutorials\"\n",
    "!wget -qO- {remote}/master/notebooks/resources/colab_bootstrap.sh | bash -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b268717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources import show_answer, interact, import_from_nb, get_jointplotter\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec2093d",
   "metadata": {},
   "source": [
    "# T3 - Bayesian inference\n",
    "\n",
    "The [previous tutorial](T2%20-%20Gaussian%20distribution.ipynb)\n",
    "$\n",
    "\\newcommand{\\Expect}[0]{\\mathbb{E}}\n",
    "\\newcommand{\\NormDist}{\\mathscr{N}}\n",
    "\\newcommand{\\ObsMod}[0]{\\mathscr{H}}\n",
    "\\newcommand{\\mat}[1]{{\\mathbf{{#1}}}}\n",
    "\\newcommand{\\bvec}[1]{{\\mathbf{#1}}}\n",
    "\\newcommand{\\supa}[0]{^\\text{a}}\n",
    "\\newcommand{\\supf}[0]{^\\text{f}}\n",
    "$\n",
    "studied the Gaussian probability density function (pdf), defined in 1D by:\n",
    "$$ \\large \\NormDist(x \\mid \\mu, \\sigma^2) = (2 \\pi \\sigma^2)^{-1/2} e^{-(x-\\mu)^2/2 \\sigma^2} \\,,\\tag{G1} $$\n",
    "which we implemented and tested alongside the uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d1dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(pdf_G1, pdf_U1, bounds, dx, grid1d) = import_from_nb(\"T2\", (\"pdf_G1\", \"pdf_U1\", \"bounds\", \"dx\", \"grid1d\"))\n",
    "pdfs = dict(N=pdf_G1, U=pdf_U1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8add4a2",
   "metadata": {},
   "source": [
    "*We no longer use uppercase to distinguish random variables from their outcomes (an unfortunate consequence of the myriad of notations to keep track of)!*\n",
    "\n",
    "Now that we have reviewed some probability, we can look at statistical inference and estimation, in particular\n",
    "\n",
    "# Bayes' rule\n",
    "\n",
    "<details style=\"border: 1px solid #aaaaaa; border-radius: 4px; padding: 0.5em 0.5em 0;\">\n",
    "  <summary style=\"font-weight: normal; font-style: italic; margin: -0.5em -0.5em 0; padding: 0.5em;\">\n",
    "    In the Bayesian approach, knowledge and uncertainty about some unknown ($x$) is quantified through probability ... (optional reading üîç)\n",
    "  </summary>\n",
    "\n",
    "  For example, what is the temperature at the surface of Mercury (at some given point and time)?\n",
    "  Not many people know the answer. Perhaps you say $500^{\\circ} C, \\, \\pm \\, 20$.\n",
    "  But that's hardly anything compared to you real uncertainty, so you revise that to $\\pm \\, 1000$.\n",
    "  But then you're allowing for temperature below absolute zero, which you of course don't believe is possible.\n",
    "  You can continue to refine the description of your uncertainty.\n",
    "  Ultimately (in the limit) the complete way to express your belief is as a *distribution*\n",
    "  (essentially just a list) of plausibilities for all possibilities.\n",
    "  Furthermore, the only coherent way to reason in the presence of such uncertainty\n",
    "  is to obey the laws of probability ([Jaynes (2003)](#References)).\n",
    "\n",
    "  - - -\n",
    "</details>\n",
    "\n",
    "And **Bayes' rule** is how we do inference: it says how to condition/merge/assimilate/update this belief based on data/observation ($y$).\n",
    "For *continuous* random variables, $x$ and $y$, it reads:\n",
    "$$\n",
    "\\large\n",
    "\\color{red}{\\overset{\\mbox{Posterior}}{p(\\color{black}{x|y})}} = \\frac{\\color{blue}{\\overset{\\mbox{  Prior  }}{p(\\color{black}{x})}} \\, \\color{green}{\\overset{\\mbox{ Likelihood}}{p(\\color{black}{y|x})}}}{\\color{gray}{\\underset{\\mbox{Constant wrt. x}}{p(\\color{black}{y})}}} \\,. \\tag{BR} \\\\[1em]\n",
    "$$\n",
    "\n",
    "**Exc -- Bayes' rule derivation:** Derive eqn. (BR) from the definition of [conditional pdf's](https://en.wikipedia.org/wiki/Conditional_probability_distribution#Conditional_continuous_distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb63318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('symmetry of conjunction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe582dc",
   "metadata": {},
   "source": [
    "It is hard to overstate how simple Bayes' rule, eqn. (BR), is, consisting merely of scalar multiplication and division.\n",
    "However, we want to compute the function $p(x|y)$ for **all values of $x$**.\n",
    "Thus, upon discretization, eqn. (BR) becomes the multiplication of two *arrays* of values (followed by a normalisation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a7b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bayes_rule(prior_values, lklhd_values, dx):\n",
    "    prod = prior_values * lklhd_values         # pointwise multiplication\n",
    "    posterior_values = prod/(np.sum(prod)*dx)  # normalization\n",
    "    return posterior_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a814ed05",
   "metadata": {},
   "source": [
    "#### Exc (optional) -- BR normalization\n",
    "\n",
    "Show that the normalization in `Bayes_rule()` amounts to (approximately) the same as dividing by $p(y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c28fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('quadrature marginalisation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad362aca",
   "metadata": {},
   "source": [
    "In fact, since $p(y)$ is thusly implicitly known,\n",
    "we often don't bother to write it down, simplifying Bayes' rule (eqn. BR) to\n",
    "\n",
    "$$ p(x|y) \\propto p(x) \\, p(y|x) \\,.  \\tag{BR2} $$\n",
    "\n",
    "Actually, do we even need to care about $p(y)$ at all? All we really need to know is how much more likely some value of $x$ (or an interval around it) is compared to any other $x$.\n",
    "The normalisation is only necessary because of the *convention* that all densities integrate to $1$.\n",
    "However, for large models, we usually can only afford to evaluate $p(y|x)$ at a few points (of $x$), so that the integral for $p(y)$ can only be roughly approximated. In such settings, estimation of the normalisation factor becomes an important question too.\n",
    "\n",
    "<a name=\"Interactive-illustration\"></a>\n",
    "\n",
    "## Interactive illustration\n",
    "\n",
    "The code below shows Bayes' rule in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad62ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(y=(*bounds, 1), logR=(-3, 5, .5), prior_kind=list(pdfs), lklhd_kind=list(pdfs))\n",
    "def Bayes1(y=9.0, logR=1.0, lklhd_kind=\"N\", prior_kind=\"N\"):\n",
    "    R = 4**logR\n",
    "    xf = 10\n",
    "    Pf = 4**2\n",
    "\n",
    "    # (See exercise below)\n",
    "    def H(x):\n",
    "        return 1*x + 0\n",
    "\n",
    "    x = grid1d\n",
    "    prior_vals = pdfs[prior_kind](x, xf, Pf)\n",
    "    lklhd_vals = pdfs[lklhd_kind](y, H(x), R)\n",
    "    postr_vals = Bayes_rule(prior_vals, lklhd_vals, dx)\n",
    "\n",
    "    def plot(x, y, c, lbl):\n",
    "        plt.fill_between(x, y, color=c, alpha=.3, label=lbl)\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plot(x, prior_vals, 'blue'  , f'Prior, {prior_kind}(x | {xf:.4g}, {Pf:.4g})')\n",
    "    plot(x, lklhd_vals, 'green' , f'Lklhd, {lklhd_kind}({y} | x, {R:.4g})')\n",
    "    plot(x, postr_vals, 'red'   , f'Postr, pointwise')\n",
    "\n",
    "    try:\n",
    "        # (See exercise below)\n",
    "        H_lin = H(xf)/xf # a simple linear approximation of H(x)\n",
    "        xa, Pa = Bayes_rule_LG1(xf, Pf, y, H_lin, R)\n",
    "        label = f'Postr, parametric\\nN(x | {xa:.4g}, {Pa:.4g})'\n",
    "        postr_vals_G1 = pdf_G1(x, xa, Pa)\n",
    "        plt.plot(x, postr_vals_G1, 'purple', label=label)\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    plt.ylim(0, 0.6)\n",
    "    plt.legend(loc=\"upper left\", prop={'family': 'monospace'})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36d610c",
   "metadata": {},
   "source": [
    "The illustration uses a\n",
    "\n",
    "- prior $p(x) = \\NormDist(x|x^f, P^f)$ with (fixed) mean and variance, $x^f= 10$, $P^f=4^2$.\n",
    "- likelihood $p(y|x) = \\NormDist(y|x, R)$, whose parameters are set by the interactive sliders.\n",
    "\n",
    "We are now dealing with 3 (!) separate distributions,\n",
    "giving us a lot of symbols to keep straight in our head -- a necessary evil for later.\n",
    "\n",
    "**Exc -- `Bayes1` properties:** This exercise serves to make you acquainted with how Bayes' rule blends information.\n",
    "\n",
    "Move the sliders (use arrow keys?) to animate it, and answer the following (with the boolean checkmarks both on and off).\n",
    "\n",
    "- What happens to the posterior when $R \\rightarrow \\infty$ ?\n",
    "- What happens to the posterior when $R \\rightarrow 0$ ?\n",
    "- Move $y$ around. What is the posterior's location (mean/mode) when $R$ equals the prior variance?\n",
    "- Can you say something universally valid (for any $y$ and $R$) about the height of the posterior pdf?\n",
    "- Does the posterior scale (width) depend on $y$?  \n",
    "   *Optional*: What does this mean [information-wise](https://en.wikipedia.org/wiki/Differential_entropy#Differential_entropies_for_various_distributions)?\n",
    "- Consider the shape (ignoring location & scale) of the posterior. Does it depend on $R$ or $y$?\n",
    "- Can you see a shortcut to computing this posterior rather than having to do the pointwise multiplication?\n",
    "- For the case of two uniform distributions: What happens when you move the prior and likelihood too far apart? Is the fault of the implementation, the math, or the problem statement?\n",
    "- Play around with the grid resolution (see the cell above). What is in your opinion a \"sufficient\" grid resolution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ab05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('Posterior behaviour')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a76780b",
   "metadata": {},
   "source": [
    "## With forward (observation) models\n",
    "\n",
    "In general, the observation $y$ is not a \"direct\" measurement of $x$, as above,\n",
    "but rather some transformation, i.e. function of $x$,\n",
    "which is called **observation/forward model**, $\\ObsMod$.\n",
    "Examples include:\n",
    "\n",
    "- $\\ObsMod(x) = x + 273$ for a thermometer reporting ¬∞C, while $x$ is the temperature in ¬∞K.\n",
    "- $\\ObsMod(x) = 10 x$ for a ruler using mm, while $x$ is stored as cm.\n",
    "- $\\ObsMod(x) = \\log(x)$ for litmus paper (pH measurement), where $x$ is the molar concentration of hydrogen ions.\n",
    "- $\\ObsMod(x) = |x|$ for bicycle speedometers (measuring rpm, i.e. Hall effect sensors).\n",
    "- $\\ObsMod(x) = 2 \\pi h \\, x^2$ if observing inebriation (drunkenness), and the unknown, $x$, is the radius of the beer glasses.\n",
    "\n",
    "Of course, the linear and logarithmic transformations are hardly worthy of the name \"model\", since they merely change the scale of measurement, and so could be trivially done away with. But doing so is not necessary, and they will serve to illustrate some important points.\n",
    "\n",
    "In addition, measurement instruments always (at least for continuous variables) have limited accuracy,\n",
    "i.e. there is an **measurement noise/error** corrupting the observation. For simplicity, this noise is usually assumed *additive*, so that the observation, $y$, is related to the true state, $x$, by\n",
    "$$\n",
    "y = \\ObsMod(x) + \\varepsilon \\,, \\;\\; \\qquad \\tag{Obs}\n",
    "$$\n",
    "and $\\varepsilon \\sim \\NormDist(0, R)$ for some variance $R>0$.\n",
    "Then the likelihood is $$p(y|x) = \\NormDist(y| \\ObsMod(x), R) \\,. \\tag{Lklhd}$$\n",
    "\n",
    "**Exc (optional) -- The likelihood:** Derive the expression (Lklhd) for the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d61519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('Likelihood')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d0618c",
   "metadata": {},
   "source": [
    "#### Exc -- Obs. model gallery\n",
    "\n",
    "Consider the following observation models.\n",
    "\n",
    "- (a) $\\ObsMod(x) = x + 15$.\n",
    "- (b) $\\ObsMod(x) = 2 x$.\n",
    "- (c) $\\ObsMod(x) = (x-5)^2$.\n",
    "  - Explain how negative values of $y$ are possible.\n",
    "- (d) Try $\\ObsMod(x) = |x|$.\n",
    "\n",
    "In each case, describe how the likelihood approximately changes as compared to $\\ObsMod(x) = x$.\n",
    "Then verify your answer by implementing `H` in the [interactive Bayes' rule](#Interactive-illustration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdf63e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('Observation models', 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc12b697",
   "metadata": {},
   "source": [
    "It is important to appreciate that the likelihood and its role in Bayes' rule, does no \"inversion\". It simply quantifies how well $x$ fits to the data in terms of its weighting. This approach also inherently handles the fact that multiple values of $x$ may be plausible.\n",
    "\n",
    "**Exc (optional) -- \"why inverse\":** Laplace called \"statistical inference\" the reasoning of \"inverse probability\" (1774). You may also have heard of \"inverse problems\" in reference to similar problems, but without a statistical framing. In view of this, why do you think we use $x$ for the unknown, and $y$ for the known/given data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c45b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer(\"what's forward?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766ae502",
   "metadata": {},
   "source": [
    "<a name=\"Linear-Gaussian-Bayes'-rule-(1D)\"></a>\n",
    "\n",
    "## Linear-Gaussian Bayes' rule (1D)\n",
    "\n",
    "In response to this computational difficulty, we try to be smart and do something more analytical (\"pen-and-paper\"): we only compute the parameters (mean and (co)variance) of the posterior pdf.\n",
    "This is doable and quite simple in the linear-Gaussian case, i.e. when $\\ObsMod$ is linear (i.e. just a number). For readability, the unknown, $x$, is colored.\n",
    "\n",
    "- Given the prior of $p(\\color{darkorange}{x}) = \\NormDist(\\color{darkorange}{x} \\mid x\\supf, P\\supf)$\n",
    "- and a likelihood $p(y|\\color{darkorange}{x}) = \\NormDist(y \\mid \\ObsMod \\color{darkorange}{x},R)$,  \n",
    "- $\\implies$ posterior $\n",
    "  p(\\color{darkorange}{x}|y)\n",
    "  = \\NormDist(\\color{darkorange}{x} \\mid x\\supa, P\\supa) \\,, $\n",
    "  where, in the 1-dimensional/univariate/scalar (multivariate is discussed in [T5](T5%20-%20Multivariate%20Kalman%20filter.ipynb)) case:\n",
    "  $$\\begin{align}\n",
    "    P\\supa &= 1/(1/P\\supf + \\ObsMod^2/R) \\,, \\tag{5} \\\\\\\n",
    "    x\\supa &= P\\supa (x\\supf/P\\supf + \\ObsMod y/R) \\,.  \\tag{6}\n",
    "  \\end{align}$$\n",
    "\n",
    "The proof is in the following exercise.\n",
    "\n",
    "#### Exc -- BR-LG1\n",
    "\n",
    "Consider the following identity, where $P\\supa$ and $x\\supa$ are given by eqns. (5) and (6).\n",
    "$$\n",
    "\\frac{(\\color{darkorange}{x}-x\\supf)^2}{P\\supf} + \\frac{(\\ObsMod \\color{darkorange}{x}-y)^2}{R} \\quad =\n",
    "\\quad \\frac{(\\color{darkorange}{x} - x\\supa)^2}{P\\supa} + \\frac{(y - \\ObsMod x\\supf)^2}{R + P\\supf} \\,, \\tag{LG1}\n",
    "$$\n",
    "Notice that the left hand side (LHS) is the sum of *two* squares with $\\color{darkorange}{x}$,\n",
    "but the RHS only contains *one*.\n",
    "\n",
    "- (a) Actually derive the first term of the RHS of (LG1), i.e. eqns. (5) and (6).  \n",
    "  *Hint: you can simplify the task by first \"hiding\" $\\ObsMod$*\n",
    "- (b) *Optional*: Derive the full RHS (i.e. also the second term).\n",
    "- (c) Show that $p(\\color{darkorange}{x}|y) = \\NormDist(\\color{darkorange}{x} \\mid x\\supa, P\\supa)$\n",
    "  using part (a), Bayes' rule (BR2), and the Gaussian pdf (G1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90eb909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('BR Gauss, a.k.a. completing the square', 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3527be59",
   "metadata": {},
   "source": [
    "**Exc -- Temperature example:**\n",
    "The statement $x = \\mu \\pm \\sigma$ is *sometimes* used\n",
    "as a shorthand for $p(x) = \\NormDist(x \\mid \\mu, \\sigma^2)$. Suppose\n",
    "\n",
    "- you think the temperature $x = 20¬∞C \\pm 2¬∞C$,\n",
    "- a thermometer yields the observation $y = 18¬∞C \\pm 2¬∞C$.\n",
    "\n",
    "Show that your posterior is $p(x|y) = \\NormDist(x \\mid 19, 2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab02ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('LG BR example')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0925f3",
   "metadata": {},
   "source": [
    "The following implements a linear-Gaussian Bayes' rule (eqns. 5 and 6).\n",
    "Note that its inputs and outputs are not discretized density values (as for `Bayes_rule()`), but simply 5 numbers: the means, variances and $\\ObsMod$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd52539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bayes_rule_LG1(xf, Pf, y, H, R):\n",
    "    Pa = 1 / (1/Pf + H**2/R)\n",
    "    xa = Pa * (xf/Pf + H*y/R)\n",
    "    return xa, Pa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b342a",
   "metadata": {},
   "source": [
    "#### Exc -- Gaussianity as an approximation\n",
    "\n",
    "Re-run/execute the interactive animation code cell up above.\n",
    "\n",
    "- (a) Under what conditions does `Bayes_rule_LG1()` provide a good approximation to `Bayes_rule()`?\n",
    "- (b) Try using one or more of the other [distributions readily available in `scipy`](https://stackoverflow.com/questions/37559470/) in the above animation by inserting them in `pdfs`.\n",
    "\n",
    "**Exc (optional) -- Gain algebra:** Show that eqn. (5) can be written as\n",
    "$$P\\supa = K R / \\ObsMod \\,,    \\tag{8}$$\n",
    "where\n",
    "$$K = \\frac{\\ObsMod P\\supf}{\\ObsMod^2 P\\supf + R} \\,,    \\tag{9}$$\n",
    "is called the \"Kalman gain\".  \n",
    "*Hint: again, try to \"hide away\" $\\ObsMod$ among the other objects before proceeding.*\n",
    "\n",
    "Then shown that eqns. (5) and (6) can be written as\n",
    "$$\n",
    "\\begin{align}\n",
    "    P\\supa &= (1-K \\ObsMod) P\\supf \\,,  \\tag{10} \\\\\\\n",
    "  x\\supa &= x\\supf + K (y- \\ObsMod x\\supf) \\tag{11} \\,,\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce20dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('BR Kalman1 algebra')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34803dbd",
   "metadata": {},
   "source": [
    "#### Exc (optional) -- Gain intuition\n",
    "\n",
    "Let $\\ObsMod = 1$ for simplicity.\n",
    "\n",
    "- (a) Show that $0 < K < 1$ since $0 < P\\supf, R$.\n",
    "- (b) Show that $P\\supa < P\\supf, R$.\n",
    "- (c) Show that $x\\supa$ is in the interval $(x\\supf, y)$.\n",
    "- (d) Why do you think $K$ is called a \"gain\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('KG intuition')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbec053",
   "metadata": {},
   "source": [
    "**Exc -- BR with Gain:** Re-define `Bayes_rule_LG1` so to as to use eqns. 9-11. Remember to re-run the cell. Verify that you get the same plots as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc66c164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('BR Kalman1 code')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369cec84",
   "metadata": {},
   "source": [
    "#### Exc (optional) -- optimalities\n",
    "\n",
    "In contrast to orthodox statistics,\n",
    "Bayes' rule (BR) itself makes no attempt at producing only a single estimate/value of $x$.\n",
    "It merely states how quantitative belief (weighted possibilities) should be updated in view of new data.\n",
    "*But if you must* pick a single point value estimate $\\hat{x}$\n",
    "(in order to perform a contingent action, for example),\n",
    "you can **decide** on it by optimising (with respect to $\\hat{x}$)\n",
    "the expected value of some utility/loss function [[ref](https://en.wikipedia.org/wiki/Bayes_estimator)],\n",
    "for example mean-squared: $\\text{Loss}(x - \\hat{x}) = (x - \\hat{x})^2$.\n",
    "\n",
    "- For example, if the density of $x$ is symmetric,\n",
    "  and $\\text{Loss}$ is convex and symmetric,\n",
    "  then $\\Expect[\\text{Loss}(x - \\hat{x})]$ is minimized\n",
    "  by the mean, $\\Expect[x]$, which also coincides with the median.\n",
    "  Ref Corollary 7.19 of Lehmann & Casella (1998).\n",
    "- (a) Show that, for the expected *squared* loss, $\\Expect[(x - \\hat{x})^2]$,\n",
    "  the minimum is the mean for *any distribution*.\n",
    "  *Hint: insert $0 = \\,?\\, - \\,?$.*\n",
    "- (b) Show that linearity can replace Gaussianity in the 1st bullet point.\n",
    "  *PS: this gives rise to various optimality claims of the Kalman filter,\n",
    "  such as it being the best linear-unbiased estimator (BLUE).*\n",
    "\n",
    "In summary, the intuitive idea of **considering the mean of $p(x)$ as the point estimate** has good theoretical foundations.\n",
    "\n",
    "## Summary\n",
    "\n",
    "Bayesian inference quantifies uncertainty (in $x$) using the notion of probability.\n",
    "Bayes' rule says how to condition/merge/assimilate/update this belief based on data/observation ($y$).\n",
    "It is simply a re-formulation of the notion of conditional probability.\n",
    "Observation can be \"inverted\" using Bayes' rule,\n",
    "in the sense that all possibilities for $x$ are weighted.\n",
    "While technically simple, Bayes' rule becomes expensive to compute in high dimensions,\n",
    "but if Gaussianity can be assumed then it reduces to only 2 formulae.\n",
    "\n",
    "### Next: [T4 - Filtering & time series](T4%20-%20Time%20series%20filtering.ipynb)\n",
    "\n",
    "<a name=\"References\"></a>\n",
    "\n",
    "### References\n",
    "\n",
    "- **Jaynes (2003)**:\n",
    "  Edwin T. Jaynes, \"Probability theory: the logic of science\", 2003.\n",
    "- **Lehmann & Casella (1998)**:\n",
    "  \"Theory of Point Estimation\", 1998."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "jupytext": {
   "formats": "ipynb,scripts//py:light,scripts//md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
