{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33a6a71d",
   "metadata": {},
   "source": [
    "# T3 - Bayesian inference\n",
    "Now that we have reviewed some probability, we can look at statistical inference.\n",
    "$\n",
    "% Loading TeX (MathJax)... Please wait\n",
    "%\n",
    "\\newcommand{\\Reals}{\\mathbb{R}}\n",
    "\\newcommand{\\Expect}[0]{\\mathbb{E}}\n",
    "\\newcommand{\\NormDist}{\\mathcal{N}}\n",
    "%\n",
    "\\newcommand{\\DynMod}[0]{\\mathscr{M}}\n",
    "\\newcommand{\\ObsMod}[0]{\\mathscr{H}}\n",
    "%\n",
    "\\newcommand{\\mat}[1]{{\\mathbf{{#1}}}}\n",
    "%\\newcommand{\\mat}[1]{{\\pmb{\\mathsf{#1}}}}\n",
    "\\newcommand{\\bvec}[1]{{\\mathbf{#1}}}\n",
    "%\n",
    "\\newcommand{\\trsign}{{\\mathsf{T}}}\n",
    "\\newcommand{\\tr}{^{\\trsign}}\n",
    "\\newcommand{\\ceq}[0]{\\mathrel{≔}}\n",
    "\\newcommand{\\xDim}[0]{D}\n",
    "\\newcommand{\\supa}[0]{^\\text{a}}\n",
    "\\newcommand{\\supf}[0]{^\\text{f}}\n",
    "%\n",
    "\\newcommand{\\I}[0]{\\mat{I}}\n",
    "\\newcommand{\\K}[0]{\\mat{K}}\n",
    "\\newcommand{\\bP}[0]{\\mat{P}}\n",
    "\\newcommand{\\bH}[0]{\\mat{H}}\n",
    "\\newcommand{\\bF}[0]{\\mat{F}}\n",
    "\\newcommand{\\R}[0]{\\mat{R}}\n",
    "\\newcommand{\\Q}[0]{\\mat{Q}}\n",
    "\\newcommand{\\B}[0]{\\mat{B}}\n",
    "\\newcommand{\\C}[0]{\\mat{C}}\n",
    "\\newcommand{\\Ri}[0]{\\R^{-1}}\n",
    "\\newcommand{\\Bi}[0]{\\B^{-1}}\n",
    "\\newcommand{\\X}[0]{\\mat{X}}\n",
    "\\newcommand{\\A}[0]{\\mat{A}}\n",
    "\\newcommand{\\Y}[0]{\\mat{Y}}\n",
    "\\newcommand{\\E}[0]{\\mat{E}}\n",
    "\\newcommand{\\U}[0]{\\mat{U}}\n",
    "\\newcommand{\\V}[0]{\\mat{V}}\n",
    "%\n",
    "\\newcommand{\\x}[0]{\\bvec{x}}\n",
    "\\newcommand{\\y}[0]{\\bvec{y}}\n",
    "\\newcommand{\\z}[0]{\\bvec{z}}\n",
    "\\newcommand{\\q}[0]{\\bvec{q}}\n",
    "\\newcommand{\\br}[0]{\\bvec{r}}\n",
    "\\newcommand{\\bb}[0]{\\bvec{b}}\n",
    "%\n",
    "\\newcommand{\\bx}[0]{\\bvec{\\bar{x}}}\n",
    "\\newcommand{\\by}[0]{\\bvec{\\bar{y}}}\n",
    "\\newcommand{\\barB}[0]{\\mat{\\bar{B}}}\n",
    "\\newcommand{\\barP}[0]{\\mat{\\bar{P}}}\n",
    "\\newcommand{\\barC}[0]{\\mat{\\bar{C}}}\n",
    "\\newcommand{\\barK}[0]{\\mat{\\bar{K}}}\n",
    "%\n",
    "\\newcommand{\\D}[0]{\\mat{D}}\n",
    "\\newcommand{\\Dobs}[0]{\\mat{D}_{\\text{obs}}}\n",
    "\\newcommand{\\Dmod}[0]{\\mat{D}_{\\text{obs}}}\n",
    "%\n",
    "\\newcommand{\\ones}[0]{\\bvec{1}}\n",
    "\\newcommand{\\AN}[0]{\\big( \\I_N - \\ones \\ones\\tr / N \\big)}\n",
    "%\n",
    "% END OF MACRO DEF\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b268717",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import resources.workspace as ws\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67741537",
   "metadata": {},
   "source": [
    "The [previous tutorial](T2%20-%20Gaussian%20distribution.ipynb)\n",
    "studied the Gaussian probability density function (pdf), defined by:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathcal{N}(x \\mid \\mu, \\sigma^2) &= (2 \\pi \\sigma^2)^{-1/2} e^{-(x-\\mu)^2/2 \\sigma^2} \\,,\\tag{G1} \\\\\n",
    "\\NormDist(\\x \\mid  \\mathbf{\\mu}, \\mathbf{\\Sigma})\n",
    "&=\n",
    "|2 \\pi \\mathbf{\\Sigma}|^{-1/2} \\, \\exp\\Big(-\\frac{1}{2}\\|\\x-\\mathbf{\\mu}\\|^2_\\mathbf{\\Sigma} \\Big) \\,, \\tag{GM}\n",
    "\\end{align}$$\n",
    "In this tutorial, we grab implentations straight from `scipy.stats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d1dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_G1(x, meanval, variance):\n",
    "    return sp.stats.norm.pdf(x, loc=meanval, scale=np.sqrt(variance))\n",
    "\n",
    "def pdf_GM(points, mu, Sigma):\n",
    "    diff = points - mu  # ensures both get broadcast\n",
    "    zero = np.zeros(len(Sigma))\n",
    "    return sp.stats.multivariate_normal(zero, Sigma).pdf(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4594d7c9",
   "metadata": {},
   "source": [
    "In addition,\n",
    "the following implements the the [uniform](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous))\n",
    "(or \"flat\" or \"box\") pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7633c2d8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def pdf_U1(x, meanval, variance):\n",
    "    # pdfx = sp.stats.uniform(loc=lower, scale=(upper-lower)).pdf(x)\n",
    "    lower = meanval - np.sqrt(3*variance)\n",
    "    upper = meanval + np.sqrt(3*variance)\n",
    "    height = 1/(upper - lower)\n",
    "    pdfx = height * np.ones_like(x)\n",
    "    pdfx[x<lower] = 0\n",
    "    pdfx[x>upper] = 0\n",
    "    return pdfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8add4a2",
   "metadata": {},
   "source": [
    "We'll be playing with these distribution on the following numerical grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d00347",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "bounds = -15, 15\n",
    "grid1d = np.linspace(*bounds, 201)\n",
    "grid2d = np.dstack(np.meshgrid(grid1d, grid1d))\n",
    "dx = grid1d[1] - grid1d[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75694783",
   "metadata": {},
   "source": [
    "This will help illustrate:\n",
    "\n",
    "# Bayes' rule\n",
    "In the Bayesian approach, knowledge and uncertainty about the unknown ($x$)\n",
    "is quantified through probability.\n",
    "And **Bayes' rule** is how we do inference: it says how to condition/merge/assimilate/update this belief based on data/observation ($y$).\n",
    "For *continuous* \"random variables\", $x$ and $y$, it reads:\n",
    "\n",
    "$$\\begin{align}\n",
    "p(x|y) &= \\frac{p(x) \\, p(y|x)}{p(y)} \\, , \\tag{BR} \\\\[1em]\n",
    "\\text{i.e.} \\qquad \\texttt{posterior}\\,\\text{[pdf of $x$ given $y$]}\n",
    "\\; &= \\;\n",
    "\\frac{\\texttt{prior}\\,\\text{[pdf of $x$]}\n",
    "\\; \\times \\;\n",
    "\\texttt{likelihood}\\,\\text{[pdf of $y$ given $x$]}}\n",
    "{\\texttt{normalisation}\\,\\text{[pdf of $y$]}} \\, ,\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95555a5",
   "metadata": {},
   "source": [
    "Note that, in contrast to (the frequent aim of) classical statistics,\n",
    "Bayes' rule in itself makes no attempt at producing only a single estimate\n",
    "(but the topic is briefly discussed [further below](#Exc-(optional)----optimality-of-the-mean)).\n",
    "It merely states how quantitative belief (weighted possibilities) should be updated in view of new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e527c45",
   "metadata": {},
   "source": [
    "**Exc -- Bayes' rule derivation:** Derive eqn. (BR) from the definition of [conditional pdf's](https://en.wikipedia.org/wiki/Conditional_probability_distribution#Conditional_continuous_distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb63318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('symmetry of conditioning')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc12b697",
   "metadata": {},
   "source": [
    "**Exc (optional) -- \"why inverse\":** Laplace called \"statistical inference\" the reasoning of \"inverse probability\" (1774). You may also have heard of \"inverse problems\" in reference to similar problems, but without a statistical framing. In view of this, why do you think we use $x$ for the unknown, and $y$ for the known/given data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c45b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer(\"what's forward?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe582dc",
   "metadata": {},
   "source": [
    "Bayes' rule, eqn. (BR), involves functions (the densities), but applies for any/all values of $x$ (and $y$).\n",
    "Thus, upon discretisation, eqn. (BR) becomes the multiplication of two arrays of values,\n",
    "followed by a normalisation (explained [below](#Exc-(optional)----BR-normalization)).\n",
    "It is hard to overstate how simple this principle is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a7b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bayes_rule(prior_values, lklhd_values, dx):\n",
    "    prod = prior_values * lklhd_values         # pointwise multiplication\n",
    "    posterior_values = prod/(np.sum(prod)*dx)  # normalization\n",
    "    return posterior_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b6c68d",
   "metadata": {},
   "source": [
    "The code below shows Bayes' rule in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad62ea",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@ws.interact(y=(*bounds, 1), R=(0.01, 20, 0.2), top=['y', 'R'])\n",
    "def Bayes1(  y=4.0,          R=1.0, prior_is_G=True, lklhd_is_G=True):\n",
    "    xf = 0\n",
    "    Pf = 1\n",
    "    x = grid1d\n",
    "\n",
    "    prior_vals = pdf_G1(x, xf, Pf) if prior_is_G else pdf_U1(x, xf, Pf)\n",
    "    lklhd_vals = pdf_G1(y, x, R)   if lklhd_is_G else pdf_U1(y, x, R)\n",
    "    postr_vals = Bayes_rule(prior_vals, lklhd_vals, dx)\n",
    "\n",
    "    def plot(x, y, c, lbl):\n",
    "        plt.fill_between(x, y, color=c, alpha=.3, label=lbl)\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plot(x, prior_vals, 'blue'  , f'Prior, N(x | {xf:.4g}, {Pf:.4g})')\n",
    "    plot(x, lklhd_vals, 'green' , f'Lklhd, N({y} | x, {R:.4g})')\n",
    "    plot(x, postr_vals, 'red'   , f'Postr, pointwise')\n",
    "\n",
    "    try:\n",
    "        # See exercise below\n",
    "        xa, Pa = Bayes_rule_G1(xf, Pf, y, R)\n",
    "        label = f'Postr, parametric\\nN(x | {xa:.4g}, {Pa:.4g})'\n",
    "        postr_vals_G1 = pdf_G1(x, xa, Pa)\n",
    "        plt.plot(x, postr_vals_G1, 'purple', label=label)\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    plt.ylim(0, 0.6)\n",
    "    plt.legend(loc=\"upper right\", prop={'family': 'monospace'})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36d610c",
   "metadata": {},
   "source": [
    "**Exc -- Bayes1 properties:** This exercise serves to make you acquainted with how Bayes' rule blends information.  \n",
    " Move the sliders (use arrow keys?) to animate it, and answer the following (with the boolean checkmarks both on and off).\n",
    " * What happens to the posterior when $R \\rightarrow \\infty$ ?\n",
    " * What happens to the posterior when $R \\rightarrow 0$ ?\n",
    " * Move $y$ around. What is the posterior's location (mean/mode) when $R$ equals the prior variance?\n",
    " * Can you say something universally valid (for any $y$ and $R$) about the height of the posterior pdf?\n",
    " * Does the posterior scale (width) depend on $y$?  \n",
    "   *Optional*: What does this mean [information-wise](https://en.wikipedia.org/wiki/Differential_entropy#Differential_entropies_for_various_distributions)?\n",
    " * Consider the shape (ignoring location & scale) of the posterior. Does it depend on $R$ or $y$?\n",
    " * Can you see a shortcut to computing this posterior rather than having to do the pointwise multiplication?\n",
    " * For the case of two uniform distributions: What happens when you move the prior and likelihood too far apart? Is the fault of the implementation, the math, or the problem statement?\n",
    " * Play around with the grid resolution (see the cell above). What is in your opinion a \"sufficient\" grid resolution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ab05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Posterior behaviour')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d4b52",
   "metadata": {},
   "source": [
    "#### Exc (optional) -- BR normalization\n",
    "Show that the normalization in `Bayes_rule()` amounts to (approximately) the same as dividing by $p(y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbec0251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('quadrature marginalisation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d71e15",
   "metadata": {},
   "source": [
    "In fact, since $p(y)$ is thusly implicitly known,\n",
    "we often don't bother to write it down, simplifying Bayes' rule (eqn. BR) to\n",
    "$$\\begin{align}\n",
    "p(x|y) \\propto p(x) \\, p(y|x) \\, .  \\tag{BR2}\n",
    "\\end{align}$$\n",
    "Actually, do we even need to care about $p(y)$ at all? All we really need to know is how much more likely some value of $x$ (or an interval around it) is compared to any other $x$.\n",
    "The normalisation is only necessary because of the *convention* that all densities integrate to $1$.\n",
    "However, for large models, we usually can only afford to evaluate $p(y|x)$ at a few points (of $x$), so that the integral for $p(y)$ can only be roughly approximated. In such settings, estimation of the normalisation factor becomes an important question too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a76780b",
   "metadata": {},
   "source": [
    "#### Exc -- Nonlinear regression\n",
    "- (a) Suppose the \"observation model\" consists in squaring, i.e.\n",
    "      $y = x^2/4 + \\varepsilon$, i.e. $p(y|x) = \\NormDist(y|x^2/4, R)$, where $R$ is the variance of $\\varepsilon$. Implement this in the above interactive animation code.\n",
    "- (b) Try $y = |x|$. Compare with (a).\n",
    "- (c) Try $y = x + 3$. Describe the impact.\n",
    "- (d) Try $y = 2 x$. Can you reproduce a posterior obtained with $y = x$ ?\n",
    "\n",
    "Restore $y = x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d41651",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Multivariate Bayes (intermezzo)\n",
    "The following illustrates Bayes' rule in the 2D, i.e. multivariate, case.\n",
    "\n",
    "#### Exc (optional) -- the likelihood\n",
    "Suppose the observation, $\\y$, is related to the true state, $\\x$, via an \"observation (forward) model\", $\\ObsMod$:\n",
    "\\begin{align*}\n",
    "\\y &= \\ObsMod(\\x) + \\br \\, , \\;\\; \\qquad (2)\n",
    "\\end{align*}\n",
    "where the noise follows the law $\\br \\sim \\NormDist(\\bvec{0}, \\R)$ for some $\\R>0$ (meaning $\\R$ is symmetric-positive-definite).\n",
    "\n",
    "\n",
    "Derive the expression for the likelihood, $p(\\y|\\x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bb5921",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# ws.show_answer('Likelihood for additive noise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be8d9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = dict(orientation=\"vertical\"),\n",
    "@ws.interact(top=[['corr_B', 'corr_R'], 'y1_only'], bottom=[['y1', 'R1']], right=[['y2', 'R2']],\n",
    "             corr_R=(-.999, .999, .01), y1=bounds,     R1=(0.01, 36, 0.2),\n",
    "             corr_B=(-.999, .999, .01), y2=bounds + v, R2=(0.01, 36, 0.2) + v)\n",
    "def Bayes2(  corr_R=.6,                 y1=3,          R1=4**2, y1_only=False,\n",
    "             corr_B=.6,                 y2=-12,        R2=1):\n",
    "    x = grid2d\n",
    "    # Prior\n",
    "    mu = np.zeros(2)\n",
    "    B = 25 * np.array([[1, corr_B],\n",
    "                       [corr_B, 1]])\n",
    "    # Likelihood\n",
    "    cov_R = np.sqrt(R1*R2)*corr_R\n",
    "    R = np.array([[R1, cov_R],\n",
    "                  [cov_R, R2]])\n",
    "    y = np.array([y1, y2])\n",
    "    Hx = x\n",
    "    #Hx = x**2/4\n",
    "    #Hx = x**3/36\n",
    "\n",
    "    #Hx = x[..., :1] * x[..., 1:2]\n",
    "    #y1_only = True\n",
    "\n",
    "    if y1_only:\n",
    "        y = y[:1]\n",
    "        R = R[:1, :1]\n",
    "        Hx = Hx[..., :1]\n",
    "\n",
    "    # Compute\n",
    "    lklhd = pdf_GM(y, Hx, R)\n",
    "    prior = pdf_GM(x, mu, B)\n",
    "    postr = Bayes_rule(prior, lklhd, dx**2)\n",
    "\n",
    "    ax, jplot = ws.get_jointplotter(grid1d)\n",
    "    contours = [jplot(prior, 'blue'),\n",
    "                jplot(lklhd, 'green'),\n",
    "                jplot(postr, 'red', linewidths=2)]\n",
    "    ax.legend(contours, ['prior', 'lklhd', 'postr'], loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9fe02b",
   "metadata": {},
   "source": [
    "#### Exc -- Sum of ellipses\n",
    "- Does the posterior (pdf) generally lie \"between\" the prior and likelihood?\n",
    "- Why is this exercises called \"sum\" ?\n",
    "\n",
    "#### Exc (optional) -- Observation models\n",
    "- Implement $\\mathcal{H}(\\x) = \\x_1$.\n",
    "- Implement $\\mathcal{H}(\\x) = \\frac{1}{\\xDim} \\sum_{i=1}^\\xDim \\x_i$.\n",
    "- Implement $\\mathcal{H}(\\x) = \\x_2 - \\x_1$.\n",
    "- Implement $\\mathcal{H}(\\x) = (\\x_1^2, \\x_2^2)$.\n",
    "- Implement $\\mathcal{H}(\\x) = \\x_1 \\x_2$.\n",
    "- For those of the above models that are linear,\n",
    "  find the (possibly rectangular) matrix $\\bH$ such that $\\mathcal{H}(\\x) = \\bH \\x$.\n",
    " \n",
    "As simple as it is, the amount of computations done by `Bayes_rule` quickly becomes a difficulty in higher dimensions. This is hammered home in the following exercise.\n",
    "\n",
    "#### Exc (optional) -- Curse of dimensionality, part 1\n",
    " * (a) How many point-multiplications are needed on a grid with $N$ points in $\\xDim$ dimensions? Imagine an $\\xDim$-dimensional cube where each side has a grid with $N$ points on it.\n",
    " * *PS: Of course, if the likelihood contains an actual model $\\mathcal{H}(x)$ as well, its evaluations (computations) could be significantly more costly than the point-multiplications of Bayes' rule itself.*\n",
    " * (b) Suppose we model 15 physical quantities (fields), at each grid node, on a discretized surface model of Earth. Assume the resolution is $1^\\circ$ for latitude (110km), $1^\\circ$ for longitude. How many variables, $\\xDim$, are there in total? This is the ***dimensionality*** of the unknown.\n",
    " * (c) Suppose each variable is has a pdf represented with a grid using only $N=20$ points. How many multiplications are necessary to calculate Bayes rule (jointly) for all variables on our Earth model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7e801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('nD-space is big', 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766ae502",
   "metadata": {},
   "source": [
    "## Gaussian-Gaussian Bayes' rule (1D)\n",
    "\n",
    "In response to this computational difficulty, we try to be smart and do something more analytical (\"pen-and-paper\"): we only compute the parameters (mean and (co)variance) of the posterior pdf.\n",
    "\n",
    "This is doable and quite simple in the Gaussian-Gaussian case:  \n",
    "- With a prior $p(x) = \\mathcal{N}(x \\mid x\\supf, P\\supf)$ and  \n",
    "- a likelihood $p(y|x) = \\mathcal{N}(y \\mid x,R)$,  \n",
    "- the posterior is\n",
    "$\n",
    "p(x|y)\n",
    "= \\mathcal{N}(x \\mid x\\supa, P\\supa) \\,,\n",
    "$\n",
    "where, in the 1-dimensional/univariate/scalar (multivariate is discussed in [T5](T5%20-%20Kalman%20filter%20(multivariate).ipynb)) case:\n",
    "\n",
    "$$\\begin{align}\n",
    "    P\\supa &= 1/(1/P\\supf + 1/R) \\, , \\tag{5} \\\\\\\n",
    "  x\\supa &= P\\supa (x\\supf/P\\supf + y/R) \\, .  \\tag{6}\n",
    "\\end{align}$$\n",
    "\n",
    "*There are a lot of sub/super-scripts. Please take a moment to somewhat digest the formulae.*\n",
    "\n",
    "#### Exc -- GG Bayes\n",
    "Consider the following identity, where $P\\supa$ and $x\\supa$ are given by eqns. (5) and (6).\n",
    "$$\\frac{(x-x\\supf)^2}{P\\supf} + \\frac{(x-y)^2}{R} \\quad\n",
    "=\\quad \\frac{(x - x\\supa)^2}{P\\supa} + \\frac{(y - x\\supf)^2}{R + P\\supf} \\,, \\tag{S2}$$\n",
    "Notice that the left hand side (LHS) is the sum of two squares with $x$,\n",
    "but the RHS only contains one square with $x$.\n",
    "- (a) Derive the first term of the RHS, i.e. eqns. (5) and (6).\n",
    "- (b) *Optional*: Derive the full RHS (i.e. also the second term).\n",
    "- (c) Derive $p(x|y) = \\mathcal{N}(x \\mid x\\supa, P\\supa)$ from eqns. (5) and (6)\n",
    "  using part (a), Bayes' rule (BR2), and the Gaussian pdf (G1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90eb909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR Gauss, a.k.a. completing the square', 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3527be59",
   "metadata": {},
   "source": [
    "**Exc -- Temperature example:**\n",
    "The statement $x = \\mu \\pm \\sigma$ is *sometimes* used\n",
    "as a shorthand for $p(x) = \\mathcal{N}(x \\mid \\mu, \\sigma^2)$. Suppose\n",
    "- you think the temperature $x = 20°C \\pm 2°C$,\n",
    "- a thermometer yields the observation $y = 18°C \\pm 2°C$.\n",
    "\n",
    "Show that your posterior is $p(x|y) = \\mathcal{N}(x \\mid 19, 2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab02ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('GG BR example')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0925f3",
   "metadata": {},
   "source": [
    "The following implements a Gaussian-Gaussian Bayes' rule (eqns 5 and 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd52539",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def Bayes_rule_G1(xf, Pf, y, R):\n",
    "    Pa = 1 / (1/Pf + 1/R)\n",
    "    xa = Pa * (xf/Pf + y/R)\n",
    "    return xa, Pa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b342a",
   "metadata": {},
   "source": [
    "**Re-run**/execute the interactive animation code cell up above.\n",
    "*Note that the inputs and outputs for `Bayes_rule_G1()` are not discretised density values (as for `Bayes_rule()`), but simply 2 numbers: the mean and the variance.*\n",
    "\n",
    "#### Exc -- Gaussianity as an approximation\n",
    "- (a) Under what conditions does `Bayes_rule_G1()` provide a good approximation to `Bayes_rule()`?\n",
    "- (b) *Optional*. Try using one or more of the other [distributions readily available in `scipy`](https://stackoverflow.com/questions/37559470/) in the above animation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6717a439",
   "metadata": {},
   "source": [
    "**Exc -- Gain algebra:** Show that eqn. (5) can be written as\n",
    "$$P\\supa = K R \\,,    \\tag{8}$$\n",
    "where\n",
    "$$K = P\\supf/(P\\supf+R) \\,,    \\tag{9}$$\n",
    "is called the \"Kalman gain\".  \n",
    "Then shown that eqns (5) and (6) can be written as\n",
    "$$\\begin{align}\n",
    "    P\\supa &= (1-K) P\\supf \\, ,  \\tag{10} \\\\\\\n",
    "  x\\supa &= x\\supf + K (y-x\\supf) \\tag{11} \\, ,\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce20dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR Kalman1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34803dbd",
   "metadata": {},
   "source": [
    "#### Exc (optional) -- Gain intuition\n",
    "- (a) Show that $0 < K < 1$ since $0 < P\\supf, R$.\n",
    "- (b) Show that $P\\supa < P\\supf, R$.\n",
    "- (c) Show that $x\\supa \\in (x\\supf, y)$.\n",
    "- (d) Why do you think $K$ is called a \"gain\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('KG intuition')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbec053",
   "metadata": {},
   "source": [
    "**Exc -- BR with Gain:** Re-define `Bayes_rule_G1` so to as to use eqns. 9-11. Remember to re-run the cell. Verify that you get the same plots as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc66c164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR Kalman1 code')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369cec84",
   "metadata": {},
   "source": [
    "#### Exc (optional) -- optimality of the mean\n",
    "*If you must* pick a single point value for your estimate (for example, an action to be taken), you can **decide** on it by optimising (with respect to the estimate) the expected value of some utility/loss function [[ref](https://en.wikipedia.org/wiki/Bayes_estimator)].\n",
    "- For example, if the density of $X$ is symmetric,\n",
    "   and $\\text{Loss}$ is convex and symmetric,\n",
    "   then $\\Expect[\\text{Loss}(X - \\theta)]$ is minimized\n",
    "   by the mean, $\\Expect[X]$, which also coincides with the median.\n",
    "   <!-- See Corollary 7.19 of Lehmann, Casella -->\n",
    "- (a) Show that, for the expected *squared* loss, $\\Expect[(X - \\theta)^2]$,\n",
    "  the minimum is the mean for *any distribution*.\n",
    "  *Hint: insert $0 = \\,?\\, - \\,?$.*\n",
    "- (b) Show that linearity can replace Gaussianity in the 1st bullet point.\n",
    "  *PS: this gives rise to various optimality claims of the Kalman filter,\n",
    "  such as it being the best linear-unibased estimator (BLUE).*\n",
    "\n",
    "In summary, the intuitive idea of **considering the mean of $p(x)$ as the point estimate** has good theoretical foundations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e02e31",
   "metadata": {},
   "source": [
    "### Next: [T4 - Filtering & time series](T4%20-%20Filtering%20%26%20time%20series.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
