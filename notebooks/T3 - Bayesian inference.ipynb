{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbcdbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import resources.workspace as ws\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a6a71d",
   "metadata": {},
   "source": [
    "$\n",
    "% START OF MACRO DEF\n",
    "% DO NOT EDIT IN INDIVIDUAL NOTEBOOKS, BUT IN macros.py\n",
    "%\n",
    "\\newcommand{\\Reals}{\\mathbb{R}}\n",
    "\\newcommand{\\Expect}[0]{\\mathbb{E}}\n",
    "\\newcommand{\\NormDist}{\\mathcal{N}}\n",
    "%\n",
    "\\newcommand{\\DynMod}[0]{\\mathscr{M}}\n",
    "\\newcommand{\\ObsMod}[0]{\\mathscr{H}}\n",
    "%\n",
    "\\newcommand{\\mat}[1]{{\\mathbf{{#1}}}}\n",
    "%\\newcommand{\\mat}[1]{{\\pmb{\\mathsf{#1}}}}\n",
    "\\newcommand{\\bvec}[1]{{\\mathbf{#1}}}\n",
    "%\n",
    "\\newcommand{\\trsign}{{\\mathsf{T}}}\n",
    "\\newcommand{\\tr}{^{\\trsign}}\n",
    "\\newcommand{\\tn}[1]{#1}\n",
    "\\newcommand{\\ceq}[0]{\\mathrel{â‰”}}\n",
    "%\n",
    "\\newcommand{\\I}[0]{\\mat{I}}\n",
    "\\newcommand{\\K}[0]{\\mat{K}}\n",
    "\\newcommand{\\bP}[0]{\\mat{P}}\n",
    "\\newcommand{\\bH}[0]{\\mat{H}}\n",
    "\\newcommand{\\bF}[0]{\\mat{F}}\n",
    "\\newcommand{\\R}[0]{\\mat{R}}\n",
    "\\newcommand{\\Q}[0]{\\mat{Q}}\n",
    "\\newcommand{\\B}[0]{\\mat{B}}\n",
    "\\newcommand{\\C}[0]{\\mat{C}}\n",
    "\\newcommand{\\Ri}[0]{\\R^{-1}}\n",
    "\\newcommand{\\Bi}[0]{\\B^{-1}}\n",
    "\\newcommand{\\X}[0]{\\mat{X}}\n",
    "\\newcommand{\\A}[0]{\\mat{A}}\n",
    "\\newcommand{\\Y}[0]{\\mat{Y}}\n",
    "\\newcommand{\\E}[0]{\\mat{E}}\n",
    "\\newcommand{\\U}[0]{\\mat{U}}\n",
    "\\newcommand{\\V}[0]{\\mat{V}}\n",
    "%\n",
    "\\newcommand{\\x}[0]{\\bvec{x}}\n",
    "\\newcommand{\\y}[0]{\\bvec{y}}\n",
    "\\newcommand{\\z}[0]{\\bvec{z}}\n",
    "\\newcommand{\\q}[0]{\\bvec{q}}\n",
    "\\newcommand{\\br}[0]{\\bvec{r}}\n",
    "\\newcommand{\\bb}[0]{\\bvec{b}}\n",
    "%\n",
    "\\newcommand{\\bx}[0]{\\bvec{\\bar{x}}}\n",
    "\\newcommand{\\by}[0]{\\bvec{\\bar{y}}}\n",
    "\\newcommand{\\barB}[0]{\\mat{\\bar{B}}}\n",
    "\\newcommand{\\barP}[0]{\\mat{\\bar{P}}}\n",
    "\\newcommand{\\barC}[0]{\\mat{\\bar{C}}}\n",
    "\\newcommand{\\barK}[0]{\\mat{\\bar{K}}}\n",
    "%\n",
    "\\newcommand{\\D}[0]{\\mat{D}}\n",
    "\\newcommand{\\Dobs}[0]{\\mat{D}_{\\text{obs}}}\n",
    "\\newcommand{\\Dmod}[0]{\\mat{D}_{\\text{obs}}}\n",
    "%\n",
    "\\newcommand{\\ones}[0]{\\bvec{1}}\n",
    "\\newcommand{\\AN}[0]{\\big( \\I_N - \\ones \\ones\\tr / N \\big)}\n",
    "%\n",
    "% END OF MACRO DEF\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67741537",
   "metadata": {},
   "source": [
    "The previous tutorial studied the Gaussian distribution, with pdf:\n",
    "$$\\begin{align}\n",
    "\\mathcal{N}(x \\mid b, B) = (2 \\pi B)^{-1/2} e^{-(x-b)^2/2 B} \\, , \\tag{G1}\n",
    "\\end{align}$$\n",
    "and implemented it. It's also available from `scipy.stats`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d1dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_G1(x, b, B):\n",
    "    return sp.stats.norm.pdf(x, loc=b, scale=np.sqrt(B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8add4a2",
   "metadata": {},
   "source": [
    "The Gaussian distribution will help to illustrate:\n",
    "\n",
    "# Bayes' rule\n",
    "In the Bayesian approach, knowledge and uncertainty about the unknown ($x$)\n",
    "is quantified through probability.\n",
    "And **Bayes' rule** is how we do inference: it says how to condition/merge/assimilate/update this belief based on data/observation ($y$).\n",
    "For *continuous* \"random variables\", $x$ and $y$, it reads:\n",
    "\n",
    "$$\\begin{align}\n",
    "p(x|y) &= \\frac{p(x) \\, p(y|x)}{p(y)} \\, , \\tag{BR} \\\\\n",
    "\\text{i.e.} \\qquad \\text{\"posterior\" (pdf of $x$ given $y$)}\n",
    "\\; &= \\;\n",
    "\\frac{\\text{\"prior\" (pdf of $x$)}\n",
    "\\; \\times \\;\n",
    "\\text{\"likelihood\" (pdf of $y$ given $x$)}}\n",
    "{\\text{\"normalization\" (pdf of $y$)}} \\, ,\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95555a5",
   "metadata": {},
   "source": [
    "Note that, in contrast to (the frequent aim of) classical statistics, Bayes' rule in itself makes no attempt at producing only a single estimate (but the topic is briefly discussed [further below](#Exc-2.28-(optional):)). It merely states how quantitative belief (weighted possibilities) should be updated in view of new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e527c45",
   "metadata": {},
   "source": [
    "**Exc 2.10:** Derive Bayes' rule from the definition of [conditional pdf's](https://en.wikipedia.org/wiki/Conditional_probability_distribution#Conditional_continuous_distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb63318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR derivation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc12b697",
   "metadata": {},
   "source": [
    "**Exc 2.11 (optional):** Laplace called \"statistical inference\" the reasoning of \"inverse probability\" (1774). You may also have heard of \"inverse problems\" in reference to similar problems, but without a statistical framing. In view of this, why do you think we use $x$ for the unknown, and $y$ for the known/given data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c45b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('inverse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe582dc",
   "metadata": {},
   "source": [
    "Bayes' rule, eqn. (BR), involves functions (the densities), but applies for any/all values of $x$ (and $y$).\n",
    "Thus, upon discretisation, eqn. (BR) becomes the multiplication of two arrays of values,\n",
    "followed by a normalisation (explained [below](#Exc-2.14:)). It is hard to overstate how simple this principle is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a7b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bayes_rule(prior_values, lklhd_values, dx):\n",
    "    prod = prior_values * lklhd_values         # pointwise multiplication\n",
    "    posterior_values = prod/(np.sum(prod)*dx)  # normalization\n",
    "    return posterior_values\n",
    "\n",
    "bounds = -15, 15\n",
    "grid1d = np.linspace(*bounds, 201)\n",
    "dx = grid1d[1]  - grid1d[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b6c68d",
   "metadata": {},
   "source": [
    "The code below shows Bayes' rule in action. Move the sliders (use arrow keys?) to animate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad62ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ws.interact(y=(*bounds, 1),\n",
    "             R=(0.01, 20, 0.2))\n",
    "def Bayes1(y=4.0, R=1.0):\n",
    "    b = 0\n",
    "    B = 1\n",
    "    # Compute\n",
    "    x = grid1d\n",
    "    prior_vals = pdf_G1(x, b, B)\n",
    "    lklhd_vals = pdf_G1(y, x, R)\n",
    "    postr_vals = Bayes_rule(prior_vals, lklhd_vals, dx)\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(x, prior_vals, label=f'Prior, {b=:.4g}, {B=:.4g}')\n",
    "    plt.plot(x, lklhd_vals, label=f'Lklhd {y=}, {R=:.4g}')\n",
    "    plt.plot(x, postr_vals, label=f'Postr, pointwise')\n",
    "    try:\n",
    "        # See exercise below\n",
    "        xhat, P = Bayes_rule_G1(b, B, y, R)\n",
    "        label = f'Postr, parametric\\n{xhat=:.4g},{P=:.4g}'\n",
    "        postr_vals_G1 = pdf_G1(x, xhat, P)\n",
    "        plt.plot(x, postr_vals_G1, '--', label=label)\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    plt.ylim(0, 0.6)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36d610c",
   "metadata": {},
   "source": [
    "**Exc 2.12:** This exercise serves to make you acquainted with how Bayes' rule blends information.  \n",
    "Move the sliders to see what happens, and answer the following:\n",
    " * What happens to the posterior when $R \\rightarrow \\infty$ ?\n",
    " * What happens to the posterior when $R \\rightarrow 0$ ?\n",
    " * Move around $y$. What is the posterior's location (mean/mode) when $R = B$ ?\n",
    " * Can you say something universally valid (for any $y$ and $R$) about the height of the posterior pdf?\n",
    " * Does the posterior scale (width) depend on $y$?  \n",
    "   *Optional*: What does this mean [information-wise](https://en.wikipedia.org/wiki/Differential_entropy#Differential_entropies_for_various_distributions)?\n",
    " * Consider the shape (ignoring location & scale) of the posterior. Does it depend on $R$ or $y$?\n",
    " * Can you see a shortcut to computing this posterior rather than having to do the pointwise multiplication?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ab05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Posterior behaviour')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d4b52",
   "metadata": {},
   "source": [
    "#### Exc 2.14 (optional):\n",
    "Show that the normalization in `Bayes_rule()` amounts to (approximately) the same as dividing by $p(y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbec0251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d71e15",
   "metadata": {},
   "source": [
    "In fact, since $p(y)$ is thusly implicitly known,\n",
    "we often don't bother to write it down, simplifying Bayes' rule (eqn. BR) to\n",
    "$$\\begin{align}\n",
    "p(x|y) \\propto p(x) \\, p(y|x) \\, .  \\tag{BR2}\n",
    "\\end{align}$$\n",
    "Actually, do we even need to care about $p(y)$ at all? All we really need to know is how much more likely some value of $x$ (or an interval around it) is compared to any other $x$.\n",
    "The normalisation is only necessary because of the *convention* that all densities integrate to $1$.\n",
    "However, for large models, we usually can only afford to evaluate $p(y|x)$ at a few points (of $x$), so that the integral for $p(y)$ can only be roughly approximated. In such settings, estimation of the normalisation factor becomes an important question too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79bb9d3",
   "metadata": {},
   "source": [
    "**Exc 2.15:** The following implements a [uniform](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)#Moments)\n",
    "(or \"flat\" or \"box\") pdf.\n",
    "In the above animations, replace `pdf_G1` with your new `pdf_U1` (both for the prior and likelihood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b222a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_U1(x, b, B):\n",
    "    \"Univariate (scalar), Uniform pdf.\"\n",
    "    lower = b - np.sqrt(3*B)\n",
    "    upper = b + np.sqrt(3*B)\n",
    "    height = 1/(upper - lower)\n",
    "    pdfx = height * np.ones_like(x)\n",
    "    pdfx[x<lower] = 0\n",
    "    pdfx[x>upper] = 0\n",
    "    return pdfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b229e4d",
   "metadata": {},
   "source": [
    "(a) Why (in the figure) are the walls of the pdf (ever so slightly) inclined?  \n",
    "What happens when you move the prior and likelihood too far apart? Is the fault of the implementation, the math, or the problem statement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9010b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR U1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c1d809",
   "metadata": {},
   "source": [
    "(b): Re-do Exc 2.12, now with `pdf_U1`.  \n",
    "(c): Now test a Gaussian prior with a uniform likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a76780b",
   "metadata": {},
   "source": [
    "**Exc 2.15 part 2**\n",
    "Restore the use of `pdf_G1` everywhere.\n",
    "- (a) Suppose the \"observation model\" consists in squaring, i.e.\n",
    "      $y = x^2/4 + \\varepsilon$, i.e. $p(y|x) = \\NormDist(y|x^2/4, R)$, where $R$ is the variance of $\\varepsilon$. Code this into the animation code.\n",
    "- (b) Try $y = |x|$. Compare with (a).\n",
    "- (c) Try $y = 2 x$. Can you reproduce a posterior obtained with $y = x$ ?\n",
    "\n",
    "Restore $y = x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d41651",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gaussian-Gaussian Bayes\n",
    "\n",
    "The above animation shows Bayes' rule in 1 dimension. Previously, we saw how a Gaussian looks in 2 dimensions. Can you imagine how Bayes' rule looks in 2 dimensions? In higher dimensions, these things get difficult to imagine, let alone visualize.\n",
    "\n",
    "Similarly, the size of the calculations required for Bayes' rule poses a difficulty. Indeed, the following exercise shows that (pointwise) multiplication for all grid points becomes preposterous in high dimensions.\n",
    "\n",
    "**Exc 2.16 (optional):**\n",
    " * (a) How many point-multiplications are needed on a grid with $N$ points in $M$ dimensions? (Imagine an $M$-dimensional cube where each side has a grid with $N$ points on it)\n",
    " * (b) Suppose we model 15 physical quantities, on each grid point, on a discretized surface model of Earth. Assume the resolution is $1^\\circ$ for latitude (110km), $1^\\circ$ for longitude. How many variables are there in total? This is the dimensionality ($M$) of the problem.\n",
    " * (c) Suppose each variable is has a pdf represented with a grid using only $N=20$ points. How many multiplications are necessary to calculate Bayes rule (jointly) for all variables on our Earth model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bdf66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Dimensionality a')\n",
    "# ws.show_answer('Dimensionality b')\n",
    "# ws.show_answer('Dimensionality c')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6753544",
   "metadata": {},
   "source": [
    "In response to this computational difficulty, we try to be smart and do something more analytical (\"pen-and-paper\"): we only compute the parameters (mean and (co)variance) of the posterior pdf.\n",
    "\n",
    "This is doable and quite simple in the Gaussian-Gaussian case:  \n",
    "With a prior $p(x) = \\mathcal{N}(x \\mid b,B)$ and  \n",
    "a likelihood $p(y|x) = \\mathcal{N}(y \\mid x,R)$,\n",
    "the posterior is\n",
    "$$\\begin{align}\n",
    "p(x|y)\n",
    "&= \\mathcal{N}(x \\mid \\hat{x},P) \\tag{4} \\, ,\n",
    "\\end{align}$$\n",
    "where, in the univariate (1-dimensional) case:\n",
    "$$\\begin{align}\n",
    "    P &= 1/(1/B + 1/R) \\, , \\tag{5} \\\\\\\n",
    "  \\hat{x} &= P(b/B + y/R) \\, .  \\tag{6}\n",
    "\\end{align}$$\n",
    "\n",
    "The multivariate case is discussed in a later tutorial.\n",
    "\n",
    "**Exc 2.17:**\n",
    "The statement $x = \\mu \\pm \\sigma$ is *sometimes* used\n",
    "as a shorthand for $p(x) = \\mathcal{N}(x \\mid \\mu, \\sigma^2)$. Suppose\n",
    "- you think the temperature $x = 20Â°C \\pm 2Â°C$,\n",
    "- a thermometer yields the observation $y = 18Â°C \\pm 2Â°C$.\n",
    "- Show that your posterior is $p(x|y) = \\mathcal{N}(x \\mid 19, 2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29eda29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('GG BR example')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b0735f",
   "metadata": {},
   "source": [
    "#### Exc  2.18 'Gaussian Bayes':\n",
    "Derive the above expressions for $P$ and $\\hat{x}$\n",
    "from Bayes' rule (BR2) and the expression for a Gaussian pdf (G1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f4c49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR Gauss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6717a439",
   "metadata": {},
   "source": [
    "**Exc 2.20:** Algebra exercise: Show that eqn. (5) can be written as\n",
    "$$P = K R \\,,    \\tag{8}$$\n",
    "where\n",
    "$$K = B/(B+R) \\,,    \\tag{9}$$\n",
    "is called the \"Kalman gain\".  \n",
    "Then shown that eqns (5) and (6) can be written as\n",
    "$$\\begin{align}\n",
    "    P &= (1-K)B \\, ,  \\tag{10} \\\\\\\n",
    "  \\hat{x} &= b + K (y-b) \\tag{11} \\, ,\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce20dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR Kalman1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34803dbd",
   "metadata": {},
   "source": [
    "**Exc 2.22 (optional):** Consider the formula for $K$ and its role in the previous couple of equations. Why do you think $K$ is called a \"gain\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('KG intuition')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b894e9ac",
   "metadata": {},
   "source": [
    "**Exc 2.24:** Implement a Gaussian-Gaussian Bayes' rule (eqns 5 and 6, or 9-11) by completing the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af85dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bayes_rule_G1(b, B, y, R):\n",
    "    ### INSERT ANSWER HERE ###\n",
    "    return xhat, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc66c164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR Gauss code')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9903c366",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "**Exc 2.26:** Go back to the above animation code cell. Restore `pdf_G1` (both for prior and likelihood). Run/execute.\n",
    "- What is the relationship between the two posterior curves?  \n",
    "  *Hint: This is the main secret of the \"Kalman filter\".*\n",
    "- Now use `_U1` instead of `_G1` to compute `prior_vals` or `lklhd_vals` or both.  \n",
    "  Does `Bayes_rule_G1()` provide a good approximation to `Bayes_rule()`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369cec84",
   "metadata": {},
   "source": [
    "#### Exc 2.28 (optional):\n",
    "If you need to pick a single point value for your estimate (for example, an action to be taken), you can **decide** on it by optimising (with respect to the estimate) the expected value of some utility/loss function [[ref](https://en.wikipedia.org/wiki/Bayes_estimator)]. For example, if the density of $X$ is symmetric,\n",
    "   and $\\text{Loss}$ is convex and symmetric,\n",
    "   then $\\Expect[\\text{Loss}(X - \\theta)]$ is minimized\n",
    "   by the mean, $\\Expect[X]$, which also coincides with the median.\n",
    "   <!-- See Corollary 7.19 of Lehmann, Casella -->\n",
    "For the expected *squared* loss, $\\Expect[(X - \\theta)^2]$,\n",
    "the minimum is the mean for *any distribution*.\n",
    "Show the latter result.  \n",
    "*Hint: insert $0 = \\,?\\, - \\,?$.*\n",
    "\n",
    "In summary, the intuitive idea of **considering the mean of $p(x)$ as the point estimate** has good theoretical foundations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666b3b6a",
   "metadata": {},
   "source": [
    "## Multivariate illlustration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee2b481",
   "metadata": {},
   "source": [
    "Unlike previous tutorial, which implemented the Gaussian pdf,\n",
    "we here take it from `scipy.stats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26fe828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "def pdf_GM(points, mu, Sigma):\n",
    "    diff = points - mu  # enable broadcasting of *mean*\n",
    "    dims = len(Sigma)\n",
    "    return multivariate_normal(np.zeros(dims), Sigma).pdf(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c03cb0c",
   "metadata": {},
   "source": [
    "Notice that we're re-using the very same `Bayes_rule` as in the 1D case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5b9d46",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid2d = np.dstack(np.meshgrid(grid1d, grid1d))\n",
    "\n",
    "@ws.interact(corr=(-0.999, 0.999, .01),\n",
    "             y1=bounds,\n",
    "             y2=bounds,\n",
    "             R1=(0.01, 36, 0.2),\n",
    "             R2=(0.01, 36, 0.2),\n",
    "             )\n",
    "def Bayes2(corr=.3, y1=3, y2=-12, R1=4**2, R2=1, y1_only=False):\n",
    "    x = grid2d\n",
    "\n",
    "    mu = np.zeros(2)\n",
    "    C = 25 * np.array([[1, corr],\n",
    "                       [corr, 1]])\n",
    "\n",
    "    y = np.array([y1, y2])\n",
    "    R = np.diag([R1, R2])\n",
    "    Hx = x\n",
    "    #Hx = x**2/4\n",
    "    #Hx = x**3/36\n",
    "    \n",
    "    #Hx = x[..., :1] * x[..., 1:2]\n",
    "    #y1_only = True\n",
    "\n",
    "    if y1_only:\n",
    "        y = y[:1]\n",
    "        R = R[:1, :1]\n",
    "        Hx = Hx[..., :1]\n",
    "\n",
    "    lklhd = pdf_GM(y, Hx, R)\n",
    "    prior = pdf_GM(x, mu, C)\n",
    "    postr = Bayes_rule(prior, lklhd, dx**2)\n",
    "\n",
    "    ax, plot = ws.get_jointplotter(grid1d)\n",
    "    contours = [plot(prior, 'royalblue'),\n",
    "                plot(lklhd, 'orange'),\n",
    "                plot(postr, 'green')]\n",
    "    ax.legend(contours, ['prior', 'lklhd', 'postr'], loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eda57e4",
   "metadata": {},
   "source": [
    "**Exc:** Try different `H`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea1e53b",
   "metadata": {},
   "source": [
    "It will not surprise you to learn that the shape of the posterior is again Gaussian,\n",
    "essentially for the same reason as in 1D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e02e31",
   "metadata": {},
   "source": [
    "### Next: [Univariate (scalar) Kalman filtering](T3%20-%20Univariate%20Kalman%20filtering.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
