{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33a6a71d",
   "metadata": {},
   "source": [
    "# T3 - Bayesian inference\n",
    "Now that we have reviewed some probability, we can look at statistical inference.\n",
    "$\n",
    "% Loading TeX (MathJax)... Please wait\n",
    "%\n",
    "\\newcommand{\\Reals}{\\mathbb{R}}\n",
    "\\newcommand{\\Expect}[0]{\\mathbb{E}}\n",
    "\\newcommand{\\NormDist}{\\mathcal{N}}\n",
    "%\n",
    "\\newcommand{\\DynMod}[0]{\\mathscr{M}}\n",
    "\\newcommand{\\ObsMod}[0]{\\mathscr{H}}\n",
    "%\n",
    "\\newcommand{\\mat}[1]{{\\mathbf{{#1}}}}\n",
    "%\\newcommand{\\mat}[1]{{\\pmb{\\mathsf{#1}}}}\n",
    "\\newcommand{\\bvec}[1]{{\\mathbf{#1}}}\n",
    "%\n",
    "\\newcommand{\\trsign}{{\\mathsf{T}}}\n",
    "\\newcommand{\\tr}{^{\\trsign}}\n",
    "\\newcommand{\\tn}[1]{#1}\n",
    "\\newcommand{\\ceq}[0]{\\mathrel{≔}}\n",
    "%\n",
    "\\newcommand{\\I}[0]{\\mat{I}}\n",
    "\\newcommand{\\K}[0]{\\mat{K}}\n",
    "\\newcommand{\\bP}[0]{\\mat{P}}\n",
    "\\newcommand{\\bH}[0]{\\mat{H}}\n",
    "\\newcommand{\\bF}[0]{\\mat{F}}\n",
    "\\newcommand{\\R}[0]{\\mat{R}}\n",
    "\\newcommand{\\Q}[0]{\\mat{Q}}\n",
    "\\newcommand{\\B}[0]{\\mat{B}}\n",
    "\\newcommand{\\C}[0]{\\mat{C}}\n",
    "\\newcommand{\\Ri}[0]{\\R^{-1}}\n",
    "\\newcommand{\\Bi}[0]{\\B^{-1}}\n",
    "\\newcommand{\\X}[0]{\\mat{X}}\n",
    "\\newcommand{\\A}[0]{\\mat{A}}\n",
    "\\newcommand{\\Y}[0]{\\mat{Y}}\n",
    "\\newcommand{\\E}[0]{\\mat{E}}\n",
    "\\newcommand{\\U}[0]{\\mat{U}}\n",
    "\\newcommand{\\V}[0]{\\mat{V}}\n",
    "%\n",
    "\\newcommand{\\x}[0]{\\bvec{x}}\n",
    "\\newcommand{\\y}[0]{\\bvec{y}}\n",
    "\\newcommand{\\z}[0]{\\bvec{z}}\n",
    "\\newcommand{\\q}[0]{\\bvec{q}}\n",
    "\\newcommand{\\br}[0]{\\bvec{r}}\n",
    "\\newcommand{\\bb}[0]{\\bvec{b}}\n",
    "%\n",
    "\\newcommand{\\bx}[0]{\\bvec{\\bar{x}}}\n",
    "\\newcommand{\\by}[0]{\\bvec{\\bar{y}}}\n",
    "\\newcommand{\\barB}[0]{\\mat{\\bar{B}}}\n",
    "\\newcommand{\\barP}[0]{\\mat{\\bar{P}}}\n",
    "\\newcommand{\\barC}[0]{\\mat{\\bar{C}}}\n",
    "\\newcommand{\\barK}[0]{\\mat{\\bar{K}}}\n",
    "%\n",
    "\\newcommand{\\D}[0]{\\mat{D}}\n",
    "\\newcommand{\\Dobs}[0]{\\mat{D}_{\\text{obs}}}\n",
    "\\newcommand{\\Dmod}[0]{\\mat{D}_{\\text{obs}}}\n",
    "%\n",
    "\\newcommand{\\ones}[0]{\\bvec{1}}\n",
    "\\newcommand{\\AN}[0]{\\big( \\I_N - \\ones \\ones\\tr / N \\big)}\n",
    "%\n",
    "% END OF MACRO DEF\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b268717",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import resources.workspace as ws\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67741537",
   "metadata": {},
   "source": [
    "The [previous tutorial](T2%20-%20Gaussian%20distribution.ipynb) studied the Gaussian probability density function (pdf), defined by:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathcal{N}(x \\mid \\mu, \\sigma^2) = (2 \\pi \\sigma^2)^{-1/2} e^{-(x-\\mu)^2/2 \\sigma^2} \\, , \\tag{G1}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d1dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_G1(x, meanval, variance):\n",
    "    return sp.stats.norm.pdf(x, loc=meanval, scale=np.sqrt(variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4594d7c9",
   "metadata": {},
   "source": [
    "The following implements the the [uniform](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous))\n",
    "(or \"flat\" or \"box\") pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7633c2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_U1(x, meanval, variance):\n",
    "    lower = meanval - np.sqrt(3*variance)\n",
    "    upper = meanval + np.sqrt(3*variance)\n",
    "    # pdfx = scipy.stats.uniform(loc=lower, scale=(upper-lower)).pdf(x)\n",
    "    height = 1/(upper - lower)\n",
    "    pdfx = height * np.ones_like(x)\n",
    "    pdfx[x<lower] = 0\n",
    "    pdfx[x>upper] = 0\n",
    "    return pdfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8add4a2",
   "metadata": {},
   "source": [
    "These distributions will help illustrate:\n",
    "\n",
    "# Bayes' rule\n",
    "In the Bayesian approach, knowledge and uncertainty about the unknown ($x$)\n",
    "is quantified through probability.\n",
    "And **Bayes' rule** is how we do inference: it says how to condition/merge/assimilate/update this belief based on data/observation ($y$).\n",
    "For *continuous* \"random variables\", $x$ and $y$, it reads:\n",
    "\n",
    "$$\\begin{align}\n",
    "p(x|y) &= \\frac{p(x) \\, p(y|x)}{p(y)} \\, , \\tag{BR} \\\\[1em]\n",
    "\\text{i.e.} \\qquad \\texttt{posterior}\\,\\text{[pdf of $x$ given $y$]}\n",
    "\\; &= \\;\n",
    "\\frac{\\texttt{prior}\\,\\text{[pdf of $x$]}\n",
    "\\; \\times \\;\n",
    "\\texttt{likelihood}\\,\\text{[pdf of $y$ given $x$]}}\n",
    "{\\texttt{normalisation}\\,\\text{[pdf of $y$]}} \\, ,\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95555a5",
   "metadata": {},
   "source": [
    "Note that, in contrast to (the frequent aim of) classical statistics, Bayes' rule in itself makes no attempt at producing only a single estimate (but the topic is briefly discussed [further below](#Exc-2.28-(optional):)). It merely states how quantitative belief (weighted possibilities) should be updated in view of new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e527c45",
   "metadata": {},
   "source": [
    "**Exc 2.10:** Derive Bayes' rule from the definition of [conditional pdf's](https://en.wikipedia.org/wiki/Conditional_probability_distribution#Conditional_continuous_distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb63318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR derivation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc12b697",
   "metadata": {},
   "source": [
    "**Exc 2.11 (optional):** Laplace called \"statistical inference\" the reasoning of \"inverse probability\" (1774). You may also have heard of \"inverse problems\" in reference to similar problems, but without a statistical framing. In view of this, why do you think we use $x$ for the unknown, and $y$ for the known/given data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c45b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('inverse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe582dc",
   "metadata": {},
   "source": [
    "Bayes' rule, eqn. (BR), involves functions (the densities), but applies for any/all values of $x$ (and $y$).\n",
    "Thus, upon discretisation, eqn. (BR) becomes the multiplication of two arrays of values,\n",
    "followed by a normalisation (explained [below](#Exc-2.14:)). It is hard to overstate how simple this principle is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a7b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bayes_rule(prior_values, lklhd_values, dx):\n",
    "    prod = prior_values * lklhd_values         # pointwise multiplication\n",
    "    posterior_values = prod/(np.sum(prod)*dx)  # normalization\n",
    "    return posterior_values\n",
    "\n",
    "bounds = -15, 15\n",
    "grid1d = np.linspace(*bounds, 201)\n",
    "dx = grid1d[1]  - grid1d[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b6c68d",
   "metadata": {},
   "source": [
    "The code below shows Bayes' rule in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad62ea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@ws.interact(y=(*bounds, 1),\n",
    "             R=(0.01, 20, 0.2),\n",
    "             top=['y', 'R'])\n",
    "def Bayes1(y=4.0, R=1.0,\n",
    "           prior_is_G=True,\n",
    "           lklhd_is_G=True):\n",
    "    xf = 0\n",
    "    Pf = 1\n",
    "    x = grid1d\n",
    "\n",
    "    prior_vals = pdf_G1(x, xf, Pf) if prior_is_G else pdf_U1(x, xf, Pf)\n",
    "    lklhd_vals = pdf_G1(y, x, R)   if lklhd_is_G else pdf_U1(y, x, R)\n",
    "    postr_vals = Bayes_rule(prior_vals, lklhd_vals, dx)\n",
    "\n",
    "    def plot(x, y, c, lbl):\n",
    "        plt.fill_between(x, y, color=c, alpha=.3, label=lbl)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plot(x, prior_vals, 'blue'  , f'Prior, N(x | {xf:.4g}, {Pf:.4g})')\n",
    "    plot(x, lklhd_vals, 'green' , f'Lklhd, N({y} | x, {R:.4g})')\n",
    "    plot(x, postr_vals, 'red'   , f'Postr, pointwise')\n",
    "\n",
    "    try:\n",
    "        # See exercise below\n",
    "        xa, Pa = Bayes_rule_G1(xf, Pf, y, R)\n",
    "        label = f'Postr, parametric\\nN(x | {xa:.4g}, {Pa:.4g})'\n",
    "        postr_vals_G1 = pdf_G1(x, xa, Pa)\n",
    "        plt.plot(x, postr_vals_G1, 'purple', label=label)\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    plt.ylim(0, 0.6)\n",
    "    plt.legend(loc=\"upper right\", prop={'family': 'monospace'})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36d610c",
   "metadata": {},
   "source": [
    "**Exc 2.12:** This exercise serves to make you acquainted with how Bayes' rule blends information.  \n",
    " Move the sliders (use arrow keys?) to animate it, and answer the following (with the boolean checkmarks both on and off).\n",
    " * What happens to the posterior when $R \\rightarrow \\infty$ ?\n",
    " * What happens to the posterior when $R \\rightarrow 0$ ?\n",
    " * Move $y$ around. What is the posterior's location (mean/mode) when $R$ equals the prior variance?\n",
    " * Can you say something universally valid (for any $y$ and $R$) about the height of the posterior pdf?\n",
    " * Does the posterior scale (width) depend on $y$?  \n",
    "   *Optional*: What does this mean [information-wise](https://en.wikipedia.org/wiki/Differential_entropy#Differential_entropies_for_various_distributions)?\n",
    " * Consider the shape (ignoring location & scale) of the posterior. Does it depend on $R$ or $y$?\n",
    " * Can you see a shortcut to computing this posterior rather than having to do the pointwise multiplication?\n",
    " * For the case of two uniform distributions: What happens when you move the prior and likelihood too far apart? Is the fault of the implementation, the math, or the problem statement?\n",
    " * Play around with the grid resolution (see the cell above). What is in your opinion a \"sufficient\" grid resolution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ab05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Posterior behaviour')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d4b52",
   "metadata": {},
   "source": [
    "#### Exc 2.14 (optional):\n",
    "Show that the normalization in `Bayes_rule()` amounts to (approximately) the same as dividing by $p(y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbec0251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d71e15",
   "metadata": {},
   "source": [
    "In fact, since $p(y)$ is thusly implicitly known,\n",
    "we often don't bother to write it down, simplifying Bayes' rule (eqn. BR) to\n",
    "$$\\begin{align}\n",
    "p(x|y) \\propto p(x) \\, p(y|x) \\, .  \\tag{BR2}\n",
    "\\end{align}$$\n",
    "Actually, do we even need to care about $p(y)$ at all? All we really need to know is how much more likely some value of $x$ (or an interval around it) is compared to any other $x$.\n",
    "The normalisation is only necessary because of the *convention* that all densities integrate to $1$.\n",
    "However, for large models, we usually can only afford to evaluate $p(y|x)$ at a few points (of $x$), so that the integral for $p(y)$ can only be roughly approximated. In such settings, estimation of the normalisation factor becomes an important question too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a76780b",
   "metadata": {},
   "source": [
    "#### Exc 2.15 'Nonlinear regression':\n",
    "- (a) Suppose the \"observation model\" consists in squaring, i.e.\n",
    "      $y = x^2/4 + \\varepsilon$, i.e. $p(y|x) = \\NormDist(y|x^2/4, R)$, where $R$ is the variance of $\\varepsilon$. Implement this in the above interactive animation code.\n",
    "- (b) Try $y = |x|$. Compare with (a).\n",
    "- (c) Try $y = x + 3$. Describe the impact.\n",
    "- (d) Try $y = 2 x$. Can you reproduce a posterior obtained with $y = x$ ?\n",
    "\n",
    "Restore $y = x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d41651",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gaussian-Gaussian Bayes\n",
    "\n",
    "The above animation shows Bayes' rule in 1 dimension. Previously, we saw how a Gaussian looks in 2 dimensions. Can you imagine how Bayes' rule looks in 2 dimensions (we'll see in [T5](T5%20-%20Kalman%20filter%20(multivariate).ipynb))? In higher dimensions ($D_x \\gg 1$), these things get difficult to imagine, let alone visualize. Similarly, the size of the problem becomes a computational difficulty.\n",
    "\n",
    "**Exc 2.16 (optional):**\n",
    " * (a) How many point-multiplications are needed on a grid with $N$ points in $D_x$ dimensions? Imagine an $D_x$-dimensional cube where each side has a grid with $N$ points on it.\n",
    " * *PS: Of course, if the likelihood contains an actual model $\\mathcal{H}(x)$ as well, its evaluations (computations) could be significantly more costly than the point-multiplications of Bayes' rule itself.*\n",
    " * (b) Suppose we model 15 physical quantities (fields), at each grid node, on a discretized surface model of Earth. Assume the resolution is $1^\\circ$ for latitude (110km), $1^\\circ$ for longitude. How many variables, $D_x$, are there in total? This is the ***dimensionality*** of the unknown.\n",
    " * (c) Suppose each variable is has a pdf represented with a grid using only $N=20$ points. How many multiplications are necessary to calculate Bayes rule (jointly) for all variables on our Earth model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bdf66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Dimensionality', 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf29f7d",
   "metadata": {},
   "source": [
    "In response to this computational difficulty, we try to be smart and do something more analytical (\"pen-and-paper\"): we only compute the parameters (mean and (co)variance) of the posterior pdf.\n",
    "\n",
    "This is doable and quite simple in the Gaussian-Gaussian case:  \n",
    "- With a prior $p(x) = \\mathcal{N}(x \\mid x^\\text{f}, P^\\text{f})$ and  \n",
    "- a likelihood $p(y|x) = \\mathcal{N}(y \\mid x,R)$,  \n",
    "- the posterior is\n",
    "$\n",
    "p(x|y)\n",
    "= \\mathcal{N}(x \\mid x^\\text{a}, P^\\text{a}) \\,,\n",
    "$\n",
    "where, in the 1-dimensional/univariate/scalar (multivariate is discussed in [T5](T5%20-%20Kalman%20filter%20(multivariate).ipynb)) case:\n",
    "\n",
    "$$\\begin{align}\n",
    "    P^\\text{a} &= 1/(1/P^\\text{f} + 1/R) \\, , \\tag{5} \\\\\\\n",
    "  x^\\text{a} &= P^\\text{a} (x^\\text{f}/P^\\text{f} + y/R) \\, .  \\tag{6}\n",
    "\\end{align}$$\n",
    "\n",
    "*There are a lot of sub/super-scripts. Please take a moment to somewhat digest the formulae.*\n",
    "\n",
    "#### Exc  2.18 'Gaussian-Gaussian Bayes':\n",
    "Consider the following identity, where $P^\\text{a}$ and $x^\\text{a}$ are given by eqns. (5) and (6).\n",
    "$$\\frac{(x-x^\\text{f})^2}{P^\\text{f}} + \\frac{(x-y)^2}{R} \\quad=\\quad \\frac{(x - x^\\text{a})^2}{P^\\text{a}} + \\frac{(y - x^\\text{f})^2}{P^\\text{f} + R} \\,, \\tag{S2}$$\n",
    "Notice that the left hand side (LHS) is the sum of two squares with $x$,\n",
    "but the RHS only contains one square with $x$.\n",
    "- (a) Derive the first term of the RHS, i.e. eqns. (5) and (6).\n",
    "- (b) *Optional*: Derive the full RHS (i.e. also the second term).\n",
    "- (c) Derive $p(x|y) = \\mathcal{N}(x \\mid x^\\text{a}, P^\\text{a})$ from eqns. (5) and (6)\n",
    "  using part (a), Bayes' rule (BR2), and the Gaussian pdf (G1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90eb909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answers.show_answer('BR Gauss, a.k.a. completing the square', 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3527be59",
   "metadata": {},
   "source": [
    "**Exc 2.19:**\n",
    "The statement $x = \\mu \\pm \\sigma$ is *sometimes* used\n",
    "as a shorthand for $p(x) = \\mathcal{N}(x \\mid \\mu, \\sigma^2)$. Suppose\n",
    "- you think the temperature $x = 20°C \\pm 2°C$,\n",
    "- a thermometer yields the observation $y = 18°C \\pm 2°C$.\n",
    "\n",
    "Show that your posterior is $p(x|y) = \\mathcal{N}(x \\mid 19, 2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab02ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('GG BR example')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0925f3",
   "metadata": {},
   "source": [
    "The following implements a Gaussian-Gaussian Bayes' rule (eqns 5 and 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd52539",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def Bayes_rule_G1(xf, Pf, y, R):\n",
    "    Pa = 1 / (1/Pf + 1/R)\n",
    "    xa = Pa * (xf/Pf + y/R)\n",
    "    return xa, Pa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b342a",
   "metadata": {},
   "source": [
    "**Re-run**/execute the interactive animation code cell up above.\n",
    "*Note that the inputs and outputs for `Bayes_rule_G1()` are not discretised density values (as for `Bayes_rule()`), but simply 2 numbers: the mean and the variance.*\n",
    "\n",
    "#### Exc 2.20:\n",
    "- (a) Under what conditions does `Bayes_rule_G1()` provide a good approximation to `Bayes_rule()`?\n",
    "- (b) *Optional*. Try using one or more of the other [distributions readily available in `scipy`](https://stackoverflow.com/questions/37559470/) in the above animation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6717a439",
   "metadata": {},
   "source": [
    "**Exc 2.22:** Algebra exercise: Show that eqn. (5) can be written as\n",
    "$$P^\\text{a} = K R \\,,    \\tag{8}$$\n",
    "where\n",
    "$$K = P^\\text{f}/(P^\\text{f}+R) \\,,    \\tag{9}$$\n",
    "is called the \"Kalman gain\".  \n",
    "Then shown that eqns (5) and (6) can be written as\n",
    "$$\\begin{align}\n",
    "    P^\\text{a} &= (1-K) P^\\text{f} \\, ,  \\tag{10} \\\\\\\n",
    "  x^\\text{a} &= x^\\text{f} + K (y-x^\\text{f}) \\tag{11} \\, ,\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce20dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR Kalman1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34803dbd",
   "metadata": {},
   "source": [
    "**Exc 2.24 (optional):**\n",
    "- (a) Show that $0 < K < 1$ since $0 < P^\\text{f}, R$.\n",
    "- (b) Show that $P^\\text{a} < P^\\text{f}, R$.\n",
    "- (c) Show that $x^\\text{a} \\in (x^\\text{f}, y)$.\n",
    "- (d) Why do you think $K$ is called a \"gain\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('KG intuition')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbec053",
   "metadata": {},
   "source": [
    "**Exc 2.26:** Re-define `Bayes_rule_G1` so to as to use eqns. 9-11. Remember to re-run the cell. Verify that you get the same plots as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc66c164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('BR Kalman1 code')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369cec84",
   "metadata": {},
   "source": [
    "#### Exc 2.28 (optional):\n",
    "*If you must* pick a single point value for your estimate (for example, an action to be taken), you can **decide** on it by optimising (with respect to the estimate) the expected value of some utility/loss function [[ref](https://en.wikipedia.org/wiki/Bayes_estimator)]. For example, if the density of $X$ is symmetric,\n",
    "   and $\\text{Loss}$ is convex and symmetric,\n",
    "   then $\\Expect[\\text{Loss}(X - \\theta)]$ is minimized\n",
    "   by the mean, $\\Expect[X]$, which also coincides with the median.\n",
    "   <!-- See Corollary 7.19 of Lehmann, Casella -->\n",
    "For the expected *squared* loss, $\\Expect[(X - \\theta)^2]$,\n",
    "the minimum is the mean for *any distribution*.\n",
    "Show the latter result.  \n",
    "*Hint: insert $0 = \\,?\\, - \\,?$.*\n",
    "\n",
    "In summary, the intuitive idea of **considering the mean of $p(x)$ as the point estimate** has good theoretical foundations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e02e31",
   "metadata": {},
   "source": [
    "### Next: [T4 - Filtering & time series](T4%20-%20Filtering%20%26%20time%20series.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
