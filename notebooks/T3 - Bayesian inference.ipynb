{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a39d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote = \"https://raw.githubusercontent.com/nansencenter/DA-tutorials\"\n",
    "!wget -qO- {remote}/master/notebooks/resources/colab_bootstrap.sh | bash -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b268717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources import show_answer, interact, import_from_nb, get_jointplotter\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec2093d",
   "metadata": {},
   "source": [
    "# T3 - Bayesian inference\n",
    "\n",
    "The [previous tutorial](T2%20-%20Gaussian%20distribution.ipynb)\n",
    "$\n",
    "\\newcommand{\\Expect}[0]{\\mathbb{E}}\n",
    "\\newcommand{\\NormDist}{\\mathscr{N}}\n",
    "\\newcommand{\\ObsMod}[0]{\\mathscr{H}}\n",
    "\\newcommand{\\mat}[1]{{\\mathbf{{#1}}}}\n",
    "\\newcommand{\\vect}[1]{{\\mathbf{#1}}}\n",
    "\\newcommand{\\ta}[0]{\\text{a}}\n",
    "\\newcommand{\\tf}[0]{\\text{f}}\n",
    "$\n",
    "studied the Gaussian probability density function (pdf), defined in 1D by:\n",
    "$$ \\large \\NormDist(x \\mid \\mu, \\sigma^2) = (2 \\pi \\sigma^2)^{-1/2} e^{-(x-\\mu)^2/2 \\sigma^2} \\,,\\tag{G1} $$\n",
    "which we implemented and tested alongside the uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d1dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(pdf_G1, pdf_U1, bounds, dx, grid1d, mean_and_var) = import_from_nb(\"T2\", (\"pdf_G1\", \"pdf_U1\", \"bounds\", \"dx\", \"grid1d\", \"mean_and_var\"))\n",
    "pdfs = dict(N=pdf_G1, U=pdf_U1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8add4a2",
   "metadata": {},
   "source": [
    "*We no longer use uppercase to distinguish random variables from their outcomes (an unfortunate consequence of the myriad of notations to keep track of)!*\n",
    "\n",
    "Now that we have reviewed some probability, we can turn to statistical inference and estimation. In particular, we will focus on\n",
    "\n",
    "# Bayes' rule\n",
    "\n",
    "<details style=\"border: 1px solid #aaaaaa; border-radius: 4px; padding: 0.5em 0.5em 0;\">\n",
    "  <summary style=\"font-weight: normal; font-style: italic; margin: -0.5em -0.5em 0; padding: 0.5em;\">\n",
    "    In the Bayesian approach, uncertain knowledge (i.e. belief) about some unknown ($x$) is quantified through probability ... (optional reading üîç)\n",
    "  </summary>\n",
    "\n",
    "  For example, what is the temperature at the surface of Mercury (at some given point and time)?\n",
    "  Not many people know the answer. Perhaps you say $500^{\\circ} C, \\, \\pm \\, 20$.\n",
    "  But that's hardly anything compared to you real uncertainty, so you revise that to $\\pm \\, 1000$.\n",
    "  But then you're allowing for temperature below absolute zero, which you of course don't believe is possible.\n",
    "  You can continue to refine the description of your uncertainty.\n",
    "  Ultimately (in the limit) the complete way to express your belief is as a *distribution*\n",
    "  (essentially just a list) of plausibilities for all possibilities.\n",
    "  Furthermore, the only coherent way to reason in the presence of such uncertainty\n",
    "  is to obey the laws of probability [[Jaynes (2003)](#References)].\n",
    "\n",
    "  - - -\n",
    "</details>\n",
    "\n",
    "**Bayes' rule** is the fundamental tool for inference: it tells us how to condition/merge/assimilate/update our beliefs based on data/observation ($y$).\n",
    "For *continuous* random variables $x$ and $y$, Bayes' rule reads:\n",
    "$$\n",
    "\\large\n",
    "\\color{red}{\\overset{\\mbox{Posterior}}{p(\\color{black}{x|y})}} = \\frac{\\color{blue}{\\overset{\\mbox{  Prior  }}{p(\\color{black}{x})}} \\, \\color{green}{\\overset{\\mbox{ Likelihood}}{p(\\color{black}{y|x})}}}{\\color{gray}{\\underset{\\mbox{Constant wrt. x}}{p(\\color{black}{y})}}} \\,. \\tag{BR} \\\\[1em]\n",
    "$$\n",
    "\n",
    "**Exc ‚Äì Bayes' rule derivation:** Derive eqn. (BR) from the definition of [conditional pdfs](https://en.wikipedia.org/wiki/Conditional_probability_distribution#Conditional_continuous_distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb63318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('symmetry of conjunction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe582dc",
   "metadata": {},
   "source": [
    "It is hard to overstate the simplicity of Bayes' rule, eqn. (BR): it consists merely of scalar multiplication and division.\n",
    "However, our goal is to compute the function $p(x|y)$ for **all values of $x$**.\n",
    "Thus, upon discretization, eqn. (BR) becomes the multiplication of two *arrays* of values (followed by normalization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a7b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bayes_rule(prior_values, lklhd_values, dx):\n",
    "    prod = prior_values * lklhd_values         # pointwise multiplication\n",
    "    posterior_values = prod/(np.sum(prod)*dx)  # normalization\n",
    "    return posterior_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a814ed05",
   "metadata": {},
   "source": [
    "#### Exc (optional) ‚Äì BR normalization\n",
    "\n",
    "Show that the normalization in `Bayes_rule()` amounts to (approximately) the same as dividing by $p(y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c28fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('quadrature marginalisation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad362aca",
   "metadata": {},
   "source": [
    "In fact, since $p(y)$ is implicitly known in this way,\n",
    "we often omit it, simplifying Bayes' rule (eqn. BR) to\n",
    "\n",
    "$$ p(x|y) \\propto p(x) \\, p(y|x) \\,.  \\tag{BR2} $$\n",
    "\n",
    "Do we even need to care about $p(y)$ at all? In practice, all we need to know is how much more likely one value of $x$ (or an interval around it) is compared to another.\n",
    "Normalization is only necessary because of the *convention* that all densities integrate to $1$.\n",
    "However, for large models, we can usually only afford to evaluate $p(y|x)$ at a few points (of $x$), so the integral for $p(y)$ can only be roughly approximated. In such settings, estimating the normalization factor becomes an important question too.\n",
    "\n",
    "<a name=\"Interactive-illustration\"></a>\n",
    "\n",
    "## Interactive illustration\n",
    "\n",
    "The code below shows Bayes' rule in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad62ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(y=(*bounds, 1), logR=(-3, 5, .5), prior_kind=list(pdfs), lklhd_kind=list(pdfs))\n",
    "def Bayes1(y=9.0, logR=1.0, lklhd_kind=\"N\", prior_kind=\"N\"):\n",
    "    R = 4**logR\n",
    "    xf = 10\n",
    "    Pf = 4**2\n",
    "\n",
    "    # (Ref. later exercise)\n",
    "    def H(x):\n",
    "        return 1*x + 0\n",
    "\n",
    "    x = grid1d\n",
    "    prior_vals = pdfs[prior_kind](x, xf, Pf)\n",
    "    lklhd_vals = pdfs[lklhd_kind](y, H(x), R)\n",
    "    postr_vals = Bayes_rule(prior_vals, lklhd_vals, dx)\n",
    "\n",
    "    def plot(x, y, c, lbl):\n",
    "        plt.fill_between(x, y, color=c, alpha=.3, label=lbl)\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plot(x, prior_vals, 'blue'  , f'Prior, {prior_kind}(x | {xf:.4g}, {Pf:.4g})')\n",
    "    plot(x, lklhd_vals, 'green' , f'Lklhd, {lklhd_kind}({y:<3}| H(x), {R:.4g})')\n",
    "    plot(x, postr_vals, 'red'   , 'Postr, pointwise: ?(x | %4.2g, %4.2g)' % mean_and_var(postr_vals, x))\n",
    "\n",
    "    # (Ref. later exercise)\n",
    "    try:\n",
    "        H_lin = H(xf)/xf # a simple linear approximation of H(x)\n",
    "        xa, Pa = Bayes_rule_LG1(xf, Pf, y, H_lin, R)\n",
    "        label = f'Postr, parametric: N(x | {xa:4.2g}, {Pa:4.2g})'\n",
    "        postr_vals_G1 = pdf_G1(x, xa, Pa)\n",
    "        plt.plot(x, postr_vals_G1, 'purple', label=label)\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    plt.ylim(0, 0.6)\n",
    "    plt.legend(loc=\"upper left\", prop={'family': 'monospace'})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36d610c",
   "metadata": {},
   "source": [
    "The illustration uses a\n",
    "\n",
    "- prior $p(x) = \\NormDist(x|x^f, P^f)$ with (fixed) mean and variance, $x^f= 10$, $P^f=4^2$\n",
    "  (but you can of course change these in the code above)\n",
    "- likelihood $p(y|x) = \\NormDist(y|x, R)$, whose parameters are set by the interactive sliders.\n",
    "\n",
    "We are now dealing with three (!) separate distributions,\n",
    "which introduces a lot of symbols to keep track of ‚Äì a necessary evil for later.\n",
    "\n",
    "**Exc ‚Äì `Bayes1` properties:** This exercise serves to make you acquainted with how Bayes' rule blends information.\n",
    "\n",
    "Move the sliders (use arrow keys?) to animate it, and answer the following (with the boolean checkmarks both on and off).\n",
    "\n",
    "- What happens to the posterior when $R \\rightarrow \\infty$ ?\n",
    "- What happens to the posterior when $R \\rightarrow 0$ ?\n",
    "- Move $y$ around. What is the posterior's location (mean/mode) when $R$ equals the prior variance?\n",
    "- Can you say something universally valid (for any $y$ and $R$) about the height of the posterior pdf?\n",
    "- Does the posterior scale (width) depend on $y$?  \n",
    "   *Optional*: What does this mean [information-wise](https://en.wikipedia.org/wiki/Differential_entropy#Differential_entropies_for_various_distributions)?\n",
    "- Consider the shape (ignoring location & scale) of the posterior. Does it depend on $R$ or $y$?\n",
    "- Can you see a shortcut to computing this posterior rather than having to do the pointwise multiplication?\n",
    "- For the case of two uniform distributions: What happens when you move the prior and likelihood too far apart? Is the fault of the implementation, the math, or the problem statement?\n",
    "- Play around with the grid resolution (see the cell above). What is in your opinion a \"sufficient\" grid resolution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ab05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('Posterior behaviour')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a76780b",
   "metadata": {},
   "source": [
    "## With forward (observation) models\n",
    "\n",
    "In general, the observation $y$ is not a \"direct\" measurement of $x$, as above,\n",
    "but rather some transformation, i.e. function of $x$,\n",
    "which is called **observation/forward model**, $\\ObsMod$.\n",
    "Examples include:\n",
    "\n",
    "- $\\ObsMod(x) = x + 273$ for a thermometer reporting ¬∞C, while $x$ is the temperature in ¬∞K.\n",
    "- $\\ObsMod(x) = 10 x$ for a ruler using mm, while $x$ is stored as cm.\n",
    "- $\\ObsMod(x) = \\log(x)$ for litmus paper (pH measurement), where $x$ is the molar concentration of hydrogen ions.\n",
    "- $\\ObsMod(x) = |x|$ for bicycle speedometers (measuring rpm, i.e. Hall effect sensors).\n",
    "- $\\ObsMod(x) = 2 \\pi h \\, x^2$ if observing inebriation (drunkenness), and the unknown, $x$, is the radius of the beer glasses.\n",
    "\n",
    "Of course, the linear and logarithmic transformations are hardly worthy of the name \"model\", since they merely change the scale of measurement, and so could be trivially done away with. But doing so is not necessary, and they will serve to illustrate some important points.\n",
    "\n",
    "In addition, measurement instruments always (at least for continuous variables) have limited accuracy,\n",
    "i.e. there is an **measurement noise/error** corrupting the observation. For simplicity, this noise is usually assumed *additive*, so that the observation, $y$, is related to the true state, $x$, by\n",
    "$$\n",
    "y = \\ObsMod(x) + r \\,, \\;\\; \\qquad \\tag{Obs}\n",
    "$$\n",
    "and $r \\sim \\NormDist(0, R)$ for some variance $R>0$.\n",
    "Then the likelihood is $$p(y|x) = \\NormDist(y| \\ObsMod(x), R) \\,. \\tag{Lklhd}$$\n",
    "\n",
    "**Exc (optional) ‚Äì The likelihood:** Derive the expression (Lklhd) for the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d61519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('Likelihood')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d0618c",
   "metadata": {},
   "source": [
    "<a name=\"Exc-‚Äì-Obs.-model-gallery\"></a>\n",
    "\n",
    "#### Exc ‚Äì Obs. model gallery\n",
    "\n",
    "Consider the following observation models.\n",
    "\n",
    "- (a) $\\ObsMod(x) = x + 15$.\n",
    "- (b) $\\ObsMod(x) = 2 x$.\n",
    "- (c) $\\ObsMod(x) = (x-5)^2$.\n",
    "  - Explain how negative values of $y$ are possible.\n",
    "- (d) Try $\\ObsMod(x) = |x|$.\n",
    "\n",
    "In each case, describe how and why you'd expect the likelihood to change (as compared to $\\ObsMod(x) = x$).\n",
    "Then verify your answer by implementing `H` in the [interactive Bayes' rule](#Interactive-illustration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdf63e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('Observation models', 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc12b697",
   "metadata": {},
   "source": [
    "It is important to appreciate that the likelihood, and its role in Bayes' rule, does not perform any \"inversion\". It simply quantifies how well each $x$ fits the data, in terms of its weighting. This approach also inherently handles the fact that multiple values of $x$ may be plausible.\n",
    "\n",
    "**Exc (optional) ‚Äì \"why inverse\":** Laplace called \"statistical inference\" the reasoning of \"inverse probability\" (1774). You may also have heard of \"inverse problems\" in reference to similar problems, but without a statistical framing. In view of this, why do you think we use $x$ for the unknown, and $y$ for the known/given data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c45b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer(\"what's forward?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766ae502",
   "metadata": {},
   "source": [
    "<a name=\"Linear-Gaussian-Bayes'-rule-(1D)\"></a>\n",
    "\n",
    "## Linear-Gaussian Bayes' rule (1D)\n",
    "\n",
    "To address this computational difficulty, we can take a more analytical (\"pen-and-paper\") approach: instead of computing the full posterior, we compute only its parameters (mean and (co)variance).\n",
    "This is straightforward in the linear-Gaussian case, i.e. when $\\ObsMod$ is linear (just a number). For readability, the unknown, $x$, is colored.\n",
    "\n",
    "- Given the prior of $p(\\color{darkorange}{x}) = \\NormDist(\\color{darkorange}{x} \\mid x^\\tf, P^\\tf)$\n",
    "- and a likelihood $p(y|\\color{darkorange}{x}) = \\NormDist(y \\mid \\ObsMod \\color{darkorange}{x},R)$,  \n",
    "- $\\implies$ posterior $\n",
    "  p(\\color{darkorange}{x}|y)\n",
    "  = \\NormDist(\\color{darkorange}{x} \\mid x^\\ta, P^\\ta) \\,, $\n",
    "  where, in the 1-dimensional/univariate/scalar (multivariate is discussed in [T5](T5%20-%20Multivariate%20Kalman%20filter.ipynb)) case:\n",
    "  $$\\begin{align}\n",
    "    P^\\ta &= 1/(1/P^\\tf + \\ObsMod^2/R) \\,, \\tag{5} \\\\\\\n",
    "    x^\\ta &= P^\\ta (x^\\tf/P^\\tf + \\ObsMod y/R) \\,.  \\tag{6}\n",
    "  \\end{align}$$\n",
    "\n",
    "The proof is in the following exercise.\n",
    "\n",
    "<a name=\"Exc-‚Äì-BR-LG1\"></a>\n",
    "\n",
    "#### Exc ‚Äì BR-LG1\n",
    "\n",
    "Consider the following identity, where $P^\\ta$ and $x^\\ta$ are given by eqns. (5) and (6).\n",
    "$$\n",
    "\\frac{(\\color{darkorange}{x}-x^\\tf)^2}{P^\\tf} + \\frac{(\\ObsMod \\color{darkorange}{x}-y)^2}{R} \\quad =\n",
    "\\quad \\frac{(\\color{darkorange}{x} - x^\\ta)^2}{P^\\ta} + \\frac{(y - \\ObsMod x^\\tf)^2}{R + P^\\tf} \\,, \\tag{LG1}\n",
    "$$\n",
    "Notice that the left hand side (LHS) is the sum of *two* squares with $\\color{darkorange}{x}$,\n",
    "but the RHS only contains *one*.\n",
    "\n",
    "- (a) Actually derive the first term of the RHS of (LG1), i.e. eqns. (5) and (6).  \n",
    "  *Hint: you can simplify the task by first \"hiding\" $\\ObsMod$*\n",
    "- (b) *Optional*: Derive the full RHS (i.e. also the second term).\n",
    "- (c) Show that $p(\\color{darkorange}{x}|y) = \\NormDist(\\color{darkorange}{x} \\mid x^\\ta, P^\\ta)$\n",
    "  using part (a), Bayes' rule (BR2), and the Gaussian pdf (G1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90eb909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('BR Gauss, a.k.a. completing the square', 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3527be59",
   "metadata": {},
   "source": [
    "**Exc ‚Äì Temperature example:**\n",
    "The statement $x = \\mu \\pm \\sigma$ is *sometimes* used\n",
    "as a shorthand for $p(x) = \\NormDist(x \\mid \\mu, \\sigma^2)$. Suppose\n",
    "\n",
    "- you think the temperature $x = 20¬∞C \\pm 2¬∞C$,\n",
    "- a thermometer yields the observation $y = 18¬∞C \\pm 2¬∞C$.\n",
    "\n",
    "Show that your posterior is $p(x|y) = \\NormDist(x \\mid 19, 2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab02ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('LG BR example')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0925f3",
   "metadata": {},
   "source": [
    "The following implements the linear-Gaussian Bayes' rule (eqns. 5 and 6).\n",
    "Note that its inputs and outputs are not arrays (as in `Bayes_rule()`), but simply scalar numbers: the means, variances, and $\\ObsMod$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd52539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bayes_rule_LG1(xf, Pf, y, H, R):\n",
    "    Pa = 1 / (1/Pf + H**2/R)\n",
    "    xa = Pa * (xf/Pf + H*y/R)\n",
    "    return xa, Pa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee39757f",
   "metadata": {},
   "source": [
    "#### Exc ‚Äì Gaussianity as an approximation\n",
    "\n",
    "- (a) Again, try the various $\\ObsMod$ from the [above exercise](#Exc-‚Äì-Obs.-model-gallery) in the [interactive Bayes' rule widget](#Interactive-illustration).  \n",
    "  For which $\\ObsMod$ does `Bayes_rule_LG1()` reproduce `Bayes_rule()`?\n",
    "- (b) For simplicity, revert back to the identity for $\\ObsMod$.\n",
    "  Then run the cell below, which fits distributions [from `scipy`s library of distributions](https://stackoverflow.com/questions/37559470/)\n",
    "  so as to respect `mu` and `sig2` and adds them to the pdfs in the dropdown menus of the widget\n",
    "  (upon re-running its cell). Try them out for the likelihood, and answer the following.\n",
    "  Which ones\n",
    "\n",
    "  - Are skewed (or at least asymmetric) ?\n",
    "  - Have excess kurtosis (tails heavier than for the Gaussian) ?\n",
    "  - Produces a posterior variance that increases with the distance prior-observation ?  \n",
    "    *PS: this one (the Student's) is frequently used\n",
    "    to increase the robustness of the (ensemble) Kalman filter,\n",
    "    or [estimate the \"inflation\" factor](https://rmets.onlinelibrary.wiley.com/doi/10.1002/qj.3386).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49572dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "for dist in [\n",
    "  ss.chi2(df=3),\n",
    "  ss.beta(a=5, b=2),\n",
    "  ss.anglit(),\n",
    "  ss.t(df=3),\n",
    "]:\n",
    "    def pdf_fitted(x, mu, sigma2, dist=dist):\n",
    "        mean, var = dist.stats(moments='mv')\n",
    "        # mean, var = mean_and_var(dist.pdf(grid1d), grid1d)\n",
    "        ratio = np.sqrt(var / sigma2)\n",
    "        u = ratio * (x - mu) + mean\n",
    "        return ratio * dist.pdf(u)\n",
    "    pdfs[dist.dist.name + str(dist.kwds)] = pdf_fitted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b342a",
   "metadata": {},
   "source": [
    "**Exc (optional) ‚Äì Gain algebra:** Show that eqn. (5) can be written as\n",
    "$$P^\\ta = K R / \\ObsMod \\,,    \\tag{8}$$\n",
    "where\n",
    "$$K = \\frac{\\ObsMod P^\\tf}{\\ObsMod^2 P^\\tf + R} \\,,    \\tag{9}$$\n",
    "is called the \"Kalman gain\".  \n",
    "*Hint: again, try to \"hide away\" $\\ObsMod$ among the other objects before proceeding.*\n",
    "\n",
    "Then shown that eqns. (5) and (6) can be written as\n",
    "$$\n",
    "\\begin{align}\n",
    "    P^\\ta &= (1-K \\ObsMod) P^\\tf \\,,  \\tag{10} \\\\\\\n",
    "  x^\\ta &= x^\\tf + K (y- \\ObsMod x^\\tf) \\tag{11} \\,,\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce20dbf",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# show_answer('BR Kalman1 algebra')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34803dbd",
   "metadata": {},
   "source": [
    "<a name=\"Exc-(optional)-‚Äì-Gain-intuition\"></a>\n",
    "\n",
    "#### Exc (optional) ‚Äì Gain intuition\n",
    "\n",
    "Let $\\ObsMod = 1$ for simplicity.\n",
    "\n",
    "- (a) Show that $0 < K < 1$ since $0 < P^\\tf, R$.\n",
    "- (b) Show that $P^\\ta < P^\\tf, R$.\n",
    "- (c) Show that $x^\\ta$ is in the interval $(x^\\tf, y)$.\n",
    "- (d) Why do you think $K$ is called a \"gain\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('KG intuition')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbec053",
   "metadata": {},
   "source": [
    "**Exc ‚Äì BR with Gain:** Re-define `Bayes_rule_LG1` so to as to use eqns. 9-11. Remember to re-run the cell. Verify that you get the same plots as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc66c164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('BR Kalman1 code')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369cec84",
   "metadata": {},
   "source": [
    "<a name=\"Exc-(optional)-‚Äì-optimalities\"></a>\n",
    "\n",
    "#### Exc (optional) ‚Äì optimalities\n",
    "\n",
    "In contrast to orthodox statistics,\n",
    "Bayes' rule (BR) does not attempt to produce a single estimate/value of $x$.\n",
    "It merely states how to update our quantitative belief (weighted possibilities) in light of new data.\n",
    "As such, barring any approximations (such as using `Bayes_rule_LG1` outside the linear-Gaussian case),\n",
    "the (full) posterior will be **optimal** from the perspective of any [proper scoring rule](https://en.wikipedia.org/wiki/Scoring_rule#Propriety_and_consistency).\n",
    "\n",
    "*But if you must* pick a single point value estimate $\\widehat{x}$\n",
    "then you should **decide** on it by optimising (with respect to $\\widehat{x}$)\n",
    "the expectation (with respect to $x$) of some utility/loss function,\n",
    "i.e. $\\Expect\\, \\text{Loss}(x - \\widehat{x})$.\n",
    "For instance, if the posterior pdf happens to be symmetric\n",
    "(as in the linear-Gaussian context above),\n",
    "and your loss function is convex and symmetric,\n",
    "then the mean/median will be optimal [[Lehmann & Casella (1998)](#References), Corollary 7.19].\n",
    "More specifically, for any given distribution of $x$,\n",
    "the optimal [Bayes estimator](https://en.wikipedia.org/wiki/Bayes_estimator) is:\n",
    "\n",
    "- the mode if $\\text{Loss}(d) = \\begin{cases} 0 & \\text{if } d = 0 \\\\ 1 & \\text{otherwise} \\end{cases}$\n",
    "- the median if $\\text{Loss}(d) = |d|$\n",
    "- the mean if $\\text{Loss}(d) = d^2$\n",
    "\n",
    "The last case (squared-error loss) is most commonly used,\n",
    "and the resulting estimator is sometimes called\n",
    "the minimum mean-square error (MMSE) estimator.\n",
    "*Prove that the MMSE is indeed the mean of the distribution!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e70e885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('MMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd619684",
   "metadata": {},
   "source": [
    "However, it is not generally easy to find the posterior mean, median, or mode,\n",
    "so the above optimalities are mainly useful in the linear-Gaussian case,\n",
    "where they justify a preference for $x^\\ta$.\n",
    "\n",
    "<details style=\"border: 1px solid #aaaaaa; border-radius: 4px; padding: 0.5em 0.5em 0;\">\n",
    "  <summary style=\"font-weight: normal; font-style: italic; margin: -0.5em -0.5em 0; padding: 0.5em;\">\n",
    "      It is possible to drop the linear-Gaussian assumption and still\n",
    "      claim optimality for $x^\\ta$ of eqns. (6) and (11) as\n",
    "      the best (min. variance), linear, unbiased, estimate (BLUE ... üîç).\n",
    "  </summary>\n",
    "The result requires reformulating the prior\n",
    "as \"background\" zero-mean noise onto the (non-random) $x$,\n",
    "whose outcome was the prior mean, $x^\\tf$, and whose covariance is $P^\\tf$.\n",
    "Then, by explicit augmentation (i.e. pseudo-obs: $[y, x^\\tf]$) one recovers the linear regression problem\n",
    "of the celebrated Gauss-Markov theorem, generalized by Aitken to the case of correlated noise.\n",
    "Some additional notes follow.\n",
    "\n",
    "- A similar proof uses an ansatz that is linear in both $x^\\tf$ and $y$ separately (without concatenating them).\n",
    "  It does not need the Woodbury matrix identity to derive the Kalman gain form of $x^\\ta$.\n",
    "- The same results is [sometimes](https://en.wikipedia.org/wiki/Minimum_mean_square_error#Linear_MMSE_estimator)\n",
    "  reframed as MMSE, causing confusion with the above (and different) meaning of MMSE.\n",
    "- The result is closely related to the fact that the Moore-Penrose pseudoinverse of linear algebra\n",
    "  finds the minimum (Euclidean) norm solution to a system of linear equations with multiple solutions.\n",
    "- If Gaussianity is assumed (but the perspective remains \"frequentist\"),\n",
    "  then one can drop the linearity requirement,\n",
    "  yielding the (uniformly) minimum-variance unbiased estimate (UMVUE),\n",
    "  as per [Lehmann-Sheff√©](https://stats.stackexchange.com/a/398911), or [Cram√©r-Rao](https://stats.stackexchange.com/a/596307).\n",
    "  Without Gaussianity, the linearity imposition cannot be omitted [P√∂tscher & Preinerstorfer (2024)](#References).\n",
    "\n",
    "- - -\n",
    "</details>\n",
    "\n",
    "All in all, the intuitive idea of **considering the mean of $p(x)$ as the point\n",
    "estimate** has good theoretical foundations.\n",
    "\n",
    "## Summary\n",
    "\n",
    "Bayesian inference quantifies uncertainty (in $x$) using probability.\n",
    "Bayes' rule tells us how to update this belief based on data or observation ($y$).\n",
    "It is simply a reformulation of conditional probability.\n",
    "Observation can be \"inverted\" using Bayes' rule,\n",
    "in the sense that all possibilities for $x$ are weighted.\n",
    "While technically simple, Bayes' rule requires many pointwise multiplications.\n",
    "But if Gaussianity can be assumed, it reduces to just two formulae.\n",
    "\n",
    "### Next: [T4 - Filtering & time series](T4%20-%20Time%20series%20filtering.ipynb)\n",
    "\n",
    "<a name=\"References\"></a>\n",
    "\n",
    "### References\n",
    "\n",
    "<!--\n",
    "@book{jaynes2003probability,\n",
    "  title={Probability theory: the logic of science},\n",
    "  author={Jaynes, Edwin T},\n",
    "  year={2003},\n",
    "  publisher={Cambridge university press}\n",
    "}\n",
    "\n",
    "@book{lehmann1998theory,\n",
    "    title={Theory of Point Estimation},\n",
    "    author={Lehmann, Erich Leo and Casella, George},\n",
    "    volume={31},\n",
    "    year={1998},\n",
    "    publisher={Springer}\n",
    "}\n",
    "@article{potscher2024comment,\n",
    "  title={A Comment on:‚ÄúA Modern Gauss--Markov Theorem‚Äù},\n",
    "  author={P{\\\"o}tscher, Benedikt M and Preinerstorfer, David},\n",
    "  journal={Econometrica},\n",
    "  volume={92},\n",
    "  number={3},\n",
    "  pages={913--924},\n",
    "  year={2024},\n",
    "  publisher={Wiley Online Library}\n",
    "}\n",
    "-->\n",
    "\n",
    "- **Jaynes (2003)**:\n",
    "  Edwin T. Jaynes, \"Probability theory: the logic of science\", 2003.\n",
    "- **Lehmann & Casella (1998)**:\n",
    "  \"Theory of Point Estimation\", 1998.\n",
    "- **P√∂tscher & Preinerstorfer (2024)**:\n",
    "  Benedikt M. P√∂tscher and David Preinerstorfer, \"A Comment on: 'A Modern Gauss-Markov Theorem'\", Econometrica, 2024."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "jupytext": {
   "formats": "ipynb,nb_mirrors//py:light,nb_mirrors//md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
