{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e12b1d07",
   "metadata": {},
   "source": [
    "# T4 - Time series filtering\n",
    "Before we look at the full (multivariate) Kalman filter,\n",
    "let's get more familiar with time-dependent (temporal/sequential) problems.\n",
    "$\n",
    "% Loading TeX (MathJax)... Please wait\n",
    "%\n",
    "\\newcommand{\\Reals}{\\mathbb{R}}\n",
    "\\newcommand{\\Expect}[0]{\\mathbb{E}}\n",
    "\\newcommand{\\NormDist}{\\mathcal{N}}\n",
    "%\n",
    "\\newcommand{\\DynMod}[0]{\\mathscr{M}}\n",
    "\\newcommand{\\ObsMod}[0]{\\mathscr{H}}\n",
    "%\n",
    "\\newcommand{\\mat}[1]{{\\mathbf{{#1}}}}\n",
    "%\\newcommand{\\mat}[1]{{\\pmb{\\mathsf{#1}}}}\n",
    "\\newcommand{\\bvec}[1]{{\\mathbf{#1}}}\n",
    "%\n",
    "\\newcommand{\\trsign}{{\\mathsf{T}}}\n",
    "\\newcommand{\\tr}{^{\\trsign}}\n",
    "\\newcommand{\\ceq}[0]{\\mathrel{≔}}\n",
    "\\newcommand{\\xDim}[0]{D}\n",
    "\\newcommand{\\supa}[0]{^\\text{a}}\n",
    "\\newcommand{\\supf}[0]{^\\text{f}}\n",
    "%\n",
    "\\newcommand{\\I}[0]{\\mat{I}}\n",
    "\\newcommand{\\K}[0]{\\mat{K}}\n",
    "\\newcommand{\\bP}[0]{\\mat{P}}\n",
    "\\newcommand{\\bH}[0]{\\mat{H}}\n",
    "\\newcommand{\\bF}[0]{\\mat{F}}\n",
    "\\newcommand{\\R}[0]{\\mat{R}}\n",
    "\\newcommand{\\Q}[0]{\\mat{Q}}\n",
    "\\newcommand{\\B}[0]{\\mat{B}}\n",
    "\\newcommand{\\C}[0]{\\mat{C}}\n",
    "\\newcommand{\\Ri}[0]{\\R^{-1}}\n",
    "\\newcommand{\\Bi}[0]{\\B^{-1}}\n",
    "\\newcommand{\\X}[0]{\\mat{X}}\n",
    "\\newcommand{\\A}[0]{\\mat{A}}\n",
    "\\newcommand{\\Y}[0]{\\mat{Y}}\n",
    "\\newcommand{\\E}[0]{\\mat{E}}\n",
    "\\newcommand{\\U}[0]{\\mat{U}}\n",
    "\\newcommand{\\V}[0]{\\mat{V}}\n",
    "%\n",
    "\\newcommand{\\x}[0]{\\bvec{x}}\n",
    "\\newcommand{\\y}[0]{\\bvec{y}}\n",
    "\\newcommand{\\z}[0]{\\bvec{z}}\n",
    "\\newcommand{\\q}[0]{\\bvec{q}}\n",
    "\\newcommand{\\br}[0]{\\bvec{r}}\n",
    "\\newcommand{\\bb}[0]{\\bvec{b}}\n",
    "%\n",
    "\\newcommand{\\bx}[0]{\\bvec{\\bar{x}}}\n",
    "\\newcommand{\\by}[0]{\\bvec{\\bar{y}}}\n",
    "\\newcommand{\\barB}[0]{\\mat{\\bar{B}}}\n",
    "\\newcommand{\\barP}[0]{\\mat{\\bar{P}}}\n",
    "\\newcommand{\\barC}[0]{\\mat{\\bar{C}}}\n",
    "\\newcommand{\\barK}[0]{\\mat{\\bar{K}}}\n",
    "%\n",
    "\\newcommand{\\D}[0]{\\mat{D}}\n",
    "\\newcommand{\\Dobs}[0]{\\mat{D}_{\\text{obs}}}\n",
    "\\newcommand{\\Dmod}[0]{\\mat{D}_{\\text{obs}}}\n",
    "%\n",
    "\\newcommand{\\ones}[0]{\\bvec{1}}\n",
    "\\newcommand{\\AN}[0]{\\big( \\I_N - \\ones \\ones\\tr / N \\big)}\n",
    "%\n",
    "% END OF MACRO DEF\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6215bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import resources.workspace as ws\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0defcd8",
   "metadata": {},
   "source": [
    "## Example problem: AR(1).\n",
    "Consider the scalar, stochastic process $\\{x_k\\}$ generated by\n",
    "$$ x_{x+1} = \\mathcal{M}_k x_k + q_k \\,, \\tag{Dyn} $$\n",
    "for sequentially increasing time index $k$,\n",
    "where $q_k$ is white noise ($q_k$ independent of $q_l$ for $k \\neq l$).\n",
    "This is a so-called order-1 auto-regressive process,\n",
    "i.e. [AR(1)](https://en.wikipedia.org/wiki/Autoregressive_model#Example:_An_AR(1)_process).\n",
    "Suppose we get observations, $\\{y_k\\}$, corrupted by noise, as in\n",
    "$$ y_k = \\mathcal{H}_k x_k + \\varepsilon_k \\,, \\tag{Obs} $$\n",
    "where the noise, $\\varepsilon_k$, is again independent of everything.\n",
    "For our present purposes, the dynamical and observation \"models\",\n",
    "$\\mathcal{M}_k$ and $\\mathcal{H}_k$, are just some numbers that we know.\n",
    "Merely to alleviate the burden of bookkeeping,\n",
    "we henceforth assume that they are constant in time.\n",
    "Similarly, for each $k$, let\n",
    "- $q_k \\sim \\NormDist(0, Q)$,\n",
    "- $\\varepsilon_k \\sim \\NormDist(0, R)$.\n",
    "\n",
    "Also assume $x_0 \\sim \\NormDist(x\\supa_0, P\\supa_0)$.\n",
    "The code below simulates a random realisation of this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad09cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use H=1 so that it makes sense to plot data on same axes as state.\n",
    "H = 1\n",
    "\n",
    "# Initial estimate\n",
    "xa = 0   # mean\n",
    "Pa = 10  # variance\n",
    "\n",
    "def simulate(nTime, xa, Pa, M, H, Q, R):\n",
    "    \"\"\"Simulate synthetic truth (x) and observations (y).\"\"\"\n",
    "    x = xa + np.sqrt(Pa)*rnd.randn()        # Draw initial condition\n",
    "    truths = np.zeros(nTime)                # Allocate\n",
    "    obsrvs = np.zeros(nTime)                # Allocate\n",
    "    for k in range(nTime):                  # Loop in time\n",
    "        x = M * x + np.sqrt(Q)*rnd.randn()  # Dynamics\n",
    "        y = H * x + np.sqrt(R)*rnd.randn()  # Measurement\n",
    "        truths[k] = x                       # Assign\n",
    "        obsrvs[k] = y                       # Assign\n",
    "    return truths, obsrvs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9122bd69",
   "metadata": {},
   "source": [
    "The following plots the process.\n",
    "If you find that there are not enough sliders to play around with,\n",
    "feel free to alter the code to suit your needs\n",
    "(for example, you can comment out the line plotting observations, or `cInterval`).\n",
    "*PS: Some of the sliders get activated later.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd20e5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ws.interact(seed=(1, 12), M=(0, 1.03, .01), nTime=(0, 100),\n",
    "             logR=(-9, 9), logR_bias=(-9, 9),\n",
    "             logQ=(-9, 9), logQ_bias=(-9, 9))\n",
    "def exprmt(seed, nTime, M=0.97, logR=1, logQ=1, analyses_only=False, logR_bias=0, logQ_bias=0):\n",
    "    R, Q, Q_bias, R_bias = 4.0**np.array([logR, logQ, logQ_bias, logR_bias])\n",
    "\n",
    "    rnd.seed(seed)\n",
    "    truths, obsrvs = simulate(nTime, xa, Pa, M, H, Q, R)\n",
    "\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    kk = 1 + np.arange(nTime)\n",
    "    plt.plot(kk, truths, 'k' , label='True state ($x$)')\n",
    "    plt.plot(kk, obsrvs, 'g*', label='Noisy obs ($y$)', ms=9)\n",
    "\n",
    "    try:\n",
    "        estimates, variances = KF(nTime, xa, Pa, M, H, Q*Q_bias, R*R_bias, obsrvs)\n",
    "        if analyses_only:\n",
    "            plt.plot(kk, estimates[:, 1], label='Kalman$\\supa$ ± 1$\\sigma$')\n",
    "            plt.fill_between(kk, *ws.cInterval(estimates[:, 1], variances[:, 1]), alpha=.2)\n",
    "        else:\n",
    "            kk2 = kk.repeat(2)\n",
    "            plt.plot(kk2, estimates.flatten(), label='Kalman ± 1$\\sigma$')\n",
    "            plt.fill_between(kk2, *ws.cInterval(estimates, variances), alpha=.2)\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    sigproc = {}\n",
    "    ### INSERT ANSWER TO EXC \"signal processing\" HERE ###\n",
    "    # sigproc['some method'] = ...\n",
    "    for method, estimate in sigproc.items():\n",
    "        plt.plot(kk[:len(estimate)], estimate, label=method)\n",
    "\n",
    "    plt.xlabel('Time index (k)')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.axhline(0, c='k', lw=1, ls='--')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a6c90a",
   "metadata": {},
   "source": [
    "**Exc -- AR1 properties:** Answer the following.\n",
    "- What does `seed` control?\n",
    "- Explain what happens when `M=0`. Also consider $Q \\rightarrow 0$.  \n",
    "  Can you give a name to this `truth` process,\n",
    "  i.e. a link to the relevant Wikipedia page?  \n",
    "  What about when `M=1`?  \n",
    "  Describe the general nature of the process as `M` changes from 0 to 1.  \n",
    "  What about when `M>1`?  \n",
    "- What happens when $R \\rightarrow 0$ ?\n",
    "- What happens when $R \\rightarrow \\infty$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb1548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('AR1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8a3170",
   "metadata": {},
   "source": [
    "## The (univariate) Kalman filter (KF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229c5fcb",
   "metadata": {},
   "source": [
    "Now we have a random variable that evolves in time, that we can pretend is unknown,\n",
    "in order to estimate (or \"track\") it.\n",
    "More specifically, while $x_0$ is unknown,\n",
    "we do know that $x_0 \\sim \\NormDist(x\\supa_0, P\\supa_0)$, including the parameters.\n",
    "We also know that $x_k$ evolves according to eqn. (Dyn).\n",
    "Therefore, as shown in the following exercise,\n",
    "for $x_1$ we know that\n",
    "$x_1 \\sim \\NormDist(x\\supf_1, P\\supf_1)$, with\n",
    "$$\\begin{align}\n",
    "x\\supf_1 &= \\DynMod \\, x\\supa_0 \\tag{5} \\\\\n",
    "P\\supf_1 &= \\DynMod^2 \\, P\\supa_0 + Q \\tag{6}\n",
    "\\end{align}$$\n",
    "\n",
    "#### Exc -- linear algebra of Gaussian random variables\n",
    "- (a) Show the linearity of the expectation operator:\n",
    "      $\\Expect [ \\DynMod  x + b ] = \\DynMod \\Expect[x] + b$, for some constant $b$.\n",
    "- (b) Thereby, show that $\\mathbb{Var}[ \\DynMod  x + b ] = \\DynMod^2 \\mathbb{Var} [x]$.\n",
    "- (c) Now let $z = x + q$, with $x$ and $q$ independent and Guassian.\n",
    "      Then the pdf of this sum of random variables, $p_z(z)$, is given by convolution\n",
    "      (hopefully this makes intuitive sense, at least in the discrete case):\n",
    "      $$ p_z(z) = \\int p_x(x) \\, p_q(z - x) \\, d x \\,.$$\n",
    "      Show that $z$ is also Gaussian,\n",
    "      whose mean and variance are the sum of the means and variances (respectively).  \n",
    "      *Hint: you will need the result on [completing the square](T3%20-%20Bayesian%20inference.ipynb#Exc----GG-Bayes),\n",
    "      specifically the part that we did not make use of for Bayes' rule.  \n",
    "      If you get stuck, you can also view the excellent [3blue1brown](https://www.youtube.com/watch?v=d_qvLDhkg00&t=266s&ab_channel=3Blue1Brown) on the topic.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d699eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Sum of Gaussians', 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66da5c8",
   "metadata": {},
   "source": [
    "Formulae (5) and (6) are called the **forecast step** of the KF.\n",
    "But when $y_1$ becomes available, according to eqn. (Obs),\n",
    "then we can update/condition our estimate of $x_1$, i.e. compute the posterior,\n",
    "$p(x_1 | y_1) = \\mathcal{N}(x_1 \\mid x\\supa_1, P\\supa_1) \\,,$\n",
    "using the formulae we developed for Bayes' rule with\n",
    "[Gaussian distributions](T3%20-%20Bayesian%20inference.ipynb#Gaussian-Gaussian-Bayes'-rule-(1D)).\n",
    "We call this the **analysis step** of the KF.\n",
    "Note that there are no approximations to these steps (with the linear-Gaussianassumptions we have made).\n",
    "\n",
    "If $k=1$ is \"today\", then we can say that \"yesterday's forecast became today's prior\".\n",
    "We can subsequently apply the same two steps again\n",
    "to produce forecast and analysis estimates for the next time index $k$.\n",
    "Thus, at any point in time, $k$, we compute the exact Bayesian pdf's for $x_k$:\n",
    "The analysis step \"assimilates\" $y_k$ to compute $p(x_k | y_{1:k})$,\n",
    "where $y_{1:k} = y_1, \\ldots, y_k$,\n",
    "while the forecast computes $p(x_{k+1}| y_{1:k})$.\n",
    "It is important to appreciate two benefits to this **recursive** procedure\n",
    "\n",
    "- The recursiveness of the procedure reflects the recursiveness (Markov property) of nature:\n",
    "  Both in the problem and our solution, time $k+1$ *builds on* time $k$.\n",
    "  It means that we do not have to re-do the entire problem for each $k$.\n",
    "- At every time $k$ we only deal with functions of 1 or 2 variables: $x_k$ and $x_{k+1}$.\n",
    "  Even if we were doing $k$ computations, this is a significantly smaller domain\n",
    "  (in which to quanitify our densities) than that of the joint pdf $p(x_{1:k} | y_{1:k})$.\n",
    "  Ref. [curse of dimensionality](http://localhost:8888/notebooks/notebooks/T3%20-%20Bayesian%20inference.ipynb#Exc-(optional)----Curse-of-dimensionality,-part-1).\n",
    "\n",
    "Note, however, that our recursive procedure, called ***filtering***, does not compute $p(x_l | y_{1:k})$ for any $l<k$. In other words, the filtering estimate only contains *past* information. Updating estimates of the state at any previous time(s) is called ***smoothing***.  However, for the purposes of prediction/forecasting, filtering is all we need: accurate initial conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99f0088",
   "metadata": {},
   "source": [
    "#### Exc -- Implementation\n",
    "Below is a very rudimentary sequential estimator (not the KF!), essentially just doing \"persistance\" forecasts, and setting the analysis estimates to the value of the observations (*which is only generally a possibility in this linear, scalar case*). Run its cell to define it, and then re-run the above interactive animation cell. Then\n",
    "- Implement the KF properly by replace the forecast and analysis steps below. *Re-run the cell.*\n",
    "- Try implementing the analysis step both in the \"precision\" and \"gain\" forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1864c6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KF(nTime, xa, Pa, M, H, Q, R, obsrvs):\n",
    "    \"\"\"Kalman filter. PS: (xa, Pa) should be input with *initial* values.\"\"\"\n",
    "    ############################\n",
    "    # TEMPORARY IMPLEMENTATION #\n",
    "    ############################\n",
    "    estimates = np.zeros((nTime, 2))\n",
    "    variances = np.zeros((nTime, 2))\n",
    "    for k in range(nTime):\n",
    "        # Forecast step\n",
    "        xf = xa\n",
    "        Pf = Pa\n",
    "        # Analysis update step\n",
    "        Pa = R / H**2\n",
    "        xa = obsrvs[k] / H\n",
    "        # Assign\n",
    "        estimates[k] = xf, xa\n",
    "        variances[k] = Pf, Pa\n",
    "    return estimates, variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee23d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('KF1 code')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbaa4bf",
   "metadata": {},
   "source": [
    "#### Exc -- KF behaviour\n",
    "- Set `logQ` to its minimum, and `M=1`.  \n",
    "  We established in Exc \"AR1\" that the true states are now constant in time (but unknown).  \n",
    "  How does the KF fare in estimating it?  \n",
    "  Does its uncertainty variance ever reach 0?\n",
    "- What is the KF uncertainty variance in the case of `M=0`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33843343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('KF behaviour')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350ae0cd",
   "metadata": {},
   "source": [
    "#### Exc -- Temporal convergence\n",
    "In general, $\\DynMod$, $\\ObsMod$, $Q$, and $R$ depend on time, $k$\n",
    "(often to parameterize exogenous/outside factors/forces/conditions),\n",
    "and there are no limit values that the KF parameters converge to.\n",
    "But, let $Q=0$ in eqn (Dyn) so that $x_{k+1} = \\DynMod x_k$ for some *constant* $\\DynMod$.\n",
    "Show that sequence of $P_k\\supa$ converges to\n",
    "- (a) 0 if $\\DynMod < 1$.  \n",
    "  *Hint: Merge the analysis and forecast equations.*\n",
    "- (b) 0 if $\\DynMod = 1$.\n",
    "- (c) $R (1-1/\\DynMod^2)$ if $\\DynMod > 1$.  \n",
    "  *Hint: Look for the fixed point.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba5c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Asymptotic Riccati')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3abbb0",
   "metadata": {},
   "source": [
    "#### Exc -- Impact of biases\n",
    "Re-run the above interative animation to set the default control values. Answer the following\n",
    "\n",
    "- `logR_bias`/`logQ_bias` control the (multiplicative) bias in $R$/$Q$ that is fed to the KF.\n",
    "  What happens when the KF \"thinks\" the measurement/dynamical error\n",
    "  is (much) smaller than it actually is?\n",
    "  What about larger?\n",
    "- Re-run the animation to get default values.\n",
    "  Set `logQ` to 0, which will make the following behaviour easier to describe.\n",
    "  In the code, add 20 to the initial `xa` **given to the KF**.\n",
    "  How long does it take for it to recover from this initial bias?\n",
    "- Multiply `Pa` **given to the KF** by 0.01. What about now?\n",
    "- Remove the previous biases.\n",
    "  Instead, multiply `M` **given to the KF** by 2, and observe what happens.\n",
    "  Try the same, but dividing `M` by 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99621a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('KF with bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594eff0",
   "metadata": {},
   "source": [
    "When it comes to (especially univariate) time series analysis,\n",
    "the Kalman filter (KF) is not the only game in town.\n",
    "For example, signal processing offers several alternative filters\n",
    "(indeed, the word \"filter\" in the KF originates in that domain).\n",
    "But for the above problem (which is linear-Gaussian!),\n",
    "the KF is guaranteed (on average, in the long run, in terms of mean square error)\n",
    "to outperform any other method.\n",
    "We will see cases later (of full-blown state estimation)\n",
    "where the difference is much clearer,\n",
    "and indeed it might not even be clear how to apply signal processing methods.\n",
    "However, the KF has an unfair advantage: we are giving it a ton of information\n",
    "about the problem (`M, H, R, Q`) that the signal processing methods\n",
    "do not get. Therefore, they typically also require a good deal of tuning.\n",
    "We will not review any of its theory here, but play around with what `scipy` can offer us.\n",
    " \n",
    "#### Exc (optional) -- signal processing\n",
    "Run the following cell to import and define some more tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474ff892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.signal as sig\n",
    "def nrmlz(x):\n",
    "    return x / x.sum()\n",
    "def trunc(x, n):\n",
    "    return np.pad(x[:n], (0, len(x)-n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1cc96d",
   "metadata": {},
   "source": [
    "Now try to \"filter\" the `obsrvs` to produce estimates of `truth`.\n",
    "In each case, add your estimate (\"filtered signal\" in that domain's parlance)\n",
    "to the `sigproc` dictionnary in the interactive animation cell,\n",
    "with an appropriate name/key (this will automatically include it in the plotting).  \n",
    "Use\n",
    "- (a) [`sig.wiener`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.wiener.html).  \n",
    "      *PS: this is a direct ancestor of the KF*.\n",
    "- (b) a moving average, for example [`sig.windows.hamming`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.windows.hamming.html).  \n",
    "      *Hint: you may also want to use [`sig.convolve`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve.html#scipy.signal.convolve)*.\n",
    "- (c) a low-pass filter using [`np.fft`](https://docs.scipy.org/doc/scipy/reference/fft.html#).  \n",
    "      *Hint: you may also want to use the above `trunc` function.*\n",
    "- (d) The [`sig.butter`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.butter.html) filter.\n",
    "      *Hint: apply with [`sig.filtfilt`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.filtfilt.html).*\n",
    "- (e) not really a signal processing method: [`sp.interpolate.UniveriateSpline`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.UnivariateSpline.html)\n",
    "\n",
    "The answers should be considered examples, not the uniquely right way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('signal processing', 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7fa142",
   "metadata": {},
   "source": [
    "## Summary\n",
    "As a subset of state estimation we can do time series estimation\n",
    "[(wherein state-estimation is called state-space approach)](https://www.google.com/search?q=\"We+now+demonstrate+how+to+put+these+models+into+state+space+form\").\n",
    "Moreover, DA methods produce uncertainty quantification, something which is usually more obscure with time series analysis methods."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
