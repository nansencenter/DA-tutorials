{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e12b1d07",
   "metadata": {},
   "source": [
    "# T4 - Time series filtering\n",
    "Before we look at the full (multivariate) Kalman filter,\n",
    "let's get more familiar with time-dependent (temporal/sequential) problems.\n",
    "$\n",
    "% ######################################## Loading TeX (MathJax)... Please wait ########################################\n",
    "\\newcommand{\\Reals}{\\mathbb{R}} \\newcommand{\\Expect}[0]{\\mathbb{E}} \\newcommand{\\NormDist}{\\mathcal{N}} \\newcommand{\\DynMod}[0]{\\mathscr{M}} \\newcommand{\\ObsMod}[0]{\\mathscr{H}} \\newcommand{\\mat}[1]{{\\mathbf{{#1}}}} \\newcommand{\\bvec}[1]{{\\mathbf{#1}}} \\newcommand{\\trsign}{{\\mathsf{T}}} \\newcommand{\\tr}{^{\\trsign}} \\newcommand{\\ceq}[0]{\\mathrel{≔}} \\newcommand{\\xDim}[0]{D} \\newcommand{\\supa}[0]{^\\text{a}} \\newcommand{\\supf}[0]{^\\text{f}} \\newcommand{\\I}[0]{\\mat{I}} \\newcommand{\\K}[0]{\\mat{K}} \\newcommand{\\bP}[0]{\\mat{P}} \\newcommand{\\bH}[0]{\\mat{H}} \\newcommand{\\bF}[0]{\\mat{F}} \\newcommand{\\R}[0]{\\mat{R}} \\newcommand{\\Q}[0]{\\mat{Q}} \\newcommand{\\B}[0]{\\mat{B}} \\newcommand{\\C}[0]{\\mat{C}} \\newcommand{\\Ri}[0]{\\R^{-1}} \\newcommand{\\Bi}[0]{\\B^{-1}} \\newcommand{\\X}[0]{\\mat{X}} \\newcommand{\\A}[0]{\\mat{A}} \\newcommand{\\Y}[0]{\\mat{Y}} \\newcommand{\\E}[0]{\\mat{E}} \\newcommand{\\U}[0]{\\mat{U}} \\newcommand{\\V}[0]{\\mat{V}} \\newcommand{\\x}[0]{\\bvec{x}} \\newcommand{\\y}[0]{\\bvec{y}} \\newcommand{\\z}[0]{\\bvec{z}} \\newcommand{\\q}[0]{\\bvec{q}} \\newcommand{\\br}[0]{\\bvec{r}} \\newcommand{\\bb}[0]{\\bvec{b}} \\newcommand{\\bx}[0]{\\bvec{\\bar{x}}} \\newcommand{\\by}[0]{\\bvec{\\bar{y}}} \\newcommand{\\barB}[0]{\\mat{\\bar{B}}} \\newcommand{\\barP}[0]{\\mat{\\bar{P}}} \\newcommand{\\barC}[0]{\\mat{\\bar{C}}} \\newcommand{\\barK}[0]{\\mat{\\bar{K}}} \\newcommand{\\D}[0]{\\mat{D}} \\newcommand{\\Dobs}[0]{\\mat{D}_{\\text{obs}}} \\newcommand{\\Dmod}[0]{\\mat{D}_{\\text{obs}}} \\newcommand{\\ones}[0]{\\bvec{1}} \\newcommand{\\AN}[0]{\\big( \\I_N - \\ones \\ones\\tr / N \\big)}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11ab7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote = \"https://raw.githubusercontent.com/nansencenter/DA-tutorials\"\n",
    "!wget -qO- {remote}/master/notebooks/resources/colab_bootstrap.sh | bash -s\n",
    "import resources.workspace as ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6215bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0defcd8",
   "metadata": {},
   "source": [
    "## Example problem: AR(1).\n",
    "Consider the scalar, stochastic process $\\{x_k\\}$ generated by\n",
    "$$ x_{x+1} = \\DynMod_k x_k + q_k \\,, \\tag{Dyn} $$\n",
    "for sequentially increasing time index $k$,\n",
    "where $q_k$ is white noise ($q_k$ independent of $q_l$ for $k \\neq l$).\n",
    "For our present purposes, the dynamical \"model\",\n",
    "$\\DynMod_k$ is just some number that we know.\n",
    "Merely to alleviate the burden of bookkeeping,\n",
    "we henceforth assume that it is constant in time.\n",
    "Then $\\{x_k\\}$ is a so-called order-1 auto-regressive process,\n",
    "i.e. [AR(1)](https://en.wikipedia.org/wiki/Autoregressive_model#Example:_An_AR(1)_process).\n",
    "Suppose we get observations, $\\{y_k\\}$, corrupted by noise, as in\n",
    "$$ y_k = \\ObsMod_k x_k + r_k \\,, \\tag{Obs} $$\n",
    "where the noise, $r_k$, is again independent of everything.\n",
    "Moreover, for simplicity,\n",
    "assume that the measurement model, $\\ObsMod$, is independent of $k$,\n",
    "and for each $k$, let\n",
    "- $q_k \\sim \\NormDist(0, Q)$,\n",
    "- $r_k \\sim \\NormDist(0, R)$.\n",
    "\n",
    "Also assume $x_0 \\sim \\NormDist(x\\supa_0, P\\supa_0)$.\n",
    "The code below simulates a random realisation of this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad09cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use H=1 so that it makes sense to plot data on same axes as state.\n",
    "H = 1\n",
    "\n",
    "# Initial estimate\n",
    "xa = 0   # mean\n",
    "Pa = 10  # variance\n",
    "\n",
    "def simulate(nTime, xa, Pa, M, H, Q, R):\n",
    "    \"\"\"Simulate synthetic truth (x) and observations (y).\"\"\"\n",
    "    x = xa + np.sqrt(Pa)*rnd.randn()        # Draw initial condition\n",
    "    truths = np.zeros(nTime)                # Allocate\n",
    "    obsrvs = np.zeros(nTime)                # Allocate\n",
    "    for k in range(nTime):                  # Loop in time\n",
    "        x = M * x + np.sqrt(Q)*rnd.randn()  # Dynamics\n",
    "        y = H * x + np.sqrt(R)*rnd.randn()  # Measurement\n",
    "        truths[k] = x                       # Assign\n",
    "        obsrvs[k] = y                       # Assign\n",
    "    return truths, obsrvs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9122bd69",
   "metadata": {},
   "source": [
    "The following plots the process. *You don't need to read & understand it*,\n",
    "but if you find that there are not enough sliders to play around with,\n",
    "feel free to alter the code to suit your needs\n",
    "(for example, you can comment out the line plotting observations, or `cInterval`).\n",
    "*PS: Some of the sliders get activated later.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd20e5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ws.interact(seed=(1, 12), M=(0, 1.03, .01), nTime=(0, 100),\n",
    "             logR=(-9, 9), logR_bias=(-9, 9),\n",
    "             logQ=(-9, 9), logQ_bias=(-9, 9))\n",
    "def exprmt(seed=4, nTime=50, M=0.97, logR=1, logQ=1, analyses_only=False, logR_bias=0, logQ_bias=0):\n",
    "    R, Q, Q_bias, R_bias = 4.0**np.array([logR, logQ, logQ_bias, logR_bias])\n",
    "\n",
    "    rnd.seed(seed)\n",
    "    truths, obsrvs = simulate(nTime, xa, Pa, M, H, Q, R)\n",
    "\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    kk = 1 + np.arange(nTime)\n",
    "    plt.plot(kk, truths, 'k' , label='True state ($x$)')\n",
    "    plt.plot(kk, obsrvs, 'g*', label='Noisy obs ($y$)', ms=9)\n",
    "\n",
    "    try:\n",
    "        estimates, variances = KF(nTime, xa, Pa, M, H, Q*Q_bias, R*R_bias, obsrvs)\n",
    "        if analyses_only:\n",
    "            plt.plot(kk, estimates[:, 1], label='Kalman$\\supa$ ± 1$\\sigma$')\n",
    "            plt.fill_between(kk, *ws.cInterval(estimates[:, 1], variances[:, 1]), alpha=.2)\n",
    "        else:\n",
    "            kk2 = kk.repeat(2)\n",
    "            plt.plot(kk2, estimates.flatten(), label='Kalman ± 1$\\sigma$')\n",
    "            plt.fill_between(kk2, *ws.cInterval(estimates, variances), alpha=.2)\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    sigproc = {}\n",
    "    ### INSERT ANSWER TO EXC \"signal processing\" HERE ###\n",
    "    # sigproc['some method'] = ...\n",
    "    for method, estimate in sigproc.items():\n",
    "        plt.plot(kk[:len(estimate)], estimate, label=method)\n",
    "\n",
    "    plt.xlabel('Time index (k)')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.axhline(0, c='k', lw=1, ls='--')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a6c90a",
   "metadata": {},
   "source": [
    "**Exc -- AR1 properties:** Answer the following.\n",
    "- What does `seed` control?\n",
    "- Explain what happens when `M=0`. Also consider $Q \\rightarrow 0$.  \n",
    "  Can you give a name to this `truth` process,\n",
    "  i.e. a link to the relevant Wikipedia page?  \n",
    "  What about when `M=1`?  \n",
    "  Describe the general nature of the process as `M` changes from 0 to 1.  \n",
    "  What about when `M>1`?  \n",
    "- What happens when $R \\rightarrow 0$ ?\n",
    "- What happens when $R \\rightarrow \\infty$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb1548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('AR1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8a3170",
   "metadata": {},
   "source": [
    "## The (univariate) Kalman filter (KF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229c5fcb",
   "metadata": {},
   "source": [
    "Now we have a random variable that evolves in time, that we can pretend is unknown,\n",
    "in order to estimate (or \"track\") it.\n",
    "More specifically, while $x_0$ is unknown,\n",
    "we do know that $x_0 \\sim \\NormDist(x\\supa_0, P\\supa_0)$, including the parameters.\n",
    "We also know that $x_k$ evolves according to eqn. (Dyn).\n",
    "Therefore, as shown in the following exercise,\n",
    "for $x_1$ we know that\n",
    "$x_1 \\sim \\NormDist(x\\supf_1, P\\supf_1)$, with\n",
    "$$\\begin{align}\n",
    "x\\supf_1 &= \\DynMod \\, x\\supa_0 \\tag{5} \\\\\n",
    "P\\supf_1 &= \\DynMod^2 \\, P\\supa_0 + Q \\tag{6}\n",
    "\\end{align}$$\n",
    "\n",
    "#### Exc -- linear algebra of Gaussian random variables\n",
    "- (a) Show the linearity of the expectation operator:\n",
    "      $\\Expect [ \\DynMod  x + b ] = \\DynMod \\Expect[x] + b$, for some constant $b$.\n",
    "- (b) Thereby, show that $\\mathbb{Var}[ \\DynMod  x + b ] = \\DynMod^2 \\mathbb{Var} [x]$.\n",
    "- (c) *Optional*: Now let $z = x + q$, with $x$ and $q$ independent and Guassian.\n",
    "      Then the pdf of this sum of random variables, $p_z(z)$, is given by convolution\n",
    "      (hopefully this makes intuitive sense, at least in the discrete case):\n",
    "      $$ p_z(z) = \\int p_x(x) \\, p_q(z - x) \\, d x \\,.$$\n",
    "      Show that $z$ is also Gaussian,\n",
    "      whose mean and variance are the sum of the means and variances (respectively).  \n",
    "      *Hint: you will need the result on [completing the square](T3%20-%20Bayesian%20inference.ipynb#Exc----GG-Bayes),\n",
    "      specifically the part that we did not make use of for Bayes' rule.  \n",
    "      If you get stuck, you can also view the excellent [3blue1brown](https://www.youtube.com/watch?v=d_qvLDhkg00&t=266s&ab_channel=3Blue1Brown) on the topic.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d699eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Sum of Gaussians', 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66da5c8",
   "metadata": {},
   "source": [
    "Formulae (5) and (6) are called the **forecast step** of the KF.\n",
    "But when $y_1$ becomes available, according to eqn. (Obs),\n",
    "then we can update/condition our estimate of $x_1$, i.e. compute the posterior,\n",
    "$p(x_1 | y_1) = \\mathcal{N}(x_1 \\mid x\\supa_1, P\\supa_1) \\,,$\n",
    "using the formulae we developed for Bayes' rule with\n",
    "[Gaussian distributions](T3%20-%20Bayesian%20inference.ipynb#Gaussian-Gaussian-Bayes'-rule-(1D)).\n",
    "We call this the **analysis step** of the KF.\n",
    "Note that there are no approximations to these steps (with the linear-Gaussianassumptions we have made).\n",
    "\n",
    "If $k=1$ is \"today\", then we can say that \"yesterday's forecast became today's prior\".\n",
    "We can subsequently apply the same two steps again\n",
    "to produce forecast and analysis estimates for the next time index $k$.\n",
    "Thus, at any point in time, $k$, we compute the exact Bayesian pdf's for $x_k$:\n",
    "The analysis step \"assimilates\" $y_k$ to compute $p(x_k | y_{1:k})$,\n",
    "where $y_{1:k} = y_1, \\ldots, y_k$,\n",
    "while the forecast computes $p(x_{k+1}| y_{1:k})$.\n",
    "It is important to appreciate two benefits to this **recursive** procedure\n",
    "\n",
    "- The recursiveness of the procedure reflects the recursiveness (Markov property) of nature:\n",
    "  Both in the problem and our solution, time $k+1$ *builds on* time $k$.\n",
    "  It means that we do not have to re-do the entire problem for each $k$.\n",
    "- At every time $k$ we only deal with functions of 1 or 2 variables: $x_k$ and $x_{k+1}$.\n",
    "  Even if we were doing $k$ computations, this is a significantly smaller domain\n",
    "  (in which to quanitify our densities or covariances) than that of the joint pdf $p(x_{1:k} | y_{1:k})$.\n",
    "  Ref. [curse of dimensionality](http://localhost:8888/notebooks/notebooks/T3%20-%20Bayesian%20inference.ipynb#Exc-(optional)----Curse-of-dimensionality,-part-1).\n",
    "\n",
    "Note, however, that our recursive procedure, called ***filtering***, does *not* compute $p(x_l | y_{1:k})$ for any $l<k$. In other words, any filtering estimate only contains *past* information. Updating estimates of the state at any previous time(s) is called ***smoothing***.  However, for the purposes of prediction/forecasting, filtering is all we need: accurate initial conditions (estimates of the present moment)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99f0088",
   "metadata": {},
   "source": [
    "#### Exc -- Implementation\n",
    "Below is a very rudimentary sequential estimator (not the KF!), essentially just doing \"persistance\" forecasts, and setting the analysis estimates to the value of the observations (*which is only generally a possibility in this linear, scalar case*). Run its cell to define it, and then re-run the above interactive animation cell. Then\n",
    "- Implement the KF properly by replace the forecast and analysis steps below. *Re-run the cell.*\n",
    "- Try implementing the analysis step both in the \"precision\" and \"gain\" forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1864c6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KF(nTime, xa, Pa, M, H, Q, R, obsrvs):\n",
    "    \"\"\"Kalman filter. PS: (xa, Pa) should be input with *initial* values.\"\"\"\n",
    "    ############################\n",
    "    # TEMPORARY IMPLEMENTATION #\n",
    "    ############################\n",
    "    estimates = np.zeros((nTime, 2))\n",
    "    variances = np.zeros((nTime, 2))\n",
    "    for k in range(nTime):\n",
    "        # Forecast step\n",
    "        xf = xa\n",
    "        Pf = Pa\n",
    "        # Analysis update step\n",
    "        Pa = R / H**2\n",
    "        xa = obsrvs[k] / H\n",
    "        # Assign\n",
    "        estimates[k] = xf, xa\n",
    "        variances[k] = Pf, Pa\n",
    "    return estimates, variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee23d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('KF1 code')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbaa4bf",
   "metadata": {},
   "source": [
    "#### Exc -- KF behaviour\n",
    "- Set `logQ` to its minimum, and `M=1`.  \n",
    "  We established in Exc \"AR1\" that the true states are now constant in time (but unknown).  \n",
    "  How does the KF fare in estimating it?  \n",
    "  Does its uncertainty variance ever reach 0?\n",
    "- What is the KF uncertainty variance in the case of `M=0`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33843343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('KF behaviour')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350ae0cd",
   "metadata": {},
   "source": [
    "#### Exc -- Temporal convergence\n",
    "In general, $\\DynMod$, $\\ObsMod$, $Q$, and $R$ depend on time, $k$\n",
    "(often to parameterize exogenous/outside factors/forces/conditions),\n",
    "and there are no limit values that the KF parameters converge to.\n",
    "But, we assumed that they are all stationary.\n",
    "In addition, suppose $Q=0$ and $\\ObsMod = 1$.\n",
    "Show that\n",
    "\n",
    "- (a) $1/P\\supa_k = 1/(\\DynMod^2 P\\supa_{k-1}) + 1/R$,\n",
    "  by combining the forecast and analysis equations for the variance.\n",
    "- (b) $1/P\\supa_k = 1/P\\supa_0 + k/R$, if $\\DynMod = 1$.\n",
    "- (c) $P\\supa_{\\infty} = 0$, if $\\DynMod = 1$.\n",
    "- (d) $P\\supa_{\\infty} = 0$, if $\\DynMod < 1$.  \n",
    "- (e) $P\\supa_{\\infty} = R (1-1/\\DynMod^2)$, if $\\DynMod > 1$.  \n",
    "  *Hint: Look for the fixed point of the recursion of part (a).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba5c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Asymptotic Riccati')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c323a2",
   "metadata": {},
   "source": [
    "**Exc (optional) -- Temporal CV, part 2:**\n",
    "Now we don't assume that $Q$ is zero. Instead\n",
    "- (a) Suppose $\\DynMod = 0$. What does $P\\supa_k$ equal?\n",
    "- (b) Suppose $\\DynMod = 1$. Show that $P\\supa_\\infty$\n",
    "      satisfies the quadratic equation: $0 = P^2 + Q P - Q R$.  \n",
    "      Thereby, without solving the quadratic equation, show that\n",
    "    - (c) $P\\supa_\\infty \\rightarrow R$ (from below) if $Q \\rightarrow +\\infty$.\n",
    "    - (d) $P\\supa_\\infty \\rightarrow \\sqrt{ Q R}$ (from above) if $Q \\rightarrow 0^+$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d6fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('Asymptotes when Q>0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3abbb0",
   "metadata": {},
   "source": [
    "#### Exc -- Impact of biases\n",
    "Re-run the above interative animation to set the default control values. Answer the following\n",
    "\n",
    "- `logR_bias`/`logQ_bias` control the (multiplicative) bias in $R$/$Q$ that is fed to the KF.\n",
    "  What happens when the KF \"thinks\" the measurement/dynamical error\n",
    "  is (much) smaller than it actually is?\n",
    "  What about larger?\n",
    "- Re-run the animation to get default values.\n",
    "  Set `logQ` to 0, which will make the following behaviour easier to describe.\n",
    "  In the code, add 20 to the initial `xa` **given to the KF**.\n",
    "  How long does it take for it to recover from this initial bias?\n",
    "- Multiply `Pa` **given to the KF** by 0.01. What about now?\n",
    "- Remove the previous biases.\n",
    "  Instead, multiply `M` **given to the KF** by 2, and observe what happens.\n",
    "  Try the same, but dividing `M` by 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99621a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('KF with bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594eff0",
   "metadata": {},
   "source": [
    "## Alternative methods\n",
    "\n",
    "When it comes to (especially univariate) time series analysis,\n",
    "the Kalman filter (KF) is not the only game in town.\n",
    "For example, **signal processing** offers several alternative filters.\n",
    "Indeed, the word \"filter\" in the KF originates in that domain,\n",
    "where it originally referred to the removal of high-frequency noise,\n",
    "since this tends to coincide with an improved estimate of the signal.\n",
    "\n",
    "But for the above problem (which is linear-Gaussian!),\n",
    "the KF is guaranteed (on average, in the long run, in terms of mean square error)\n",
    "to outperform any other method.\n",
    "We will see cases later (of full-blown state estimation)\n",
    "where the difference is much clearer,\n",
    "and indeed it might not even be clear how to apply signal processing methods.\n",
    "However, the KF has an unfair advantage: we are giving it a ton of information\n",
    "about the problem (`M, H, R, Q`) that the signal processing methods do not get.\n",
    "Therefore, they typically also require a good deal of tuning\n",
    "(in practice, so does the KF, since `Q` and `R` are rarely well determined).\n",
    "We will not review any signal processing theory here,\n",
    "but challenge you to make use of what `scipy` already has to offer.\n",
    " \n",
    "#### Exc (optional) -- signal processing\n",
    "Run the following cell to import and define some more tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474ff892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.signal as sig\n",
    "def nrmlz(x):\n",
    "    return x / x.sum()\n",
    "def trunc(x, n):\n",
    "    return np.pad(x[:n], (0, len(x)-n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1cc96d",
   "metadata": {},
   "source": [
    "Now try to \"filter\" the `obsrvs` to produce estimates of `truth`.\n",
    "In each case, add your estimate (\"filtered signal\" in that domain's parlance)\n",
    "to the `sigproc` dictionnary in the interactive animation cell,\n",
    "with an appropriate name/key (this will automatically include it in the plotting).  \n",
    "Use\n",
    "- (a) [`sig.wiener`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.wiener.html).  \n",
    "      *PS: this is a direct ancestor of the KF*.\n",
    "- (b) a moving average, for example [`sig.windows.hamming`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.windows.hamming.html).  \n",
    "      *Hint: you may also want to use [`sig.convolve`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve.html#scipy.signal.convolve)*.\n",
    "- (c) a low-pass filter using [`np.fft`](https://docs.scipy.org/doc/scipy/reference/fft.html#).  \n",
    "      *Hint: you may also want to use the above `trunc` function.*\n",
    "- (d) The [`sig.butter`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.butter.html) filter.\n",
    "      *Hint: apply with [`sig.filtfilt`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.filtfilt.html).*\n",
    "- (e) not really a signal processing method: [`sp.interpolate.UniveriateSpline`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.UnivariateSpline.html)\n",
    "\n",
    "The answers should be considered examples, not the uniquely right way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws.show_answer('signal processing', 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8173122",
   "metadata": {},
   "source": [
    "#### Exc (optional) -- Analytic simplification in the case of an unknown constant\n",
    "\n",
    "- Note that in case $Q = 0$,\n",
    "then $x_{k+1} = \\DynMod^k x_0$.  \n",
    "- So if $\\DynMod = 1$, then $x_k = x_0$, so we are estimating an unknown *constant*,\n",
    "and can drop its time index subscript.  \n",
    "- For simplicity, assume $\\ObsMod = 1$, and $P^a_0 \\rightarrow +\\infty$.  \n",
    "- Then $p(x | y_{1:k}) \\propto \\exp \\big\\{- \\sum_l \\| y_l - x \\|^2_R / 2 \\big\\}\n",
    "= \\NormDist(x | \\bar{y}, R/k )$, which again follows by completing the square.  \n",
    "- In words, the (accumulated) posterior mean is the sample average,\n",
    "  $\\bar{y} = \\frac{1}{k}\\sum_l y_l$,  \n",
    "  and the variance is that of a single observation divided by $k$.\n",
    "\n",
    "Show that this is the same posterior that the KF recursions produce.  \n",
    "*Hint: while this is straightforward for the variance,\n",
    "you will probably want to prove the mean using induction.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7fa142",
   "metadata": {},
   "source": [
    "## Summary\n",
    "As a subset of state estimation we can do time series estimation\n",
    "[(wherein state-estimation is called state-space approach)](https://www.google.com/search?q=\"We+now+demonstrate+how+to+put+these+models+into+state+space+form\").\n",
    "Moreover, DA methods produce uncertainty quantification, something which is usually more obscure with time series analysis methods.\n",
    "\n",
    "### Next: [T5 - Multivariate Kalman filter](T5%20-%20Multivariate%20Kalman%20filter.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "jupytext": {
   "formats": "ipynb,scripts//py"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
