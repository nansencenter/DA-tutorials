{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote = \"https://raw.githubusercontent.com/nansencenter/DA-tutorials\"\n",
    "!wget -qO- {remote}/master/notebooks/resources/colab_bootstrap.sh | bash -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77148a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources import show_answer, EnKF_animation\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "plt.ion();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1e8188",
   "metadata": {},
   "source": [
    "# T9 - Writing your own EnKF\n",
    "In this tutorial we're going to code an EnKF implementation using numpy.\n",
    "As with the KF, the EnKF consists of the recursive application of\n",
    "a forecast step and an analysis step.\n",
    "$\n",
    "% ######################################## Loading TeX (MathJax)... Please wait ########################################\n",
    "\\newcommand{\\Reals}{\\mathbb{R}} \\newcommand{\\Expect}[0]{\\mathbb{E}} \\newcommand{\\NormDist}{\\mathscr{N}} \\newcommand{\\DynMod}[0]{\\mathscr{M}} \\newcommand{\\ObsMod}[0]{\\mathscr{H}} \\newcommand{\\mat}[1]{{\\mathbf{{#1}}}} \\newcommand{\\bvec}[1]{{\\mathbf{#1}}} \\newcommand{\\trsign}{{\\mathsf{T}}} \\newcommand{\\tr}{^{\\trsign}} \\newcommand{\\ceq}[0]{\\mathrel{â‰”}} \\newcommand{\\xDim}[0]{D} \\newcommand{\\supa}[0]{^\\text{a}} \\newcommand{\\supf}[0]{^\\text{f}} \\newcommand{\\I}[0]{\\mat{I}} \\newcommand{\\K}[0]{\\mat{K}} \\newcommand{\\bP}[0]{\\mat{P}} \\newcommand{\\bH}[0]{\\mat{H}} \\newcommand{\\bF}[0]{\\mat{F}} \\newcommand{\\R}[0]{\\mat{R}} \\newcommand{\\Q}[0]{\\mat{Q}} \\newcommand{\\B}[0]{\\mat{B}} \\newcommand{\\C}[0]{\\mat{C}} \\newcommand{\\Ri}[0]{\\R^{-1}} \\newcommand{\\Bi}[0]{\\B^{-1}} \\newcommand{\\X}[0]{\\mat{X}} \\newcommand{\\A}[0]{\\mat{A}} \\newcommand{\\Y}[0]{\\mat{Y}} \\newcommand{\\E}[0]{\\mat{E}} \\newcommand{\\U}[0]{\\mat{U}} \\newcommand{\\V}[0]{\\mat{V}} \\newcommand{\\x}[0]{\\bvec{x}} \\newcommand{\\y}[0]{\\bvec{y}} \\newcommand{\\z}[0]{\\bvec{z}} \\newcommand{\\q}[0]{\\bvec{q}} \\newcommand{\\br}[0]{\\bvec{r}} \\newcommand{\\bb}[0]{\\bvec{b}} \\newcommand{\\bx}[0]{\\bvec{\\bar{x}}} \\newcommand{\\by}[0]{\\bvec{\\bar{y}}} \\newcommand{\\barB}[0]{\\mat{\\bar{B}}} \\newcommand{\\barP}[0]{\\mat{\\bar{P}}} \\newcommand{\\barC}[0]{\\mat{\\bar{C}}} \\newcommand{\\barK}[0]{\\mat{\\bar{K}}} \\newcommand{\\D}[0]{\\mat{D}} \\newcommand{\\Dobs}[0]{\\mat{D}_{\\text{obs}}} \\newcommand{\\Dmod}[0]{\\mat{D}_{\\text{obs}}} \\newcommand{\\ones}[0]{\\bvec{1}} \\newcommand{\\AN}[0]{\\big( \\I_N - \\ones \\ones\\tr / N \\big)}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47b5d3d",
   "metadata": {},
   "source": [
    "This presentation follows the traditional template, presenting the EnKF as the \"the Monte Carlo version of the KF\n",
    "where the state covariance is estimated by the ensemble covariance\".\n",
    "It is not obvious that this postulated method should work;\n",
    "indeed, it is only justified upon inspection of its properties,\n",
    "deferred to later.\n",
    "\n",
    "<mark><font size=\"-1\">\n",
    "<b>NB:</b>\n",
    "Since we're going to focus on a single filtering cycle (at a time),\n",
    "the subscript $k$ is dropped. Moreover, <br>\n",
    "The superscript $f$ indicates that $\\{\\x_n\\supf\\}_{n=1..N}$ is the forecast (prior) ensemble.<br>\n",
    "The superscript $a$ indicates that $\\{\\x_n\\supa\\}_{n=1..N}$ is the analysis (posterior) ensemble.\n",
    "</font></mark>\n",
    "\n",
    "### The forecast step\n",
    "Suppose $\\{\\x_n\\supa\\}_{n=1..N}$ is an iid. sample from $p(\\x_{k-1} \\mid \\y_1,\\ldots, \\y_{k-1})$, which may or may not be Gaussian.\n",
    "\n",
    "The forecast step of the EnKF consists of a Monte Carlo simulation\n",
    "of the forecast dynamics for each $\\x_n$:\n",
    "$$\n",
    "\t\\forall n, \\quad \\x\\supf_n = \\DynMod(\\x_n\\supa) + \\q_n  \\,, \\\\\n",
    "$$\n",
    "where $\\{\\q_n\\}_{n=1..N}$ are sampled iid. from $\\NormDist(\\bvec{0},\\Q)$,\n",
    "or whatever noise model is assumed,  \n",
    "and $\\DynMod$ is the model dynamics.\n",
    "The dynamics could consist of *any* function, i.e. the EnKF can be applied with nonlinear models.\n",
    "\n",
    "The ensemble, $\\{\\x_n\\supf\\}_{n=1..N}$, is then an iid. sample from the forecast pdf,\n",
    "$p(\\x_k \\mid \\y_1,\\ldots,\\y_{k-1})$. This follows from the definition of the latter, so it is a relatively trivial idea and way to obtain this pdf. However, before Monte-Carlo methods were computationally feasible, the computation of the forecast pdf required computing the [Chapman-Kolmogorov equation](https://en.wikipedia.org/wiki/Chapman%E2%80%93Kolmogorov_equation), which constituted a major hurdle for filtering methods.\n",
    "\n",
    "### The analysis update step\n",
    "of the ensemble is given by:\n",
    "$$\\begin{align}\n",
    "\t\\forall n, \\quad \\x\\supa_n &= \\x_n\\supf + \\barK \\left\\{\\y - \\br_n - \\ObsMod(\\x_n\\supf) \\right\\}\n",
    "\t\\,, \\\\\n",
    "\t\\text{or,}\\quad\n",
    "\t\\E\\supa &=  \\E\\supf  + \\barK \\left\\{\\y\\ones\\tr - \\Dobs - \\ObsMod(\\E\\supf)  \\right\\} \\,,\n",
    "    \\tag{4}\n",
    "\\end{align}\n",
    "$$\n",
    "where the \"observation perturbations\", $\\br_n$, are sampled iid. from the observation noise model, e.g. $\\NormDist(\\bvec{0},\\R)$,  \n",
    "and form the columns of $\\Dobs$,  \n",
    "and the observation operator (again, any type of function), $\\ObsMod$, is applied column-wise to $\\E\\supf$.\n",
    "\n",
    "The gain $\\barK$ is defined by inserting the ensemble estimates for\n",
    " * (i) $\\bP\\supf \\bH\\tr$: the cross-covariance between $\\x\\supf$ and $\\ObsMod(\\x\\supf)$, and\n",
    " * (ii) $\\bH \\bP\\supf \\bH\\tr$: the covariance matrix of $\\ObsMod(\\x\\supf)$,\n",
    "\n",
    "in the formula for $\\K$, namely eqn. (K1) of [T5](T5%20-%20Multivariate%20Kalman%20filter.ipynb).\n",
    "Using the estimators from [T8](T8%20-%20Monte-Carlo%20%26%20ensembles.ipynb) yields\n",
    "\n",
    "$$\\begin{align}\n",
    "\t\\barK &= \\X \\Y\\tr ( \\Y \\Y\\tr + (N{-}1) \\R )^{-1} \\,, \\tag{5a}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\Y \\in \\Reals^{P \\times N}$\n",
    "is the centered, *observed* ensemble\n",
    "$\\Y \\ceq\n",
    "\\begin{bmatrix}\n",
    "    \\y_1 -\\by, & \\ldots & \\y_n -\\by, & \\ldots & \\y_N -\\by\n",
    "\\end{bmatrix} \\,,$ where $\\y_n = \\ObsMod(\\x_n\\supf)$.\n",
    "\n",
    "The EnKF is summarized in the animation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d374c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EnKF_animation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7817a318",
   "metadata": {},
   "source": [
    "#### Exc -- Woodbury for the ensemble subspace\n",
    "(a) Use the Woodbury identity (C2) of [T5](T5%20-%20Multivariate%20Kalman%20filter.ipynb) to show that eqn. (5a) can also be written\n",
    "$$\\begin{align}\n",
    "\t\\barK &= \\X ( \\Y\\tr \\Ri \\Y + (N{-}1)\\I_N  )^{-1} \\Y\\tr \\Ri \\,. \\tag{5b}\n",
    "\\end{align}\n",
    "$$\n",
    "(b) What is the potential benefit of (5b) vs. (5a) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e06c77b",
   "metadata": {},
   "source": [
    "#### Exc -- KG workings\n",
    "The above animation assumed that the observation operator is just the identity matrix, $\\I$, rather than a general observation operator, $\\ObsMod()$. Meanwhile, the Kalman gain used by the EnKF, eqn. (5a), is applicable for any $\\ObsMod()$. On the other hand, the formula (5a) consists solely of linear algebra. Therefore it cannot perfectly represent any general (nonlinear) $\\ObsMod()$. So how does it actually treat the observation operator? What meaning can we assign to the resulting updates?  \n",
    "*Hint*: consider the limit of $\\R \\rightarrow 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e5ee87",
   "metadata": {},
   "source": [
    "#### Exc -- EnKF nobias (a)\n",
    "Consider the ensemble averages,\n",
    " - $\\bx\\supa = \\frac{1}{N}\\sum_{n=1}^N \\x\\supa_n$, and\n",
    " - $\\bx\\supf = \\frac{1}{N}\\sum_{n=1}^N \\x\\supf_n$,\n",
    "\n",
    "and recall that the analysis step, eqn. (4), defines $\\x\\supa_n$ from $\\x\\supf_n$.\n",
    "\n",
    "\n",
    "(a) Show that, in case $\\ObsMod$ is linear (the matrix $\\bH$),\n",
    "$$\\begin{align}\n",
    "\t\\Expect \\bx\\supa &=  \\bx\\supf  + \\barK \\left\\{\\y\\ones\\tr - \\bH\\bx\\supf  \\right\\} \\,, \\tag{6}\n",
    "\\end{align}\n",
    "$$\n",
    "where the expectation, $\\Expect$, is taken with respect to $\\Dobs$ only (i.e. not the sampling of the forecast ensemble, $\\E\\supf$ itself).\n",
    "\n",
    "What does this mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d2f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer(\"EnKF_nobias_a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f594689",
   "metadata": {},
   "source": [
    "#### Exc (optional) -- EnKF nobias (b)\n",
    "Consider the ensemble covariance matrices:\n",
    "$$\\begin{align}\n",
    "\\barP\\supf &= \\frac{1}{N-1} \\X{\\X}\\tr \\,, \\tag{7a} \\\\\\\n",
    "\\barP\\supa &= \\frac{1}{N-1} \\X\\supa{\\X\\supa}\\tr \\,. \\tag{7b}\n",
    "\\end{align}$$\n",
    "\n",
    "Now, denote the centralized observation perturbations\n",
    "$\\D \\ceq\n",
    "\\begin{bmatrix}\n",
    "    \\br_1 -\\bar{\\br}, & \\ldots & \\br_n -\\bar{\\br}, & \\ldots & \\br_N -\\bar{\\br}\n",
    "\\end{bmatrix} $.\n",
    "Note that $\\D \\ones = \\bvec{0}$ and that\n",
    "$$\n",
    "\\begin{align}\n",
    "\t\\label{eqn:R_sample_cov_of_D}\n",
    "\t\\frac{1}{N-1} \\D \\D\\tr &= \\R \\,, \\tag{9a} \\\\\\\n",
    "\t\\label{eqn:zero_AD_cov}\n",
    "\t\\X \\D\\tr &= \\bvec{0} \\tag{9b}\n",
    "\\end{align}\n",
    "$$\n",
    "is satisfied in the expected sense, i.e. by taking the expectation on the left-hand side.\n",
    "Thereby, show that\n",
    "\n",
    "$$\\begin{align}\n",
    "    \\Expect \\, \\barP\\supa &= [\\I_{\\xDim} - \\barK \\bH]\\barP\\supf \\, . \\tag{10}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7c979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer(\"EnKF_nobias_b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68543df5",
   "metadata": {},
   "source": [
    "#### Exc (optional) -- EnKF bias (c)\n",
    "Show that, if no observation perturbations are used in eqn. (4), then $\\barP\\supa$ would be too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78427767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer(\"EnKF_without_perturbations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9dd11c",
   "metadata": {},
   "source": [
    "## Experimental setup\n",
    "\n",
    "Before making the EnKF, we'll set up an experiment to test it with, so that you can check if you've implemented a working method or not.\n",
    "\n",
    "To that end, we'll use the Lorenz-63 model, from [T7](T7%20-%20Chaos%20%26%20Lorenz%20(optional).ipynb). The coupled ODEs are recalled here, but with some of the parameters fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbbc458",
   "metadata": {},
   "outputs": [],
   "source": [
    "xDim = 3\n",
    "\n",
    "def dxdt(x, sig=10, rho=28, beta=8/3):\n",
    "    x,y,z = x\n",
    "    d = np.zeros(3)\n",
    "    d[0] = sig*(y - x)\n",
    "    d[1] = rho*x - y - x*z\n",
    "    d[2] = x*y - beta*z\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440ac108",
   "metadata": {},
   "source": [
    "Next, we make the forecast model $\\DynMod$ out of $\\frac{d \\x}{dt}$ such that $\\x(t+dt) = \\DynMod(\\x(t),t,dt)$. We'll make use of the \"4th order Runge-Kutta\" integrator `rk4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccc7627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dapper.mods.integration import rk4\n",
    "\n",
    "def Dyn(E, t0, dt):\n",
    "\n",
    "    def step(x0):\n",
    "        return rk4(lambda x, t: dxdt(x), x0, t0, dt)\n",
    "\n",
    "    if E.ndim == 1:\n",
    "        # Truth (single state vector) case\n",
    "        E = step(E)\n",
    "    else:\n",
    "        # Ensemble case\n",
    "        for n in range(E.shape[1]):\n",
    "            E[:, n] = step(E[:, n])\n",
    "\n",
    "    return E\n",
    "\n",
    "Q12 = np.zeros((xDim, xDim))\n",
    "Q = Q12 @ Q12.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a94cd6",
   "metadata": {},
   "source": [
    "Notice the loop over each ensemble member. For better performance, this should be vectorized, if possible. Or, if the forecast model is computationally demanding (as is typically the case in real applications), the loop should be parallelized: i.e. the forecast simulations should be distributed to separate computers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9621c685",
   "metadata": {},
   "source": [
    "The following are the time settings that we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a79370",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.01           # integrational time step\n",
    "dko = 25            # number of steps between observations\n",
    "dto = dko*dt        # time between observations\n",
    "Ko = 60             # total number of observations\n",
    "nTime = dko*(Ko+1)  # total number of time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e263dc7",
   "metadata": {},
   "source": [
    "Initial conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb521a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xa = np.array([1.509, -1.531, 25.46])\n",
    "Pa12 = np.eye(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac05f69",
   "metadata": {},
   "source": [
    "Observation model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd617b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 3 # ndim obs\n",
    "def Obs(E, t):\n",
    "    return E[:p] if E.ndim == 1 else E[:p, :]\n",
    "\n",
    "R12 = np.sqrt(2)*np.eye(p)\n",
    "R = R12 @ R12.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36baf273",
   "metadata": {},
   "source": [
    "Generate synthetic truth and observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a5baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init\n",
    "truths = np.zeros((nTime+1, xDim))\n",
    "obsrvs = np.zeros((Ko+1, p))\n",
    "truths[0] = xa + Pa12 @ rnd.randn(xDim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9479cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop\n",
    "for k in range(1, nTime+1):\n",
    "    truths[k] = Dyn(truths[k-1], (k-1)*dt, dt)\n",
    "    truths[k] += Q12 @ rnd.randn(xDim)\n",
    "    if k % dko == 0:\n",
    "        Ko = k//dko-1\n",
    "        obsrvs[Ko] = Obs(truths[k], np.nan) + R12 @ rnd.randn(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b14fa7",
   "metadata": {},
   "source": [
    "## EnKF implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aaa678",
   "metadata": {},
   "source": [
    "We will make use of `estimate_mean_and_cov` and `estimate_cross_cov` from the previous section. Paste them in below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1cc69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def estimate_mean_and_cov ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab32164",
   "metadata": {},
   "source": [
    "**Exc -- EnKF implementation:** Complete the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6762f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful linear algebra: compute B/A\n",
    "import numpy.linalg as nla\n",
    "\n",
    "ens_means = np.zeros((nTime+1, xDim))\n",
    "ens_vrncs = np.zeros((nTime+1, xDim))\n",
    "\n",
    "def my_EnKF(N):\n",
    "    \"\"\"My implementation of the EnKF.\"\"\"\n",
    "    ### Init ###\n",
    "    E = np.zeros((xDim, N))\n",
    "    for k in tqdm(range(1, nTime+1)):\n",
    "        t = k*dt\n",
    "        ### Forecast ##\n",
    "        # E = ...  # use model\n",
    "        # E = ...  # add noise\n",
    "        if k % dko == 0:\n",
    "            ### Analysis ##\n",
    "            y = obsrvs[[k//dko-1]].T  # current observation\n",
    "            Eo = Obs(E, t)            # observed ensemble\n",
    "            # Compute ensemble moments\n",
    "            PH = ...\n",
    "            HPH = ...\n",
    "            # Compute Kalman Gain\n",
    "            KG = ...\n",
    "            # Generate perturbations\n",
    "            Perturb = ...\n",
    "            # Update ensemble with KG\n",
    "            # E = ...\n",
    "        # Save statistics\n",
    "        ens_means[k] = np.mean(E, axis=1)\n",
    "        ens_vrncs[k] = np.var(E, axis=1, ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8dc62e",
   "metadata": {},
   "source": [
    "Notice that we only store some stats (`ens_means`). This is because in large systems,\n",
    "keeping the entire ensemble (or its covariance) in memory is probably too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbe50b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('EnKF v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e01b69",
   "metadata": {},
   "source": [
    "Now let's try out its capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f2cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run assimilation\n",
    "my_EnKF(10)\n",
    "\n",
    "# Plot\n",
    "fig, axs = plt.subplots(nrows=3, sharex=True)\n",
    "for i in range(3):\n",
    "    axs[i].plot(dt*np.arange(nTime+1), truths   [:, i], 'k', label=\"Truth\")\n",
    "    axs[i].plot(dt*np.arange(nTime+1), ens_means[:, i], 'b', label=\"Estimate\")\n",
    "    if i<p:\n",
    "        axs[i].plot(dto*np.arange(1, Ko+2), obsrvs[:, i], 'g*')\n",
    "    axs[i].set_ylabel(f\"{'xyz'[i]}\")\n",
    "axs[0].legend()\n",
    "plt.xlabel(\"Time (t)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6cd74",
   "metadata": {},
   "source": [
    "**Exc -- Diagnostics:** The visuals of the plots are nice. But it would be good to have a summary statistic of the accuracy performance of the filter. Make a function `average_rmse(truths, means)` that computes $ \\frac{1}{K+1} \\sum_{k=0}^K \\sqrt{\\frac{1}{\\xDim} \\| \\bx_k - \\x_k \\|_2^2} \\,.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e04a349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_rmse(truth, estimates):\n",
    "    ### INSERT ANSWER ###\n",
    "    average = ...\n",
    "    return average\n",
    "\n",
    "# Test\n",
    "average_rmse(truths, ens_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5391e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('rmse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debc6ac7",
   "metadata": {},
   "source": [
    "**Exc -- Experiment variations:**\n",
    " * (a). Repeat the above experiment, but now observing only the first (0th) component of the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6360b13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('Repeat experiment a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13e5bf6",
   "metadata": {},
   "source": [
    " * (b). Put a `seed()` command in the right place so as to be able to recreate exactly the same results from an experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef700c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('Repeat experiment b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f846bf",
   "metadata": {},
   "source": [
    " * (c). Use $N=5$, and repeat the experiments. This is quite a small ensemble size, and quite often it will yield divergence: the EnKF \"definitely loses track\" of the truth, typically because of strong nonlinearity in the forecast models, and underestimation (by $\\barP\\supa)$ of the actual errors. Repeat the experiment with different seeds until you observe in the plots that divergence has happened.\n",
    " * (d). Implement \"multiplicative inflation\" to remedy the situation; this is a factor that should spread the ensemble further apart; a simple version is to inflate the perturbations. Implement it, and tune its value to try to avoid divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a7e28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_answer('Repeat experiment cd')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9ec3d6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "The EnKF is a simple algorithm for data assimilation (DA),\n",
    "that is capable of handling some degree of nonlinearity and big data/problems.\n",
    "It is a consistent approximation of the Kalman filter,\n",
    "but also contains biases.\n",
    "Successful application requires tuning of some ad-hoc parameters.\n",
    "\n",
    "<a name=\"References\"></a>\n",
    "\n",
    "### References"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "jupytext": {
   "formats": "ipynb,scripts//py:light,scripts//md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
